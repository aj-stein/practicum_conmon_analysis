{
  "id": "D_kwDOOKttEc4AfarT",
  "url": "https://github.com/FedRAMP/continuous-reporting-cwg/discussions/38",
  "title": "Incorporating Data into Reporting Outputs",
  "body": "What are some ideas on how FedRAMP could incorporate data into reporting outputs to facilitate easy integration of a consolidated risk posture?",
  "author": "JohnWHamilton",
  "created_at": "2025-04-23T18:42:31Z",
  "comments": [
    {
      "id": "DC_kwDOOKttEc4Ax4iv",
      "url": "https://github.com/FedRAMP/continuous-reporting-cwg/discussions/38#discussioncomment-13076655",
      "body": "I see not many people have commented on this one yet, but I will post some quick take ideas I think about when we talk about reporting and how we incorporate data.  BLUF immediately follows, explanations come after that.\r\n\r\n1. Data is reported by shipping bytes, people using software to understand it comes after\r\n1. Data by ID reference, not by value\r\n1. Blob storage is great, APIs with specifications good, many custom APIs bad\r\n1. If APIs, fewer specified APIs over many custom APIs\r\n1. If APIs, pagination and rate limiting by default\r\n1. If blob storage or API, avoid compression if possible\r\n\r\nI put these in presumed order of importance because my opinions are a flow diagram of sorts: I strongly prefer 1, I would really prefer 1 and 2 together, but if not 1 then I hope for 2, if 2 is not possible, then I prefer 3 et cetera.\r\n\r\n# Data is reported by shipping bytes\r\n\r\nThere are a great many formats in town for serializing data or presenting a lot of markup for long-form human descriptions of things (HTML, Markdown, AsciiDoc, the dreaded PDF standards), but reporting should pass them along as bytes. The \"intelligence\" or logical processing for people to understand the data should happen \"at the edge.\" That means as a step of some magical lifeycle after reports are received, not doing the logical processing as you send reports all over the place. People and their software should not be peeking into data to understand what it is semantically or syntactically in hops from sender to receiver, or you get nasty reliability and security problems. This last point may sound counter-intuitive but this touches on strongly preferring [enveloping signatures over inline or detached ones](https://en.wikipedia.org/wiki/XML_Signature#:~:text=An%20XML%20signature%20used%20to,is%20called%20an%20enveloping%20signature), how you conditionally process or inspect data with middleboxes, and other stuff. That is dangerous with data, so reporting should see _all data as bytes_ until it gets to the receiver.\r\n\r\n# Data by ID reference, not by value\r\n\r\nAs fun as it is to encapsulate all data that is supporting evidence for a report, Rev5 ConMon or otherwise, it is very hazardous to paste big blobs of bytes into other blobs of bytes to build very large data objects to ship over the wire. Instead, it is preferable to have stable IDs (yes, even [URLs like addresses are a subset of Universal Resource Identifiers](https://assets.danielmiessler.com/images/68ca2654-8f07-48e6-855e-88a9e4c9f906-URL-URI-Miessler-2022.jpg)), and have intelligence at the edge retrieve it as needed, and only if that resource reading a report has the authentication and authorization to retrieve information from that ID.\r\n\r\nThe end resulf of this is continuous reporting has smaller payloads of data sent frequently and put back together on the receiver end on a case-by-case basis. This approach really enables the possibility of continuous reporting, because a system that has to download a terabyte all at once will be inflexible whether or not the frequency is daily, weekly, or monthly. It certainly won't work minutely unless the bandwidth is massive, and that is still wasteful even if feasible for the environment.\r\n\r\n# Blob storage is great\r\n\r\nThe concept of blob (a collection of bytes, conventional file or something else), and an object storage service for these blobs, is an interesting \"de-facto standard\" that is increasingly ubiqituous in cloud computing. It took me a while to realize that, although Amazon provides Simple Storage Service (S3) as a proprietary cloud service offering, the core API is also a de-facto standard people use to share data in more and more non-AWS services. There is no specification, but other services have followed the API through the publicly licensed SDK.\r\n\r\n- Cloudflare provides a [R2 services marketed as a S3 compatible object store](https://web.archive.org/web/20250405174705/https://www.cloudflare.com/developer-platform/solutions/s3-compatible-object-storage/).\r\n- Digital Ocean provides [Spaces, a service they similarly market as S3 compatible](https://web.archive.org/web/20250505181212/https://www.digitalocean.com/products/spaces).\r\n- You can use [GCP storage services as a S3 service in some cases](https://dzlab.github.io/gcp/2022/02/26/gs-with-s3-sdk/).\r\n- Similarly, you can use [Azure storages as a S3 service with community tooling](https://learn.microsoft.com/en-us/answers/questions/1183760/s3-api-support-over-azure-blob-storage)\r\n\r\nSo what is the consequence of this trend? Not only can you storage large datasets (of bytes, broken down into small pieces you can refer to by ID reference, not value), many solutions for data engineering, not just security use cases, can process data in place and run queries on them without additional extract, transform, and load procedures into a redundant copy. This approach underlies many of the popular data lake solutions and is a useful way to access data by reference (with ID) for internal _and_ external solutions, if we are talking just about bytes of data that make evidence. APIs are nice, but they are yet another thing to implement. More on that below.\r\n\r\n# If APIs, fewer specified APIs over many custom APIs\r\n\r\nIf there must be an API for data, it must have a specification that is maintained and the implementations are kept in sync with it. This problem is not limited to security solutions, but it is pretty rampant in my experience: many security services and software do not full specify their API, be it with OpenAPI/Swagger, RAML, or another API specification tool. Even if specified, companies can implement multiple versions and drift between specification and implementation becomes frequent. At this time, there is no requirement for a single, FedRAMP-managed API or a single API for respective CSPs, 3PAOs, or other stakeholders. That said, the fewer the APIs, the better, even if specified.\r\n\r\nMany existing APIs for each CSP are specific to their service offerings, even those that are similar (a counterpoint to the blob storage topic above; most CSPs have similar compute offerings that all have bespoke APIs, specified or not, so you have to implement a new client and mapping logic for each one). As a rule, the fewer necessary APIs specified, the better it is for data collection.\r\n\r\n# If APIs, pagination and rate limiting by default\r\n\r\nIf there must be an API for data, it is best that there by a form of pagination for calls to retrieve (not create or modify) large sets of records. To start, supporting evidence may be small for FedRAMP systems, it grows significantly. If an API call lists every extant data by ID reference without pagination, it will likely crash downstream systems. [Best practices in API design with pagination](https://apisyouwonthate.com/blog/api-design-basics-pagination/) are important. Additionally, many clients for larger CSPs accessing this data can be overwhelmed as the number of customers grow. APIs, in specification and implementation, should keep this point in mind from the beginning.\r\n\r\n# If blob storage or API, avoid compression if possible\r\n\r\nThere will be times that compression is unavoidable, but it is best to avoid compression if possible. By breaking down data in bytes form into smaller segments and accessing them by reference, you allow intelligent software \"at the edge\" to seek through parts of data item, not download and process the whole thing, with software that is aware of the formats and semantics of the files. Sometimes, this is achievable with compression depending on which archive format selected, but often not. For some data analysis platforms there are common compression techniques and they effectively decompress blob storage or embedded API data accordingly, but it makes seeking and download parts instead of the whole more difficult.",
      "author": "aj-stein-gsa",
      "created_at": "2025-05-08T14:03:30Z",
      "replies": [
        {
          "id": "DC_kwDOOKttEc4Ax5PD",
          "url": "https://github.com/FedRAMP/continuous-reporting-cwg/discussions/38#discussioncomment-13079491",
          "body": "Generally from my read of the post, much of this seems reasonable - makes data available by providing access from storage (though there should be a consistent standard hear that is on par with NIST standards) and ensures that it's at least capable of being delivered in a standardized format that can (and should) be cloud service agnostic. \r\n\r\nFrom any agency perspective, we've done the work to develop a risk posture dashboard based on our existing tool sets and would prefer to ingest data from the CSPs internally to apply the same methodology to risk management. As long as the data is accessible and standardized, we can develop the integration for ingest from the various cloud services and continue to operate our program with better access to the existing risk posture of the CSPs we leverage.",
          "author": "TWeikle",
          "created_at": "2025-05-08T17:53:38Z"
        },
        {
          "id": "DC_kwDOOKttEc4Ax5g2",
          "url": "https://github.com/FedRAMP/continuous-reporting-cwg/discussions/38#discussioncomment-13080630",
          "body": "> Generally from my read of the post, much of this seems reasonable - makes data available by providing access from storage (though there should be a consistent standard hear that is on par with NIST standards)\r\n\r\nCan you tell us which standard or what kind of information you would expect? I usually don't use NIST standards as an example because, as far as storage or access protocols go, is much more detailed and implementer-focused than NIST FIPS or Special Publication docs. Do you mean something like an API specification or protocol document from a standards-defining organization?\r\n\r\n> and ensures that it's at least capable of being delivered in a standardized format that can (and should) be cloud service agnostic.\r\n\r\nCompletely agree, hence themes 3-5 in the doc, glad we share that view!\r\n\r\n> From any agency perspective, we've done the work to develop a risk posture dashboard based on our existing tool sets and would prefer to ingest data from the CSPs internally to apply the same methodology to risk management. As long as the data is accessible and standardized, we can develop the integration for ingest from the various cloud services and continue to operate our program with better access to the existing risk posture of the CSPs we leverage.\r\n\r\nI am happy to read more about this perspective, would you mind sharing what detail you can about what and how you ingest said data and what that dashboard looks like? From your agency perspective, what is working well, what doesn't, and what do you wish you could ingest and analyze in that dashboard now, but cannot?\r\n\r\n",
          "author": "aj-stein-gsa",
          "created_at": "2025-05-08T20:29:32Z"
        }
      ]
    },
    {
      "id": "DC_kwDOOKttEc4AyQdN",
      "url": "https://github.com/FedRAMP/continuous-reporting-cwg/discussions/38#discussioncomment-13174605",
      "body": "There has been some good discussion on this topic in the last few weeks, but we are going to close this one for now. We will open up new posts for commenting accordingly around different facets of this topic when the time is right, or you can when it is right for you, fellow community members!",
      "author": "aj-stein-gsa",
      "created_at": "2025-05-16T19:42:31Z",
      "replies": []
    }
  ]
}