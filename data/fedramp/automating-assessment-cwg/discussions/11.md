# Metadata

title:Project Scoping: Developing a Scope of Efforts Regarding Automated Evidence Collection and Verification

author: [github.com/JBockmann](https://github.com/JBockmann)

url: [https://github.com/FedRAMP/automating-assessment-cwg/discussions/11](https://github.com/FedRAMP/automating-assessment-cwg/discussions/11)

created: 2025-04-03T21:20:23Z

id: D_kwDOOMDw3M4AfJO7



# Post

Reading through the discussions it is clear that many of us see areas that could benefit from the use of automation, and we've begun adding ideas and iterating; all of which is great! However, I think it would be prudent to ensure that we have a clear scope of our part in this project  which we can then use to focus our efforts.

### What's the purpose of this discussion?
To define the scope of our engagement with this project. This will have several key benefits.

1. To grant clarity of where efforts should be focused,
2. To ensure a complete picture is envisioned and understood by all parties, and
3. To ensure that no areas go left unaddressed or lacking in consideration.

### So then, let's first define the scope of our engagement with this project
- Automated evidence collection and validation for use with FedRAMP assessments and 3PAO engagement.
(I'm assuming the bit about the 3PAO to be true)

### Next, let's evaluate what this means we need to provide (ideally):
- A technical means to collect and validate evidence from the within the boundary.
- A technical means to provide the results of the above to FedRAMP/Marketplace and the 3PAO for review/authorization.

Theoretical added bonuses:
- A technical means to collaborate on the evidence gathered via automated means (say if narration is required).

Now that we have our list of deliverables as it were, and given that providing the results can be as simple as emailing a file; let's start with the largest portion of this project and break it down further.

### A technical means to collect and validate evidence from the within the boundary
- The boundary is referring to the authorization boundary of the offering in question. This will include the hosting environment (AWS GovCloud, Azure for Government, custom hosted, etc), the offering (the platform or software being provided), the relevant policies (with hosting systems if separate from above), and any external connections to the offering.
- The evidence is referring to proof that a control is fulfilled. Obvious, sure, but this currently ranges from text/log files, to screenshots, to any number of data or media types. Understanding what this is and how to interact will be key in developing the technical means to automating the collection and validation.
- Collecting and validating are mostly self explanatory, but it is worth mentioning that validating the effect of the information collected will be crucial to the efficacy of this project. Additional checks and balances may be required for a satisfactory result.
- The technical means will be what we are working on, but how do we collect and validate the evidence and what/where are we collecting it all from?

Let's take another look at the boundary from above to pull a list of potential focus areas together:
1. Hosting env (AWS GovCloud, Azure for Government, custom hosted, etc)
4. The offering (the platform or software being provided)
5. The corporate policies (and hosting system)
6. All external connections to the offering.

With each of these areas being a potential project on it's own, where should efforts be placed? It seems obvious that big name hosting environments would be the "easiest", but should that be the one and only focus at this time? I'll leave that up for debate.

As for the other categories, here are some considerations or potential solutions that could be utilized:

- If a standardized data structure for the status of the compliance is developed, then it would be much easier to build a path for integration all of the categories. With some compliance frameworks it feels like the data structure is nothing more than a PDF at times. I would strongly urge that a robust structure is developed and careful consideration is utilized to the possible methods of integration. Specifically, I'd love to see support for JSON, CSV export/import (if applicable), and a flat DB template structure that allows for ease of maintenance should a SMB like to host and use it themselves.
- Related the prior, where do we see the single source of truth for this data being housed, and how do we expect the various systems to integrate/interact? Are we still anticipating a largely manual process to initiate and supply evidence to FedRAMP for authorization, or would we also like to see the possibility of a fully automated approach (thus allowing even FR and agencies visibility into the real time status of an offering?)
- With the offering itself, if we develop a standardized data structure and method of integration, it's conceivable that this could exist in instructions to the CSP themselves to allow for a connection and collection/validation to occur. It's obviously a tall ask, but theoretically possible assuming the above is true.
- With corporate policies, in my experience SMB's would be at a major disadvantage here given that their policies are usually just word files on a drive somewhere. Robust GRC and policy tools are too expensive for adoption, and any other form of true validation here would be tricky.
- With external connections, would it suffice that since they are effectively require to be FR authorized (and therefore one could validate based on it's status in the Marketplace), or would additional validation be required?


I look forward to hearing additional input on this as I'm sure I've overlooked something given my limited time to respond at the moment.

Thanks all!

# Comments




## Comment 1

author: [github.com/dljenkins](https://github.com/dljenkins)

url: [https://github.com/FedRAMP/automating-assessment-cwg/discussions/11#discussioncomment-12793589](https://github.com/FedRAMP/automating-assessment-cwg/discussions/11#discussioncomment-12793589)

created: 2025-04-10T15:47:47Z

id: DC_kwDOOMDw3M4Awzb1

A standardized data structure to report the status needs to have flexibility built into it. I think we need a structure that is more like a configuration that a report. In this "Conmpliance Config", we would configure the API(s) to call to check the compliance status for one or more controls, then configure the asserts used to validate the response, we also configure the secrets used to access the APIs. There should be a subset of controls where "self-attestation" is allowed, that too would be configured. Now we just need a tool that can read this config, has permissions to get to the secrets and APIs, and can execute to determine a near real-time status of the systems security posture. I posted in other threads a sample JSON format for this.

The single source of truth is in the CSP system itself. It is pulled onDemand to view the status. The CSP can choose to cache API responses for defined periods of time. This could lead to a nearly fully automated approach. The CSP uploads their config file for 3PAOs, Agencies, and FedRAMP PMO to use and execute to see status. The 3PAO would be responsible for reviewing the API calls and assertions made confirming these indeed provided automated validation and that any self-attested controls are within the subset permitted. We might required 3PAO for Mod/High but not for Low.

Leveraged services/interconnections would need to refer to the "Compliance Config" for that service. Then the status can dynamically be checked for those too.

AI could be used to check the policies. Industry created tools could be used to ingest the policy docs (.doc, .pdf) and then ask a series of questions about each policy to ensure each policy type has the required content. The AI would provide answers, check itself to ensure all have answers, then provide a summary back to the 3PAO, Agency & FedRAMP PMO on status of each policy.

### Replies



## Comment 2

author: [github.com/sunstonesecure-robert](https://github.com/sunstonesecure-robert)

url: [https://github.com/FedRAMP/automating-assessment-cwg/discussions/11#discussioncomment-12805300](https://github.com/FedRAMP/automating-assessment-cwg/discussions/11#discussioncomment-12805300)

created: 2025-04-11T14:54:42Z

id: DC_kwDOOMDw3M4Aw2S0

OSCAL does a standard data structure to report the status. OSCAL indeed can be simplified/modularized but it's open source so we can simplify it and modularize it. CNCF and others are working on that. but as I've said elsewhere, sure, define any standard - LLMs make coding transformation of data from X -> Y trivial now. Even pointy haired managers like me can do it now :)

Infrastructure as Code also addresses your idea of single source of truth (when combined with a drift assessment). So yes if we just made IaC mandatory and do a diff between the Terraform/tofu/CDK/ARM code and the actual Resource Inventory API results, voila! truth! 

crypto sign it and agencies can trust it (as much as they trust banks, credit cards, mortgages, etc)


### Replies

