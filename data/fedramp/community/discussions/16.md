# Metadata

title:FedRAMP 20x Phase One Pilot Draft Submission -Meridian Knowledge Solutions

author: [github.com/aburroughs](https://github.com/aburroughs)

url: [https://github.com/FedRAMP/community/discussions/16](https://github.com/FedRAMP/community/discussions/16)

created: 2025-05-22T03:45:10Z

id: D_kwDOOxfoic4AgAIE



# Post

This submission was published early — prior to formal 3PAO (Fortreum) sign-off — as part of a collaborative effort with Fortreum to embrace the transparency and public participation ethos of the FedRAMP 20x initiative.

With mutual agreement, we’re treating this public release as a draft submission, with the goal of gathering meaningful feedback and enabling iterative validation ahead of the final milestone.

To our knowledge, this is the first 20x submission being actively assessed in public, using GitHub as both a trust interface and audit mechanism. Our 3PAO has forked the private repository and is performing validation via tracked commits, Issues, and Pull Requests — including draft attestations for each KSI.

While the package does not use OSCAL, it is purpose-built for auditability, traceability, and real-time validation, and includes:

1. A machine-readable KSI validation file
2. Git-tracked evidence references with commit SHA and timestamps
3. A live dashboard showing assertion status, evidence freshness, and validation logic

We recognize this differs from traditional submission formats — and that’s intentional. It is meant to demonstrate what 20x was designed to enable: continuous, verifiable, and publicly inspectable compliance. It also serves to show what smaller organizations — without dedicated GRC teams or analysts — can achieve with the right technical foundation and a commitment to transparency.

We welcome engagement via GitHub Issues and Pull Requests, and hope this submission contributes to the community’s evolving understanding of trust-centered compliance and the upcoming S3AD standard.

This package is being evaluated in real time. Nothing here is final — and that’s the point.

https://github.com/Meridian-Knowledge-Solutions/fedramp_20x_public_submission_live

# Comments




## Comment 1

author: [github.com/iteuscher](https://github.com/iteuscher)

url: [https://github.com/FedRAMP/community/discussions/16#discussioncomment-13236816](https://github.com/FedRAMP/community/discussions/16#discussioncomment-13236816)

created: 2025-05-22T16:13:36Z

id: DC_kwDOOxfoic4AyfpQ

Nicely done! I like the use of GitHub to track history, issues and make the assessment continuous and publicly accessible. I agree with you that this differs from traditional audit formats in the ways that 20x wants to be different -- continuous, automated, and low cost for CSPs. The SHA hashes are a nice touch we've been thinking about how to implement that and I really like what you've put together for it. I can see other CSPs being able to fork your repository and quickly get up and running with their own 20x repository and dashboard similar to what you've put together. 

One point of feedback I have on your JSON schema is that it might be helpful to have multiple evidence artifacts within a single KSI validation. I noticed for KSI-CNA | Cloud Native Architecture | Configure firewalls/proxy servers to limit inbound and outbound traffic. you have two rows, one for (Inbound) and one for (Outbound). It makes sense to have multiple evidence artifacts for some of the KSIs that need to show more than just one file to demonstrate the KSI validation. 

In your JSON schema you currently have 

Phase One Example
`          "evidence_reference": "evidence_v2/ksi-cna/waf_acl.json"`
    
Phase Two Example
`         "evidence_reference": "evidence/ksi-cna/aws_shield_dos_protection.pdf",`
`         "evidence_type": "static",`

It may be helpful to change this to an array that could hold multiple evidence references which each have an evidence type so that multiple evidence artifacts (inbound firewall rules and outbound firewall rules) can be included within a single KSI validation instead of having to split out the KSI validations into multiple rows. 

Overall very impressed by your draft submission! Great work and thank you for contributing it! 
-- Isaac Teuscher (Paramify)

### Replies



#### Reply 1

author: [github.com/aburroughs](https://github.com/aburroughs)

url: [https://github.com/FedRAMP/community/discussions/16#discussioncomment-13237284](https://github.com/FedRAMP/community/discussions/16#discussioncomment-13237284)

created: 2025-05-22T17:04:17Z

id: DC_kwDOOxfoic4Ayfwk

@iteuscher Thank you so much — that feedback means a lot :)



You nailed exactly what we hoped this submission could model: a transparent, automatable, and replicable way for CSPs of any size to participate in 20x without heavy lift. The SHA-based evidence integrity and GPG-signed 3PAO validations were designed to unlock that kind of public trust signal - which frankly is really cool to see an audit happen this way. 



And you're right about the schema — grouping evidence references into an array (with type and path per artifact) is a great Phase Two refinement. We had originally split validations for clarity but this change would let us collapse KSI results while still keeping traceability intact.



We’ll look into evolving the JSON structure to support that pattern. Thanks again for the thoughtful engagement — it means a lot to see the ecosystem moving in this direction together.



## Comment 2

author: [github.com/sunstonesecure-robert](https://github.com/sunstonesecure-robert)

url: [https://github.com/FedRAMP/community/discussions/16#discussioncomment-13240579](https://github.com/FedRAMP/community/discussions/16#discussioncomment-13240579)

created: 2025-05-23T00:12:52Z

id: DC_kwDOOxfoic4AygkD

super dope!!  I love your KSI READMEs - gonna borrow those if ok with you.  Those should be *required* (@pete-gov )

And you beat us to the punch with GitHub actions in the repo ;) 

Kudos all around!

### Replies



#### Reply 1

author: [github.com/aburroughs](https://github.com/aburroughs)

url: [https://github.com/FedRAMP/community/discussions/16#discussioncomment-13252280](https://github.com/FedRAMP/community/discussions/16#discussioncomment-13252280)

created: 2025-05-23T23:16:56Z

id: DC_kwDOOxfoic4Ayja4

Awesome - go for it! We do plan on adjusting the default (no license) state of our repo to be more in line with the sharing is caring model but we are waiting for more clarity on our submission before doing so. Thanks for the feedback! We appreciate the love



## Comment 3

author: [github.com/aj-stein-gsa](https://github.com/aj-stein-gsa)

url: [https://github.com/FedRAMP/community/discussions/16#discussioncomment-13327164](https://github.com/FedRAMP/community/discussions/16#discussioncomment-13327164)

created: 2025-05-31T04:04:59Z

id: DC_kwDOOxfoic4Ay1s8

@aburroughs 

Thank you for an early submission and doing so in the public.  We also wanted to commend you for initial approach, implementation, and your periodic updates to your example results with metrics for continuous reporting on the key security indicators (in the new GitHub repository, [Meridian-Knowledge-Solutions/fedramp_20x_public_submission_live](https://github.com/Meridian-Knowledge-Solutions/fedramp_20x_public_submission_live)).

The below comments and questions are meant to inspire more dialogue, both amongst the community and with Meridian Knowledge Solution’s staff and their 3PAO to help iteratively enhance their submission. You should not construe the comments or questions as negative feedback or a positive endorsement.

# Approach

## Static Website as Dashboard

We really appreciate your use of a static website hosted on GitHub Pages. This platform choice enables you to dynamically generate current statuses for indicators and metrics by processing machine-readable JSON files from the same source repository. We consider this a pragmatic approach, with a preference for maintainability over time. Do you intend to continue with this approach for future submissions. Is it efficient and scalable, or are there challenges to this approach?

## Git and GitHub for submission

We also appreciate your use of a git repository hosted on GitHub for describing validation methods for the KSIs, and methods in a structured JSON file over time to show not just the current state, but the history over time. This choice aligns well with the long-term philosophy of using the FedRAMP Key Security Indicators over time for continuous reporting. Do you see any challenges to this approach for short-term and long-term use? How will you manage the use of multiple automation tools and increased data added to the pipeline, with the CSP and 3PAO consuming the changing data, metrics, and resulting pass/fail/partial validation states for real-world use?

In this initial submission, certain sensitive evidence files and data are not included in the submission, and a reviewer is expected to open an issue. Is that the intention for future submissions? How will the CSP send the information requested in the issue? How will it scale for increased demand if you use this method for future submissions?

## Failure Conditions

We also wanted to commend for itemizing a clear set of failure conditions to identify when you and the 3PAO determine a key security indicator is not met. If one or more conditions is not met but other criteria for the validation methods are successful, is that automatically a validation status of fail or partial?

# Implementation

## Digital Signing

We also support your use of digital signatures for confirming the integrity of artifacts. Your use of PGP keys with automated commit checking with GitHub Actions workflows is unique. We would like to fully understand the intent of the commit signing and how it signifies the 3PAO’s assessment: only some of the commits are signed by the 3PAO, but many commits are signed by a CSP user’s signing key (the default GitHub key; more on that below) and actions-user for automated GitHub Actions workflows. Both the CSP user and actions-user use the default PGP signing key GitHub defaults to for all users using the web interface, [web-flow.gpg](https://github.com/web-flow.gpg). How should FedRAMP reviewers and third parties interpret the signature of CSP commit after a previous commit specifically signed by a 3PAO staff member’s custom PGP signing key? What does it mean if the 3PAO does or does not sign commits with KSI validation results for statuses of either failed or partial, not just true? What should it mean if the signature checking for the 3PAO signature in GitHub Actions fails but the data is already merged in a published branch the CSP and 3PAO share with FedRAMP for review? Some of these workflow considerations would help understanding the bigger context as part of a review.

## Transparency of assessment activities

This submission also has a strong focus on transparency. This transparency and detailed information about assessment methods made the submission easy to understand. It did seem there was less detail in the specific assessment activities and validation methods, however. Many of the validation methods were the review of documentation or artifacts about the system implementation (diagrams, tool output) and say “... requirement met.” The verification status summarizes that the 3PAO completed the validation, but not what was checked and how. The repo does not include sensitive artifacts, so it is hard to spot check the assessment. Will you provide more detail in the evidence references and the notes in future submissions for the 3PAO’s assessment activities? Are there challenges you would face with increasing 

## Data Format choices

The synchronized usage of JSON data and Markdown is very helpful and makes the submission accessible for different kinds of reviewers with differing levels of comfort in software development and data science. You did not provide a schema for the draft, so do you plan to publish a JSON schema or utilities for everyone to analyze the JSON data? Aside from that, do you have a mechanism to synchronize the Markdown and JSON data as they relate to one another? Will you continue to use it because it will scale well for a final 20x package, or would you do something differently?

We hope this feedback and the questions are constructive. We would love to hear from submitters and community participants as we move forward.

### Replies



#### Reply 1

author: [github.com/aburroughs](https://github.com/aburroughs)

url: [https://github.com/FedRAMP/community/discussions/16#discussioncomment-13342845](https://github.com/FedRAMP/community/discussions/16#discussioncomment-13342845)

created: 2025-06-02T11:53:37Z

id: DC_kwDOOxfoic4Ay5h9

@aj-stein-gsa Thank you for the thoughtful feedback. Below is our response by category:

**Approach – Static Website as Dashboard**

Yes, we intend to continue with this model. The static GitHub Pages dashboard, backed by machine-readable JSON, is lightweight, version-controlled, and transparent. It scales well and lets reviewers instantly see changes and validation logic over time without needing proprietary systems or middleware.

**Git and GitHub for Submission**

We intentionally designed our 20x submission around Git and GitHub to promote transparency, reproducibility, and automation:

    Git provides immutable, timestamped evidence commits and version control across all artifacts.

    GitHub Actions orchestrates the entire CLI validation lifecycle — from evidence collection, CLI execution, and assertion logic to failed report generation and PGP signing.

    Private repo manages real-time automation and protected data, while the public repo surfaces safe, signed outputs (e.g., pass/fail states, interpreted CLI summaries, failure reports) — enabling 3PAOs and FedRAMP to observe validation results without requiring privileged access.

To manage increasing automation complexity:

    We’ve introduced auto-generated per-KSI documentation using a script that combines logic from cli_assertion_rules_full.py, the unified validation results, and our CLI command register.

    CLI output is sanitized, truncated, and interpreted before publication to balance evidence transparency with security.

    A formal JSON Schema has been published for unified_ksi_validations.json to help 3PAOs and FedRAMP reviewers validate structure, tooling compatibility, and long-term integration readiness.

We believe this architecture not only meets current FedRAMP 20x expectations but sets a scalable foundation for future iterations, including machine-readable assurance exchange and continuous assessment.

**Sensitive Evidence and Issue-Based Access**

Initially, we supported issue-based review for static files. Now that evidence is generated through automated CLI queries, we only use issue-based requests to explain failed assertions. Sensitive data is sanitized by default and never pushed. Failed KSIs are published in failed_ksi_report_readable.md, with contextual CLI output. This keeps the review model secure yet actionable.

**Failure Conditions**

Our logic enforces binary assertions: pass (✅) or fail (❌), driven by deterministic Python functions for each KSI. Each rule parses CLI command output (raw_output.json), applies structured logic (e.g. checking for the presence of resources, counts, values, or status codes), and returns a clear True/False result along with a human-readable explanation.

Partial outcomes (⚠️) are technically supported but intentionally disabled for now. Our current CLI-based rules are scoped to validate discrete security outcomes with high confidence. Once we expand to multi-command or hybrid evidence (e.g. combining AWS CLI and Terraform), we may revisit partials with weighted logic or fallback detection.

This approach ensures reviewers and 3PAOs can reproduce, audit, and trust the validation result without ambiguity.

**Digital Signing**

We use commit signing across three trust tiers:

    actions-user for validated, traceable automation

    GitHub’s web-flow.gpg for CSP commits

    A custom GPG key for 3PAO-specific attestations

We’ll provide clear guidance in our 3PAO_ASSESSMENT_GUIDE.md on how signatures should be interpreted — including what constitutes 3PAO review vs. automation. Commits are timestamped, signed, and reproducible.

**Transparency of Assessment Activities**

To strengthen validation transparency, we auto-generate per-KSI documentation (docs/ksi_rules/) that includes:

    KSI description and requirement

    CLI command(s) executed

    Rule logic and pass/fail criteria

    Justification for evidence mapping

This addresses the feedback around “requirement met” phrasing by exposing the precise method of validation and what output is checked.

**Data Format Choices**

We now publish:

    unified_ksi_validations.json: full validation set

    unified_ksi_validations.schema.json: schema for 3PAOs, CSPs, and GSA to verify formatting

    failed_ksi_report_readable.md: markdown view of all ❌ validations

    docs/ksi_rules/*.md: per-KSI rulebook

    GitHub Actions trace and source CLI commands for reproducibility

We intend to continue using this model in future iterations, potentially expanding to:

    A validation_bundle per KSI (like a mini JSON + signature + markdown trace)

    Supporting OSCAL export or conversions for submission alignment

