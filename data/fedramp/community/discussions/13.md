# Metadata

title:FedRAMP 20x Draft Pilot Submission - Confluent

author: [github.com/lucashogue-confluent](https://github.com/lucashogue-confluent)

url: [https://github.com/FedRAMP/community/discussions/13](https://github.com/FedRAMP/community/discussions/13)

created: 2025-05-21T22:21:50Z

id: D_kwDOOxfoic4AgAH8



# Post

Confluent is announcing the intent to participate in the FedRAMP 20x Phase One pilot with our [draft submission](https://confluent.safebase.us/?product=default&itemUid=092e78c8-b5c8-4428-ac96-c357c088ef66&source=click).

This draft submission outlines our methodology and approach for capturing KSIs, along with several examples. The goal is to gather initial feedback, and to quickly iterate on any discussions by providing additional information to our draft. While we plan to include actual evaluations and evidence along with our final submission, these drafts currently contain placeholder and / or sample data. Our final submission will also contain all KSIs along with 3PAO validation.

We‚Äôve read through many of the discussions and share the same thoughts and questions as others:

1. With respect to continuous reporting, will there be another reporting mechanism for controls that are not easily validated by a programmatic method?

- Example 1: Change Management is implemented as a process where change requests are filed before deployments to production systems are made. How do we prove that we are continuously following a standard operating procedure?
- Example 2: We utilize 3rd party tooling for Asset Inventory. How do we prove that we are continuously using this tooling to keep track of components deployed in our production environment(s)? Are we expected to share a list for continuous reporting?

2. What will be the role of 3PAOs in FedRAMP 20x? Will they still generate documentation on behalf of organizations? Will they be giving their approvals on methodologies of validating controls?
3. Recently, OSCAL has been alluded to more frequently, but has not been officially recognized as the reporting format. Our draft submission details our flexibility of the format for continuous reporting, but will any format be officially recommended by FedRAMP?

# Comments




## Comment 1

author: [github.com/lucashogue-confluent](https://github.com/lucashogue-confluent)

url: [https://github.com/FedRAMP/community/discussions/13#discussioncomment-13226436](https://github.com/FedRAMP/community/discussions/13#discussioncomment-13226436)

created: 2025-05-21T22:25:18Z

id: DC_kwDOOxfoic4AydHE

@pete-gov sorry in advance for the JSON in PDF üòÖ (we are also working on getting a public GitHub repo set up)

### Replies



## Comment 2

author: [github.com/paulagosta](https://github.com/paulagosta)

url: [https://github.com/FedRAMP/community/discussions/13#discussioncomment-13248438](https://github.com/FedRAMP/community/discussions/13#discussioncomment-13248438)

created: 2025-05-23T15:44:23Z

id: DC_kwDOOxfoic4Ayie2

Lucas,

Thank you for being in the first groups of submissions and doing so in a open fashion.  The FedRAMP PMO appreciates the approach of starting with a high level framework, while providing two concrete examples (encryption and unsuccessful logins attempts) of how individual components could be assessed in an automated fashion in order to receive feedback and iterate on your submission.    

For feedback, we will focus on the Encryption example provided, vice the unsuccessful login attempts as it requires a more complex solution.

Just a quick recap of your logic.  You started out at the component level providing the macros that would continuously validate each instance of your RDS, S3 buckets, and EBS to ensure encryption was enabled.  From there you showed how you would be able to aggregate each individual validation, showing the totality of encryption across the enterprise in real-time.  Each validation is also tied to a control and KSI, so there is the option to visualize this multiple ways, which would be useful during Continuous Monitoring or as part of your normal assessments, driving away from point in time documentation.  

The below questions/comments are meant to spur dialogue amongst the community while providing feedback to iterate/mature the submission in discussion.  This should by no means be taken as negative feedback or an authorization/endorsement.  

**Schema Questions**

1. How does this model verify the totality of inventory? In your example there are validated checks to make sure your S3 buckets, EBS, and RDS clusters are encrypted, but how does the automated mechanism show that no other types exist within the boundary? Maybe validation of compliance code to inventory type becomes a new 3PAO task?  
2. How would you check encryption type in a scenario where there are multiple encryption types available?    
3. How do we add context to those data stores that don't pass the encryption check?  There is a huge difference on impact if it is the data store holding publicly available data vs the one data store with all the federal data.  
4. As you move into some of your other components, how do you deal with scan timeline differences?  For example, your encryption scans now run daily, but that probably isn't needed for scans associated with cybersecurity education, assuming you can pull this from your internal LMS.  Based on your questions posed, there may also be some manual validations and those would probably be at greater intervals of periodicity.  

**Reporting vs Informing vs Alerting**

Food for thought before I get to your questions if we put this in context of not only using this for the initial authorization assessment, but also for continuous monitoring purposes.  The way you have the report schema set up allows us to break the notion of establishing static reports that are then sent to stakeholders at regular intervals and move us more towards a dashboard approach.  This would provide the ability for stakeholders to monitor KSIs in near real time instead of waiting for the first of each month to see a snapshot in time.  Looking at your trust center it is already set up in a fashion to support this.  

If you take this concept one step further, you could even set up alerts based on stakeholders defined thresholds.  Instead of waiting on a monthly report to be delivered or checking into the dashboards every few days to see statuses, I could establish a threshold on each KSI for notification. For example, since the encryption scan is run daily as a stakeholder with 

During your submission, you asked three very specific questions.  Below is a recap and our answers.  

1. **With respect to continuous reporting, will there be another reporting mechanism for controls that are not easily validated by a programmatic method?**

There are going to be some aspects to the assessment that do need some manual input, but can still be reportable in machine readable fashion.  We accounted for this in our initial phase one requirements requiring an ‚Äúautomated validation approach for a significant portion of the FedRAMP Key Security Indicators (KSIs)‚Äú in our initial standard.  Based on the tools and processes implemented across each organization, the specific components that require some manual analysis will vary.

As with other aspects of this pilot, during the pilot we are looking for innovative solutions to address situations like this along with clear explanations of the reasoning and approach to the solution.

2. **What will be the role of 3PAOs in FedRAMP 20x? Will they still generate documentation on behalf of organizations? Will they be giving their approvals on methodologies of validating controls?**

This is a really excellent question that we have posed on other submissions discussion post.  We don't think the need for 3PAOs goes away, but the nature of how and when they play roles will need to adapt to whatever the final process becomes (as will all roles).  

For example, with the concept you propose, there has to be an impartial body to ensure those automated assessment parameters identified actually meet the KSIs (validating the process vice the output or control).  Who does this if not the 3PAO?

Given your above question about those mechanisms that cannot be programmatically validated, there will still need to be some form of impartial validation, even if it is manual and just reported in an automated fashion.  

This new process may bring the 3PAO in more along the lifecycle of the system, vice bringing them in heavy only at assessment windows.  

Again, this is an aspect of the pilot that we expect to learn a lot from based on how private innovation drives solutions, and how cloud service providers and 3PAOs align on where they can best support the need for clear assessments. It may be that this will vary in different situations based on how a provider implements a given KSI.

3. **Recently, OSCAL has been alluded to more frequently, but has not been officially recognized as the reporting format. Our draft submission details our flexibility of the format for continuous reporting, but will any format be officially recommended by FedRAMP?**

As of right now, there is no officially recommended format for reporting from FedRAMP.  During the phase one pilot, we have tried to relieve CSPs from traditional restraints in order to provide the maximum flexibility.    

FedRAMP will eventually define ongoing standards for this type of authorization to ensure maximum interoperability and reuse so packages eventually converge towards a standardized structure. Pilot participants will be expected to adjust their packages to meet formalized standards to maintain FedRAMP authorization \- this is why initial authorizations are only for 12mo.

It‚Äôs our expectation that as industry converges on the most effective approach here that it will be easier for folks to transition from their experimental pilot approach to a more standardized one.

We are looking forward to your iterations. 

### Replies



#### Reply 1

author: [github.com/robwittman](https://github.com/robwittman)

url: [https://github.com/FedRAMP/community/discussions/13#discussioncomment-13250257](https://github.com/FedRAMP/community/discussions/13#discussioncomment-13250257)

created: 2025-05-23T19:02:02Z

id: DC_kwDOOxfoic4Ayi7R

Thanks for the feedback @paulagosta! 

> 1. How does this model verify the totality of inventory? In your example there are validated checks to make sure your S3 buckets, EBS, and RDS clusters are encrypted, but how does the automated mechanism show that no other types exist within the boundary? Maybe validation of compliance code to inventory type becomes a new 3PAO task?

The vendor we‚Äôre working with to sync data emits metrics and logs related to the sync itself; mainly providers and tables that were synced, and records that succeeded or failed. Internally, we'll have monitoring for the sync processes themselves to ensure that it was able to get all of the appropriate data from the individual sources. As far as ensuring each source we need to evaluate is configured, we can verify those internally with ad-hoc validation, but that‚Äôs something that may lean on 3PAO?

At the moment, the sample macro(s) is only checking that encryption is enabled on the various data stores. It doesn't delineate between AWS managed keys, or customer managed keys, which ones are multi-region, etc. By "no other types", are you referring to other CSP services that may possibly store data and aren't being checked? Other datastore services (e.g. ElastiCache, etc..) are synced by default too, and will be pushed to our warehouse. The datastores used as examples in our submission are not an exhaustive list, and if there are any that need to be evaluated for a particular KSI, then we can easily add them to the compliance model.

The validation of those compliance models would probably be a 3PAO task, yes. We define what we are going to evaluate, how we're going to aggregate the data, etc. The 3PAO would then verify that our collection methods are complete and valid, and that our model evaluations correctly assess compliance. We‚Äôre also working on getting a public repository setup, and finding a path for making public the DBT models we‚Äôre creating. This way others with a similar schema could leverage them, and auditors, agencies, and the community can keep us honest.

> 2. How would you check encryption type in a scenario where there are multiple encryption types available?

At least for the encryption examples provided, AFAIK any encryption would be "passing" since they use KMS. That may not apply to other services / APIs, but in those cases we would have to codify that requirement in the model (which may be complex, but should be doable). One example might be ALB listeners, which could be ‚Äúencrypted‚Äù with AWS Managed Policy `ELBSecurityPolicy-2016-08`, but not be an acceptable policy in relation to KSIs. In this case, we could define a static list of accepted policies, and verify the value of the encryption settings exists in that list.

```sql
{% set accepted_ssl_policies = ('ELBSecurityPolicy-TLS13-1-3-FIPS-2023-04') %}
select
    'KSI-002' as ksi_id,
    'XXX' as check_id,
    'Valid ALB listener TLS policy' as title,
    arn as identifier,
    null as metadata,
    case when ssl_policy IN {{ accepted_ssl_policies }}.  # check ssl_policy for a listener against accepted_ssl_policies for KSI
      then 'pass'
      else 'fail'
end as status
from aws_elbv2_listeners
where protocol = 'HTTPS';
```
We‚Äôd also have to verify that HTTP listeners redirect to HTTPS, but again, just an example

> 3. How do we add context to those data stores that don't pass the encryption check? There is a huge difference on impact if it is the data store holding publicly available data vs the one data store with all the federal data.

At this stage, our intention is to run the compliance model against everything in our boundary. That generates tables for our baseline of all the passes and failures that exist. From there, we'll probably have a separate layer that handles exclusion or routing of those evaluations:

- Everything defaults to production and handling federal data, and must be in compliance
- AWS accounts A, B, and C are in a non-prod environment; create a ticket to resolve instead of "alerting"
- Service X, Y, and Z are in production, but don't handle federal data; create a ticket as well, but may be different severity or require an exclusion

Our overall goal is that anything in our GovCloud partition is configured in a way that satisfies the controls. If we identify individual cases where those aren't appropriate, some sort of exclusion list will be applied (and verified again by 3PAO). In regards to the example of data stores, we intend that our initial assessment to get authorized will include an inventory of all existing data stores. These will be validated by our 3PAO, and we will continuously report that their configurations remain compliant. For any net-new data stores that would be provisioned after initial assessment, we would consider that as a [Significant Change](https://www.fedramp.gov/rfcs/0007/) and consult our 3PAO to validate that the new data stores adhere to the same compliance requirements.

> 4. As you move into some of your other components, how do you deal with scan timeline differences?

The default mechanism we're going with is syncing every source, at least once daily. Systems that don't have frequent changes, also typically don't have a large amount of data. For completeness' sake, we'll continue to sync those on the daily schedule as well. That being said, if some sources had a different scan frequency, one option would be moving those to a non-partitioned table, and using the most recent result for that provider (verifying, of course, it is within some threshold of recency as well)

I can't speak to the dashboard aspect as we haven't discussed that much internally yet. But the ability to generate frequent, consistent, and automated scans opens up quite a few options for visualizing and reporting that information.

Thanks again for taking the time, I'm happy to answer any other feedback or questions you might have!



#### Reply 2

author: [github.com/iteuscher](https://github.com/iteuscher)

url: [https://github.com/FedRAMP/community/discussions/13#discussioncomment-13251210](https://github.com/FedRAMP/community/discussions/13#discussioncomment-13251210)

created: 2025-05-23T21:12:29Z

id: DC_kwDOOxfoic4AyjKK

A couple thoughts to share
> **Schema Questions**
> 
> 1. How does this model verify the totality of inventory? In your example there are validated checks to make sure your S3 buckets, EBS, and RDS clusters are encrypted, but how does the automated mechanism show that no other types exist within the boundary? Maybe validation of compliance code to inventory type becomes a new 3PAO task?

This will likely arise with every KSI validation evidence method. How can I be sure the method is covering the totality of inventory? I wonder if it will prove easier for the 3PAO to perform one assessment of the whole inventory (as code or by IaaS query) compared to the inventory the CSP claims. For example
- CSP: we only use S3, EBS, and RDS for data storage
- 3PAO checks via IaaC state or IaaS inventory query to make sure they are not using EFS, Aurora DB, etc 

This way each KSI validation check (is storage encrypted?) does not need to prove it is covering the totality of inventory, it just needs to prove it is covering the totality of the CSP's claimed inventory. For example
- CSP's KSI script does not need to show EFS and Aurora encryption status yield 0/0, instead they can show S3, EBS, and RDS encryption and the 3PAO having already validated that claim is comfortable with the script just checking those three data sources.

From the CSP perspective working to show evidence it would be easier to have one step of proving the inventory is what we claim it is and not have to include every possible option in the individual KSI validations. 

> **Reporting vs Informing vs Alerting**
> 
> Food for thought before I get to your questions if we put this in context of not only using this for the initial authorization assessment, but also for continuous monitoring purposes. The way you have the report schema set up allows us to break the notion of establishing static reports that are then sent to stakeholders at regular intervals and move us more towards a dashboard approach. This would provide the ability for stakeholders to monitor KSIs in near real time instead of waiting for the first of each month to see a snapshot in time. Looking at your trust center it is already set up in a fashion to support this.
> 
> If you take this concept one step further, you could even set up alerts based on stakeholders defined thresholds. Instead of waiting on a monthly report to be delivered or checking into the dashboards every few days to see statuses, I could establish a threshold on each KSI for notification. For example, since the encryption scan is run daily as a stakeholder with

I appreciate this perspective and think it is a great goal to move towards. I noticed the [Continuous Reporting Standard RFC](https://github.com/FedRAMP/rfcs/discussions/27#top) defines the update requirement as "Providers MUST update Key Security Metrics at least monthly." which is not quite real time but could easily tie into this idea of "alerts based on stakeholders defined thresholds. Instead of waiting on a monthly report to be delivered..." 

I also wonder if the alert process could work for notifying the 3PAO when there are new changes or parts to review which then connects to your comment here
> This new process may bring the 3PAO in more along the lifecycle of the system, vice bringing them in heavy only at assessment windows.

The only caution I have is that some CSPs may feel cautious about having security issues perform real time alerts and messaging to their agencies, customers and 3PAO. There is already often alert fatigue when an incident happens and having to deal with automated messages sent to all federal customers and auditors whenever a validation fails could make that issue worse. Plus if an AO is getting constant automated alerts from each authorized CSO they may ignore them. Alerts would have to be carefully tuned to reduce false positives. Enabling stakeholders to see trends and set their own thresholds like you mention would help with this. 

-- Isaac Teuscher (Paramify)







## Comment 3

author: [github.com/austinsonger](https://github.com/austinsonger)

url: [https://github.com/FedRAMP/community/discussions/13#discussioncomment-13413723](https://github.com/FedRAMP/community/discussions/13#discussioncomment-13413723)

created: 2025-06-09T20:56:11Z

id: DC_kwDOOxfoic4AzK1b

### Questions:
- How does Confluent ensure parity and consistency in control enforcement and evidence gathering across both AWS and GCP components?
- Is there a trigger-based model for re-running compliance evaluations after changes in configurations (e.g., IaC commits, pipeline updates), or is the cadence fixed?
- Will future versions enrich metadata fields to include timestamps, sources, or evaluation confidence? This would aid in human auditing.
- Are there any exceptions for egress or metadata push operations (e.g., using GitHub for macro sync)? Clarifying boundaries helps reviewers assess egress risk.


### Suggestions:
- Add a note on how observations classified as ‚Äúfail‚Äù are triaged, and whether DBT macros support exclusions or exemptions for known false findings.
- Consider exposing a read-only OpenAPI or GraphQL schema for stakeholders to directly query evidence within compliance boundaries (e.g., FedRAMP reviewers, AOs).
- Consider using OSCAL implemented-requirement sections to map KSI macros directly to FedRAMP control parts (e.g., AC-2(1), SC-12(b)) for better fidelity and future reusability.
- Add a section that maps KSIs to NIST SP 800-53 controls (including enhancements where applicable).









### Replies

