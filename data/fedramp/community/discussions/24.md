# Metadata

title:A Streamlined Approach to FedRAMP Continuous Monitoring

author: [github.com/wisecryptic](https://github.com/wisecryptic)

url: [https://github.com/FedRAMP/community/discussions/24](https://github.com/FedRAMP/community/discussions/24)

created: 2025-04-01T16:55:44Z

id: D_kwDOOxfoic4AgAK_



# Post

## Overview

This approach focuses on optimizing the current FedRAMP Continuous Monitoring (ConMon) process, emphasizing simplicity, transparency, and efficiency. Feedback is welcome as this is a preliminary framework.

## The Core Issue: Trust

Public sector agencies need to rely on CSPs (Cloud Service Providers) while maintaining transparency. FedRAMP's role is to monitor this trust, not control it. Prior attempts to centralize trust under FedRAMP reduced efficiency and clarity.

## Key Principles for Effective ConMon

These principles represent the value add of continuous monitoring:

1. **Where is the Signal?**
   - Focus on meaningful metrics that highlight real vulnerabilities.
   - Avoid excessive data dumps and unused form fields.

2. **What is the Real Impact?**
   - Go beyond high/medium/low ratings with summaries that show actual risk.

3. **When Will It Be Resolved?**
   - Show clear timelines for resolution and allow some flexibility for minor issues.

## Procedural Practices

These practices represent how to eliminate waste. By focusing on the value-add while eliminating waste, we can maintain a high level of effectiveness.

- **Clarity**: Agencies should easily see the system's threat status.
- **Simplicity**: Reduce the reporting burden on CSPs.
- **Decoupling**: Enable asynchronous input and review to improve efficiency.

## Suggested Practices

### POA&M

The main value of the POA&M is showing that issues are being regularly addressed and resolved. Reducing fields for almost all findings to identifier (automate), title (pull from scan), level, and resolution date, or something similar to this, would save time without losing value.

Other fields generally aren’t important unless a finding shows real impact. After violating a second SLA’s time, or for items that have identified impact by either agency or customer, it may make sense to add a more verbose plan for how the issue will be resolved and additional notes on the complexity of resolving it.

### Inventory

Inventory should be eliminated from the monthly cycle, and uploaded only with the annual package. Most CSOs today have a high level of ephemeral components. Inventories have many hybrid and blended components as well. The value of posting an inventory monthly is very low—and that inventory is truly a point-in-time snapshot, not a true representation of what is running at the current time.

### Raw Files and Scans

These should be eliminated from monthly uploads and available only on request for validating an issue. They provide little value in routine maintenance and increase CSP threat surface unnecessarily.

### STIG Work

There is a lot of emphasis on CM-6 configurations currently. Most STIG work is noise. While it may be an important metric, a simple rollup percentage and an attestation in the CSP that STIGs have been reviewed and appropriately applied should suffice.

### Deviation Requests

Most deviation requests are noise – they don’t have significant impact. Agencies and FedRAMP should accept by default, deny by exception. If they have questions or wish further information, this can be done with a simple, explicit response.

All findings, reports and requests should follow the same form. The RET has made progress in this respect — it is now a copy/paste of the initial information. The deviation request form should be deleted. If it can’t be done in the POA&M form, it should not be done at all.

### Continuous Monitoring Plan

Reframe as a checklist. Provide a base checklist with room for customization. This reduces unnecessary focus on things that don't matter.

## Significant Change Process

I'm going to start a new thread on this because there is a lot that can go into significant changes. At the core, I'd like to propose a four-level classification that should loosely guide decisions. This will only work if we focus on the least level necessary and rely on a strong continuous delivery process.

- **Level 1** – No communication needed outside of CM process. Standard maintenance, patching, CI/CD, software updates and versions.
- **Level 2** – Notify agency group before release. Applies to changes already within FedRAMP framework but outside normal CI/CD scope.
- **Level 3** – Notify agency at beginning of work, get feedback, and notify again before release. Applies to changes requiring additional FedRAMP alignment.
- **Level 4** – Full significant change process including signoffs, assessments, and reviews. Reserved for major changes like compliance level upgrades or changing core CSPs.

---

Ideally, this would eliminate monthly work to simply identifying open findings and focusing on addressing them instead of documenting them or anything else that doesn't focus on addressing them. A single document—the simplified POA&M—would be uploaded every month, and everything else would be validated during assessment.

---

**What did I miss?**

# Comments




## Comment 1

author: [github.com/sam-aydlette](https://github.com/sam-aydlette)

url: [https://github.com/FedRAMP/community/discussions/24#discussioncomment-12691594](https://github.com/FedRAMP/community/discussions/24#discussioncomment-12691594)

created: 2025-04-01T17:38:32Z

id: DC_kwDOOxfoic4AwaiK

@wisecryptic do you think that the draft [1 page report](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/blob/main/conmon_ksi_report.md) that shows trends aligns with your observations?  I have a few additional questions below:

1. From an inventory perspective, I understand your points about the problems with point-in-time deliverables, especially regarding ephemeral assets, but are you also suggesting that it's not important for CSPs to understand and track what is running in their production environment internally? If not, why not? And if so, what would be the problem with rolling that up into a security metric that demonstrates the CSP doing inventory management?
2. Similarly, from a deviation request perspective, point taken on risk adjustments and false positives. But what are your thoughts on operational requirements where agencies actually accept the risk of open findings? Should agencies be expected to accept risk on whatever the CSP deems "required" or do you believe there needs to be an opt-in review process for these types of DRs?

### Replies



#### Reply 1

author: [github.com/wisecryptic](https://github.com/wisecryptic)

url: [https://github.com/FedRAMP/community/discussions/24#discussioncomment-12756599](https://github.com/FedRAMP/community/discussions/24#discussioncomment-12756599)

created: 2025-04-07T21:32:06Z

id: DC_kwDOOxfoic4AwqZ3

Hi Sam, I'll take a good look at the 1 page report idea. High level, I think it definitely aligns with simplicity, I might want to see a little more deletion on it though.
1. You make a really good point. I agree, it is important for CSPs to understand and track what is running. I like how you said 'roll that up inot a security metric that _demonstrates_... ' the important thing is that they are managing it, not what is in it. maybe a good metric would be discovered assets not accounted for in a company defined baseline, and then annually have the assessor review the baseline itself and the non-baseline assets discovered report. But completely agree with your general thrust.
2. on operational requirements, I would say they would be expected to '_default_ accept the risk', that is to say, it is reject by exception. since an operational requirement is considered an open vulnerability before acceptance, and still kept as an open risk on the poam (open findings tab), simply changing the designation doesn't expose the system to additional risk. An agency theoretically should be monitoring the poam regularly and would be able to voice an objection to the classification of a vulnerability as an or, but the the impetus would be on them, not on the CSP. if they voiced an objection, the or could go from YES to PENDING until a yes or no decision is made. There is room for arbitrary non-helpful rulings from both sides, but at least this balances the equation to force agencies to actively engage if they have concerns.



#### Reply 2

author: [github.com/sam-aydlette](https://github.com/sam-aydlette)

url: [https://github.com/FedRAMP/community/discussions/24#discussioncomment-12766901](https://github.com/FedRAMP/community/discussions/24#discussioncomment-12766901)

created: 2025-04-08T15:59:39Z

id: DC_kwDOOxfoic4Aws61

Great points @wisecryptic thanks. Non-blocking ORs with transparency is insightful.



## Comment 2

author: [github.com/austinsonger](https://github.com/austinsonger)

url: [https://github.com/FedRAMP/community/discussions/24#discussioncomment-12707957](https://github.com/FedRAMP/community/discussions/24#discussioncomment-12707957)

created: 2025-04-03T01:49:20Z

id: DC_kwDOOxfoic4Aweh1

@wisecryptic @sam-aydlette

My current projects aimed at resolving these:
- https://github.com/Elevated-Standards/RampScout
- https://github.com/Elevated-Standards/TenableTrawler
- https://github.com/Elevated-Standards/InspectorGadget
- https://github.com/Elevated-Standards/Cloudits


### Key Points to Address

#### 1. Inventory Management: Moving Beyond Point-in-Time Snapshots
You are correct that traditional inventory uploads as part of monthly reports do not reflect the true state of a system, particularly for environments heavily utilizing ephemeral resources. However, completely eliminating inventory submissions may not be the best approach.

Instead, I propose the following:

- **Automated, Real-Time Inventory Collection**: CSPs should generate and update inventories automatically through API integrations with AWS, Azure, and GCP. This ensures that inventory reports reflect actual resources rather than static snapshots.  
- **Inventory API Endpoints**: Providing agencies with access to inventory APIs would allow them to retrieve current state information on-demand rather than relying on periodic submissions. This approach aligns well with the principles of decoupling and improving efficiency.  
- **Security Metric Representation**: Rather than eliminating inventories altogether, they could be rolled into a security metric that assesses how well a CSP manages and tracks their production environment. Agencies could validate inventory practices during annual assessments while allowing automated, API-driven updates throughout the year.

---

#### 2. Deviation Requests: Reducing Noise & Improving Clarity
I completely agree that the current approach to deviation requests generates significant noise. Accepting deviations by default and denying by exception could dramatically reduce administrative overhead.

- **Streamlining the Deviation Process**: Rather than eliminating deviation requests, consider simplifying the format to allow agencies to quickly identify whether a deviation is reasonable based on objective criteria. This can be managed through a simplified POA&M form, as you suggest.  
- **Risk Acceptance Transparency**: Agencies could maintain the ability to request additional information or justification but should do so only when warranted by specific concerns or patterns of non-compliance. This would allow the CSP to continue operations without undue friction while maintaining accountability.

---

#### 3. Simplified POA&M Reporting
Your approach to simplifying the POA&M by reducing it to essential elements (identifier, title, level, resolution date) is sensible and aligns with a broader goal of reducing bureaucratic overhead.

- **Risk Prioritization**: Incorporating a mechanism for prioritizing findings based on their actual impact (rather than just high/medium/low ratings) could further improve clarity for agencies and CSPs alike.  
- **Resolution Timelines**: Standardizing how timelines are communicated and tracked could provide more consistency across ConMon submissions. This could also be rolled into the simplified POA&M process you describe.

---

#### 4. Significant Change Process: A Tiered Approach
Your four-level classification for significant changes is a good framework. By establishing clear criteria for what constitutes each level of change, you can reduce ambiguity and improve efficiency. Additionally, this approach would allow CSPs to operate with more confidence when making routine updates.

- **Documentation Efficiency**: Allowing Level 1 and Level 2 changes to proceed without formal agency approval reduces unnecessary friction while ensuring that more significant changes still receive appropriate oversight.  
- **CI/CD Integration**: Incorporating change approvals into CI/CD pipelines could provide even more efficiency, particularly for Level 1 and Level 2 changes. This would require agencies to have confidence in the CSP’s automated testing and validation processes, but that trust can be built over time.




### Replies



## Comment 3

author: [github.com/cybersechawk](https://github.com/cybersechawk)

url: [https://github.com/FedRAMP/community/discussions/24#discussioncomment-12712848](https://github.com/FedRAMP/community/discussions/24#discussioncomment-12712848)

created: 2025-04-03T11:41:39Z

id: DC_kwDOOxfoic4AwfuQ

hey @austinsonger  I think we are on similar tracks.   If you already have a good cross-cloud inventory open source solution then that would save a lot of time (https://github.com/Elevated-Standards/RampScout).   See my discussion thread on starting with simple inventory.   Curious on your thoughts.   If that piece is done, there is a lot more to ConMon, but we could move on.   My hope is to find people in the community such as you, that would be willing to build out some very basic open source tools for small to medium sized businesses just getting started on FedRAMP.   There is very little out there right now so the entry barrier is very high and it is very expensive. 

### Replies



#### Reply 1

author: [github.com/austinsonger](https://github.com/austinsonger)

url: [https://github.com/FedRAMP/community/discussions/24#discussioncomment-12720652](https://github.com/FedRAMP/community/discussions/24#discussioncomment-12720652)

created: 2025-04-04T03:02:59Z

id: DC_kwDOOxfoic4AwhoM

Yeah the whole purpose I created this github organization [https://github.com/Elevated-Standards](https://github.com/Elevated-Standards) was to create projects that helps organizations with their GRC work. 



#### Reply 2

author: [github.com/cybersechawk](https://github.com/cybersechawk)

url: [https://github.com/FedRAMP/community/discussions/24#discussioncomment-12778246](https://github.com/FedRAMP/community/discussions/24#discussioncomment-12778246)

created: 2025-04-09T13:05:23Z

id: DC_kwDOOxfoic4AwvsG

hey @paulagosta / @ryan-hodges-gsa / @sam-aydlette , I am wondering if a good place to start would be to collect code and scripts that people are willing to contribute starting with inventory.  We have at least one community member here @sunstonesecure-robert that would be willing to contribute code for inventory gathering (see https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/17#discussioncomment-12756692) , @austinsonger has code and I will see if we can contribute as well.    

There are many areas to automate, but if we start simply with a collection of scripts that can meet with CM-8 inventory requirements, then we can build from there.  As a community, we could refine the scripts further by commenting on them or even starting to coordinate together as a team and extending the scripts so that they are easy for CSPs to use within their own environments and across for additional cloud providers. 

It would be helpful, I think, to see if additional people in the community would be willing to contribute and also to see if AWS and/or other hyperscalers could help as well since we are essentially pulling together inventory using their APIs but just what is required for FedRAMP and in a format usable for automation.

Hopefully this makes sense, and I am not jumping the gun.   If easier, we could wait for the next community discussion.



## Comment 4

author: [github.com/sunstonesecure-robert](https://github.com/sunstonesecure-robert)

url: [https://github.com/FedRAMP/community/discussions/24#discussioncomment-12756996](https://github.com/FedRAMP/community/discussions/24#discussioncomment-12756996)

created: 2025-04-07T22:39:43Z

id: DC_kwDOOxfoic4AwqgE

> Most CSOs today have a high level of ephemeral components. 

I think this is an argument *for* an automatic inventory.  Since it changes - sometimes within minutes - any report/dashboard/KPI that is *not* aggregating data based on the current state of the world is essentially obsolete when viewed (+/- some pragmatic caching rules of course).  

There can be point in time trend data on the KPIs with snapshots of the corresponding inventories for each snapshot, and that should all be done automatically.  

You can do this with simple pivot tables free nowadays...and with ATO'd solutions with more fancy analytic reports (fka OLAP now Lakes, etc) at lower cost every year...and of course open source in-boundary tools.  I'd always prefer using a dynamic, standardized (eg OSCAL), API-enabled inventory to report on ephemeral resources so I know the KPIs reflect the actual state of the world the bad guys will be attacking when I  go to sleep.

> they could be rolled into a security metric that assesses how well a CSP manages and tracks their production environment. 

Yes, and, be aware of [unintended](https://en.wikipedia.org/wiki/Goodhart%27s_law) [consequences](https://en.wikipedia.org/wiki/Hawthorne_effect). As a rule, like it or not, *all* CSPs could do a better job tracking their production environment.  And 3PAOs could do a much better job verifying the inventory (doe 3PAOs even know how to get the list of running containers in a K8s cluster? No 3PAO ever asked me for it in 5 years of FedRAMP. Let alone the Lambdas or Batch jobs...) 

So I agree with having the audit(or) periodically spot check that the automation is automating the right things, and that the QA process for the automation is testing the right things, and that there are no (intentional or otherwise) un(der)-reported accounts/VPCs/subscriptions/resource groups/etc.  

Still, I would always want to have the underlying data available to validate (by machine/code) that the KPIs match up to every single resource in the boundary for all controls. That's a relatively easy thing for computer code to do. We shouldn't put that on humans annually, or ever - that should be continuous and automated.



### Replies



#### Reply 1

author: [github.com/wisecryptic](https://github.com/wisecryptic)

url: [https://github.com/FedRAMP/community/discussions/24#discussioncomment-12903938](https://github.com/FedRAMP/community/discussions/24#discussioncomment-12903938)

created: 2025-04-21T22:57:50Z

id: DC_kwDOOxfoic4AxOYC

@sunstonesecure-robert to point 1, I like the idea of automated inventories, but I see two different objectives - how we gather data and when we report. an automated inventory can support when we report, but my main point is that by the time an agency has reviewed an inventory, it will never be the the actual state of the system. Thus automation = good, reporting = waste.



#### Reply 2

author: [github.com/atfurman](https://github.com/atfurman)

url: [https://github.com/FedRAMP/community/discussions/24#discussioncomment-12909781](https://github.com/FedRAMP/community/discussions/24#discussioncomment-12909781)

created: 2025-04-22T12:16:26Z

id: DC_kwDOOxfoic4AxPzV

@wisecryptic depending on the delay between inventory generation and review the system state may no longer be as represented, but I would not consider it a `waste` if it _accurately reflects the system state at the time it was generated._ Want to have a higher fidelity view of system state? Increase your refresh rate and reduce your review latency! If an inventory can be generated in minutes to seconds (we do this currently), there fewer technical barriers to reporting state much more frequently than the current monthly cadence. 



#### Reply 3

author: [github.com/sunstonesecure-robert](https://github.com/sunstonesecure-robert)

url: [https://github.com/FedRAMP/community/discussions/24#discussioncomment-12912199](https://github.com/FedRAMP/community/discussions/24#discussioncomment-12912199)

created: 2025-04-22T15:34:58Z

id: DC_kwDOOxfoic4AxQZH

We discussed on the CNCF WG today and the notion of a time-to-live "tag" came up.  So if the system can express what we gather to report and how long that should be considered valuable, does that help? The Trestle folks say they are using 5 min as an arbitrary line: "report" on any asset > 5 min lifetime, ignore all others. So that tells you about the ephemeral nature of cloud-native: 5 minutes is considered "long lived".  We also discussed the nature of reporting on "posture" or a point in time view of what the system capabilities and potential security is vs. actual security in the moment based on current operations vs. forensic review of an incident. Each have different needs - and so to restate the various opinions here maybe this is more precise:

- For "posture" (forward looking): a snapshot inventory is fine as it expresses the overall risks in aggregate
- For operations (in the moment): realtime dynamic inventory is useful and "reporting" on this is a "waste"
- For forensics/IR: being able to trace what assets/API calls were in context during a particular event sequence in some historical period of time needs a complete and accurate inventory (or I would call "log" - or dare I say ledger and open up calls for blockchain :O please no!)

All the above can be true - we can and should report an inventory for some use cases; we maybe can, but should NOT report an inventory for others. Bringing it back to the specific ask above ... 

> trust...maintaining transparency
> Inventory should be eliminated from the monthly cycle

I would say *I* myself would not trust any system that could not generate all of the above in the above cases. But maybe AOs can and will assume greater risk. And maybe at Low (vs. Moderate or High)

As for the vulnerability part - we ran out of time and that's on the agenda for the next WG call in 2 weeks...stay tuned 



## Comment 5

author: [github.com/sunstonesecure-robert](https://github.com/sunstonesecure-robert)

url: [https://github.com/FedRAMP/community/discussions/24#discussioncomment-12757070](https://github.com/FedRAMP/community/discussions/24#discussioncomment-12757070)

created: 2025-04-07T22:52:55Z

id: DC_kwDOOxfoic4AwqhO

> Most STIG work is noise.

I agree STIGs are a bit stale and very on-prem oriented. HOWEVER - not one CSP STIG findings review I have participated in has ever, ever, EVER been perfectly clean where we didn't find huge holes in the Windows and Linux configs. And most CSPs won't do it if you don't require the evidence. Sad but true.

> attestation in the CSP that STIGs have been reviewed and appropriately applied 

 I've literally gone through every Windows and Linux STIG dozens of times with different teams and had every single CSP team tell me about a "false positive" and then only to find they didn't really understand what the STIG item was about! Attestations are never valid.  By accident or confusion or ignorance, CSPs will *always* under-report hardening flaws and *always* over-report compliance. 

I will buy a steak dinner (or vegan dinner whatever) for anyone who can show me a CSP who has 100% accurately attested to their STIG FPs/DRs. I promise you I will find errors.

So I agree the STIG (or CIS benchmark, etc) has to be "contextualized" and might best be replaced with other vendor-provided or open source baseline checks. However, if you don't require some form of hardening scan/evidence submission that can be machine/code validated - there WILL be open holes that lead to breaches.  Guaranteed.

### Replies



#### Reply 1

author: [github.com/sam-aydlette](https://github.com/sam-aydlette)

url: [https://github.com/FedRAMP/community/discussions/24#discussioncomment-12766937](https://github.com/FedRAMP/community/discussions/24#discussioncomment-12766937)

created: 2025-04-08T16:02:02Z

id: DC_kwDOOxfoic4Aws7Z

Great points @sunstonesecure-robert about the need for contextualization. Do you or others have thoughts around how we can make the hardening requirements more flexible while maintaining effectiveness? Are STIGs appropriate if we provide adequate flexibility or is a different approach needed?



#### Reply 2

author: [github.com/cybersechawk](https://github.com/cybersechawk)

url: [https://github.com/FedRAMP/community/discussions/24#discussioncomment-12778418](https://github.com/FedRAMP/community/discussions/24#discussioncomment-12778418)

created: 2025-04-09T13:16:29Z

id: DC_kwDOOxfoic4Awvuy

I would suggest STIG/CIS config deserve discussion in their own thread (see https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/22) .   Automating compliance assurance for these settings is a major undertaking for many reasons and it is better to start with secure configurations baked in as part of your IaaC.   There are a couple of paths that could benefit from an open source approach.   The first path is to collect FedRAMP secure reference architecture as code.   This would help companies that are setting up their gov environment for the first time or that are looking to update their existing IaaC for fedramp compliance.   This approach helps bake in compliance so that infrastructure is secure from default.   At the OS level, there are many options, but putting together an OS hardening guide that draws on the best practices and resources found by the community could be helpful.   

The second path is simple reporting on STIG/CIS compliance for the purpose of quickly identifying settings that are out of compliance for initial preparations for assessment and then for assurance that new deployments do not slip through with incorrect settings.



#### Reply 3

author: [github.com/sunstonesecure-robert](https://github.com/sunstonesecure-robert)

url: [https://github.com/FedRAMP/community/discussions/24#discussioncomment-12809396](https://github.com/FedRAMP/community/discussions/24#discussioncomment-12809396)

created: 2025-04-11T23:45:46Z

id: DC_kwDOOxfoic4Aw3S0

> how we can make the hardening requirements more flexible while maintaining effectiveness? Are STIGs appropriate if we provide adequate flexibility or is a different approach needed?

Well - I guess I'll put on my CNCF hat and say - use containers - and [distroless containers ](https://github.com/GoogleContainerTools/distroless) at that. If you don't have the problem - you won't need a solution :)

Now for legacy infra - at the end of the day evaluating STIGs, SRGs, CIS, etc is essentially regexing some config, registry key, etc.  What does that really do? Not much - especially for all those STIG items that say "if you have a valid business reason ..." so then you need to have already defined your threat model and defenses and go get an OR from the AO to approve your DR. (I love my job :)

So - why not skip the middleware toil - start with the threat model for your legacy server (ASIDE: ok I guess AWS etc. doesn't consider servers "legacy", but I don't think we need to design the F/R improvement for the hyperscalers that fit for the rest of the 99.99% of us) ...and dare I say "legacy" VMs for those who haven't containerized...then sure you need to know that what you deploy is hardened and stays hardened.  Ansible, Chef, etc have been doing this for 10+ years. And the clouds have AMI/VM image tooling. 

So then the STIG evaluation becomes => check the IaC source code. Maybe MITRE has something for this already. I seem to recall something but the usual vendors probably do (or easily could).



#### Reply 4

author: [github.com/atfurman](https://github.com/atfurman)

url: [https://github.com/FedRAMP/community/discussions/24#discussioncomment-12829092](https://github.com/FedRAMP/community/discussions/24#discussioncomment-12829092)

created: 2025-04-14T13:06:59Z

id: DC_kwDOOxfoic4Aw8Gk

> > how we can make the hardening requirements more flexible while maintaining effectiveness? Are STIGs appropriate if we provide adequate flexibility or is a different approach needed?
> 
> Well - I guess I'll put on my CNCF hat and say - use containers - and [distroless containers ](https://github.com/GoogleContainerTools/distroless) at that. If you don't have the problem - you won't need a solution :)
> 
> Now for legacy infra - at the end of the day evaluating STIGs, SRGs, CIS, etc is essentially regexing some config, registry key, etc. What does that really do? Not much - especially for all those STIG items that say "if you have a valid business reason ..." so then you need to have already defined your threat model and defenses and go get an OR from the AO to approve your DR. (I love my job :)
> 
> So - why not skip the middleware toil - start with the threat model for your legacy server (ASIDE: ok I guess AWS etc. doesn't consider servers "legacy", but I don't think we need to design the F/R improvement for the hyperscalers that fit for the rest of the 99.99% of us) ...and dare I say "legacy" VMs for those who haven't containerized...then sure you need to know that what you deploy is hardened and stays hardened. Ansible, Chef, etc have been doing this for 10+ years. And the clouds have AMI/VM image tooling.
> 
> So then the STIG evaluation becomes => check the IaC source code. Maybe MITRE has something for this already. I seem to recall something but the usual vendors probably do (or easily could).

I wish that it were as simple as checking the IaC source code, but my experience has been that only a subset of CSPs have reached that level of maturity, and in most cases I have experienced, there are some uncontrolled variables between what is declared in IaC and what is actually deployed and running. As bothersome as it can be to stay on top of, I think there is some value in taking nothing for granted and assessing state as far downstream as it is possible to do. 

That being said, it has been my experience that the signal to noise ratio on STIGs is not particularly good (at least in the contexts I have direct experience with), and it has often appeared to me that the value is lost amid the noise of manual checks and actual false positives, so in practice it becomes an exercise in: 

> Measure as far downstream as possible, then spend a fair amount of time and energy writing justifications and explanations for FPs, ORs and a few VDs. Then, proceed to present that substantial list to reviewers, where it often appears to get handled as a checkbox exercise of `yup, you wrote something to fill each of the cells on the DR!` since the reviewers aren't necessarily equipped to really evaluate the technical merits of the justifications or the actual risks (if any) associated with the implementation. 

Is there actually much in the way of real security benefit to this approach? In my opinion, probably not for the majority of findings. Much of it is noise, and the handling of that noise is inconsistent between assessors and AOs.

But then there are cases where I think this _does_ have value, _but only if it is actually done and validated against the actual deployed infrastructure._ It is these edge cases where things can get interesting, and real risk lives. EOL databases on VM hosts that got missed from scanning and inventory, public images that aren't in the repository (and thus aren't scanned), undocumented "temporary" bastion hosts etc. 

One thing that has repeatedly been impressed upon me throughout my career is the persistent delta between the plan (whether it be expressed as IaC, a diagram, or narrative statements) and the actual implementation. I have not yet observed a system where the plan and the implementation match exactly, and I'd go so far as to say that the actual implementation is probably only going to asymptotically approach the plan even in the best managed systems. In my experience, **there will be deltas**, whether small or large, and it is in this space where I think that much of the value of ConMon exists. Its difficult to find things you aren't looking for however;  to identify and address these deltas, I believe we must attempt in all cases to measure and assess state as far downstream as it is possible to do. 



## Comment 6

author: [github.com/sunstonesecure-robert](https://github.com/sunstonesecure-robert)

url: [https://github.com/FedRAMP/community/discussions/24#discussioncomment-12757206](https://github.com/FedRAMP/community/discussions/24#discussioncomment-12757206)

created: 2025-04-07T23:15:49Z

id: DC_kwDOOxfoic4AwqjW

> Most deviation requests are noise 

If you mean "most deviation requests from CSPs are poorly written, poorly understood by the AOs, and more or less not detailed enough" - I agree.  

If you mean "well defined and evidence based deviation requests are not useful" - I disagree...a lot.

If you mean "agencies ignore them and don't really understand them" - I think it's a mixed bag.

You have to understand that at many (all?) CSPs, the compliance and security teams are under enormous pressure by the product/business/engineering teams to accept "garbage in".  Security does not raise share prices, nor get you promoted.  The real world fact is that - by design of incentives - CSPs will do as little as possible to meet the security bar - not because they don't *want* to do good security - but the share price, and "move fast...", and "do more with less" business mandates are just too powerful.  And in vendors, the product/engineering teams call the shots and will try to assert "this is a FP or a VD or an OR" without doing much, if any, due diligence.  And even when they do - they very frequently don't understand the available solutions (or their proposals are shot down by managers as too complex, not enough time, not agile enough, etc etc).

Engineers in CSP product teams typically understand algorithms, data structures, and maybe possibly, sometimes, systems and how they interact.  But usually they are tasked with very narrowly defined code chores with lack of systems-level responsibility, and are graded and rewarded on how fast they deliver code, maybe even code that passes SAST (and infrequently, IAST) scans...but *not* on how secure the whole system is. That gets delegated to the security team (or maybe even just ops) who have practically no influence on these engineering decisions.  If I had a Bitcoin for every time I heard "we can just define a WAF rule and ignore that"...

So a machine readable and verifiable deviation request can and should be required.  This is doable in OSCAL today. Happy to provide a demo open source implementation if desired.

### Replies



#### Reply 1

author: [github.com/sam-aydlette](https://github.com/sam-aydlette)

url: [https://github.com/FedRAMP/community/discussions/24#discussioncomment-12766964](https://github.com/FedRAMP/community/discussions/24#discussioncomment-12766964)

created: 2025-04-08T16:03:49Z

id: DC_kwDOOxfoic4Aws70

@sunstonesecure-robert or others do you think that industry style triage processes can cover "machine readable and verifiable" deviation management or do you think a framework needs to be provided in order to incentivize folks to "do the right thing"?



#### Reply 2

author: [github.com/mswilson](https://github.com/mswilson)

url: [https://github.com/FedRAMP/community/discussions/24#discussioncomment-13252093](https://github.com/FedRAMP/community/discussions/24#discussioncomment-13252093)

created: 2025-05-23T22:59:14Z

id: DC_kwDOOxfoic4AyjX9

@sunstonesecure-robert said
> Engineers in CSP product teams typically understand algorithms, data structures, and maybe possibly, sometimes, systems and how they interact. But usually they are tasked with very narrowly defined code chores with lack of systems-level responsibility, and are graded and rewarded on how fast they deliver code, maybe even code that passes SAST (and infrequently, IAST) scans...but not on how secure the whole system is. That gets delegated to the security team (or maybe even just ops) who have practically no influence on these engineering decisions. If I had a Bitcoin for every time I heard "we can just define a WAF rule and ignore that"...

I'm going to try to not be offended by this broad uncharitable caricature. Your description is not aligned with my own lived experience, or that of the teams that I'm personally familiar with. It is certainly true that capabilities of service providers vary, and what you describe may be true somewhere in the universe.

But when I do the work to clearly verify a false positive, propose a mitigation strategy that justifies a risk adjustment, etc. I'm only doing so when I have the skills and experience needed to do so in an [honest and trustworthy](https://www.acm.org/code-of-ethics#h-1.3-be-honest-and-trustworthy.) manner as a computing professional.



#### Reply 3

author: [github.com/sunstonesecure-robert](https://github.com/sunstonesecure-robert)

url: [https://github.com/FedRAMP/community/discussions/24#discussioncomment-13252935](https://github.com/FedRAMP/community/discussions/24#discussioncomment-13252935)

created: 2025-05-24T00:33:29Z

id: DC_kwDOOxfoic4AyjlH

I'm sure we see different universes - maybe small/med CSPs vs larger hyperscalers, etc.  

It's not meant to be uncharitable - in fact it's meant to _defend_ the impossible position that "some" developers face every day: do it fast, do it cheap, don't have time for security - until there is a breach and then, get all the blame.  I'm glad someone has a different experience- gives me hope! 

But ... the *weakest* link is the one the bad guys go for so are you SURE 100% of your vendors, and their vendors, and THEIR open source projects all have the same lived experience? Having audited much open and commercial source code - I know that's not the case.

I'm sure MOST have the best of intentions and WANT to do more security, take the time to test, think through all the possible attacks, get more/better analysis tools, etc etc - economic factors are agin them.  I agree that most everyone tries to be honest and trustworthy - but they are put in impossible positions. Sorry - that's my 30 years lived experience with everyone from Fortune <10 to the 2 guys and a dog.

And so I'll still put a steak dinner against any DR sheet and I guarantee I find some false FPs...and sure, to your point 99% not intentionally trying to be factually wrong ... but to my point the pressures of biz and time (and now layoffs) are forcing the folks who lack the time or training or skills necessary to take shortcuts.

Keep pushing back though! Love to hear that some folks get the time and resources to do it right!! My personal gov data is on the line so I'm grateful!!  



#### Reply 4

author: [github.com/mswilson](https://github.com/mswilson)

url: [https://github.com/FedRAMP/community/discussions/24#discussioncomment-13286903](https://github.com/FedRAMP/community/discussions/24#discussioncomment-13286903)

created: 2025-05-27T17:40:39Z

id: DC_kwDOOxfoic4Ayr33

> I'm sure we see different universes - maybe small/med CSPs vs larger hyperscalers, etc.

Even within the context of a very large organization (e.g., a "hyperscaler") you naturally find variation in capabilities and investments. In my past I've worked on some of the most security-critical parts of a large provider (lowest level virtualization technologies, firmware management, mitigation of novel CPU-level microarchitectural side channel and transient execution vectors, etc.). We went down to the CPU and hardware system design to build robust protections so that other teams don't have to be concerned about them.

> And so I'll still put a steak dinner against any DR sheet and I guarantee I find some false FPs

This is almost certainly true. Today's software systems are complex, and even experts will make mistakes when analyzing. There are also varying opinions among experts about what qualifies as a "false positive."

The volume of "findings" produced my most scanners often makes it impractical for many organizations to analyze each and every one, therefore the most commonly accepted remediation strategy for findings is to make a change to the software system, even when a finding is almost certainly a false positive. 

See [CVE-2023-45853 (9.8 CRITICAL)](https://github.com/madler/zlib/issues/868#issuecomment-2807804350) as a case study. Due to limitations in the software management systems commonly used today, if I have the `zlib` package from Amazon Linux 2 that is older than `1.2.7-19.amzn2.0.3.aarch64`, most scanners will identify a "vulnerability." This is merely because Amazon Linux packages the `minizip` sample library as a sub-package of the `zlib` source rpm, and [an update](https://alas.aws.amazon.com/AL2/ALAS-2023-2320.html) was produced to incorporate the fix. Note that _nothing included in the Amazon Linux 2 distribution_ uses the `minizip` sample library.

Because FedRAMP scanner requirements require CVSSv3 base scores present in the NVD to be reported as the "risk score", you end up with a "critical" finding that you have to remediate one way or another. What would the perfect deviation request look like for this particular finding, and how many times should it have to be independently produced by a CSP during the authorization or ConMon process?



## Comment 7

author: [github.com/sunstonesecure-robert](https://github.com/sunstonesecure-robert)

url: [https://github.com/FedRAMP/community/discussions/24#discussioncomment-12757219](https://github.com/FedRAMP/community/discussions/24#discussioncomment-12757219)

created: 2025-04-07T23:18:43Z

id: DC_kwDOOxfoic4Awqjj

> Reframe as a checklist. Provide a base checklist with room for customization.

any NIST 800-53 control that has a frequency has a parameter. So just extract all controls that have some frequency parameter and add it to the list of OSCAL assessment-results to produce automatically.  Need it be more - or less - than that? Happy to provide jq code that does that if folks want it.

### Replies



## Comment 8

author: [github.com/Bscudera9](https://github.com/Bscudera9)

url: [https://github.com/FedRAMP/community/discussions/24#discussioncomment-12765492](https://github.com/FedRAMP/community/discussions/24#discussioncomment-12765492)

created: 2025-04-08T14:13:40Z

id: DC_kwDOOxfoic4Awsk0

Agree with some, but partially disagree with inventory and raw scans. If there is no reviews, then of course they have no value being submitted each month. But with a reviewer they have and would continue to have significant value. That reviewer was the FedRAMP PMO but now could be the 3PAO or an Agency reviewer (if they have the bandwidth).

There are quite a few CSPs that do not maintain their flaw remediation practices between annual assessments. Without verification and issue identification on a monthly basis (only achievable by an independent validator looking at inventory and raw scan data), many CSP ConMon programs will lose quality over time and only be brought back to compliance in the prep period for 3PAO annual audit.

### Replies



#### Reply 1

author: [github.com/sam-aydlette](https://github.com/sam-aydlette)

url: [https://github.com/FedRAMP/community/discussions/24#discussioncomment-12766996](https://github.com/FedRAMP/community/discussions/24#discussioncomment-12766996)

created: 2025-04-08T16:05:43Z

id: DC_kwDOOxfoic4Aws8U

@Bscudera9 great points, what are some approaches you'd recommend for automated verification and issue identification? E.g., using OPA, Kyverno to enforce policies in CI/CD, etc.



#### Reply 2

author: [github.com/Bscudera9](https://github.com/Bscudera9)

url: [https://github.com/FedRAMP/community/discussions/24#discussioncomment-12767578](https://github.com/FedRAMP/community/discussions/24#discussioncomment-12767578)

created: 2025-04-08T16:51:35Z

id: DC_kwDOOxfoic4AwtFa

Perhaps would have to be a few options for solutions based on the make-up of the target system. Most CSPs will have at least some persistent systems (management tools, databases, etc) and then sets of ephemeral systems that are better managed left of production. A system making heavy use of Kubernetes may find value in Kyverno but it wouldn't help most other systems, at least not with supplementing solutions. 

I think the primary point I am trying to make above is first there needs to be a general set of outcomes for asset visibility and some open-source options for gathering / maintaining that information. Even if scans and validations are done in CI/CD there must be recurring checks of the deployed assets to monitor for change (or for old versions remaining). Those validations are part of creating an effective / accurate inventory (at least) once per month. 



#### Reply 3

author: [github.com/sunstonesecure-robert](https://github.com/sunstonesecure-robert)

url: [https://github.com/FedRAMP/community/discussions/24#discussioncomment-12809443](https://github.com/FedRAMP/community/discussions/24#discussioncomment-12809443)

created: 2025-04-11T23:57:08Z

id: DC_kwDOOxfoic4Aw3Tj

> only achievable by an independent validator looking at inventory and raw scan data

I've seen inventory sheets - all 3PAO audited MULTIPLE YEARS by different 3PAOs - that are missing whole accounts, VPCs, and even systems that a quick API calls to AWS Config or Azure Resource manager would instantly uncover. So why not just let the Agency have a read only IAM service policy that allows direct query of the inventory API and the Tenable/Qualys/etc Scan results and let them do the simplest of jq queries to match counts and types.

or graph databases - Bloodhound is one, many other open source tools using Neo4j and others.  if the independence part is to validate trust - then just hash and sign the inventory and let the auditor verify it.  I mean we don't even have to sign the files today when we upload to Connect so...



#### Reply 4

author: [github.com/Bscudera9](https://github.com/Bscudera9)

url: [https://github.com/FedRAMP/community/discussions/24#discussioncomment-12819157](https://github.com/FedRAMP/community/discussions/24#discussioncomment-12819157)

created: 2025-04-13T12:05:47Z

id: DC_kwDOOxfoic4Aw5rV

> So why not just let the Agency have a read only IAM service policy that allows direct query of the inventory API and the Tenable/Qualys/etc Scan results and let them do the simplest of jq queries to match counts and types.

Do you believe each of the 230 agencies with active authorizations have the existing resources and expertise to do this? I do not. Nor does every CSP use AWS or Tenable/Qualys which means process / scripts would need to be developed for different systems using different tools against different target assets.

I'm not saying its not worth working on, but i do think we as a group need to check each comment for words like "just", "simply", "quick" and consider how easy it is going to be in practice to implement a plan. 



#### Reply 5

author: [github.com/sunstonesecure-robert](https://github.com/sunstonesecure-robert)

url: [https://github.com/FedRAMP/community/discussions/24#discussioncomment-13252953](https://github.com/FedRAMP/community/discussions/24#discussioncomment-13252953)

created: 2025-05-24T00:36:38Z

id: DC_kwDOOxfoic4AyjlZ

no - I'm drinking the kool-aid of using new 20x style automation that this here community would gladly open source (or at least CNCF which I help with). I posted somewhere a script that was created  in 10 mins during one of the 20x calls so I offer up to do 230 different vuln tools - just send me a schema or dummy file example and I will post them all in a CNCF repo. Actually I'll write an AI Agent to take my first example and write them all :)  So yea - I think all 230 agencies can use the community to get all possible scripts. Send me example files - I'll send back code.

And [here's another example of generating ](https://www.youtube.com/watch?v=_oIoaW5i-xE)lots of "scripts" (ignore the k8s specific parts - it's just code) with AI - these 2 guys did 1000 - so I think I can do 230 :)


