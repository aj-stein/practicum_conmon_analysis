# Metadata

title:Rev5 ConMon Process Improvement

author: [github.com/ryan-hodges-gsa](https://github.com/ryan-hodges-gsa)

url: [https://github.com/FedRAMP/community/discussions/26](https://github.com/FedRAMP/community/discussions/26)

created: 2025-03-31T16:13:01Z

id: D_kwDOOxfoic4AgALD



# Post

1. What do you think are the major pain points with the current Rev5 ConMon process? 

2. What changes could FedRAMP make today to improve it?

# Comments




## Comment 1

author: [github.com/jsantore-cgc](https://github.com/jsantore-cgc)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12679259](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12679259)

created: 2025-03-31T17:02:39Z

id: DC_kwDOOxfoic4AwXhb

Copied from a comment I made in the Continuous Reporting that I thought might be useful here (wasn't entirely sure where it best fit):

I don't know if this is going too far afield yet, but we need to account for ephemeral assets (so to your point https://github.com/FedRAMP/continuous-reporting-cwg/issues/1 above). It's one of the reasons getting an inventory and mapping to scan results is tricky to do. (I'll go so far as to put on my 3PAO hat, and say inventory/scan alignment is historically the single biggest derailer for assessments).

I'd say if we can find a way to define unique static asset images (and I mean that not just in the container sense, but from a baseline config perspective), be able to map those images to an active inventory, and reconcile those to the vuln scans we're solve one of the biggest hurdles. Basically, some sort of a primary key that is not IP address or hostname, but that we can still use for automation (tagging, perhaps, but I'm not sure if the various scan tools can discover/track tags).

### Replies



#### Reply 1

author: [github.com/JosephScarzone](https://github.com/JosephScarzone)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12679703](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12679703)

created: 2025-03-31T17:43:32Z

id: DC_kwDOOxfoic4AwXoX

I agree that aligning inventory to scans is ridiculously difficult, and thus, the need for automation here is highly beneficial. The whole concept of trying to capture an inventory in an Excel document when it all sits in the cloud environment is really inefficient. Compliance-as-Code is the route to go for both the vulnerability scanning perspective and controls testing perspective. ;-)



#### Reply 2

author: [github.com/AnievesPANW](https://github.com/AnievesPANW)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12680002](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12680002)

created: 2025-03-31T18:15:56Z

id: DC_kwDOOxfoic4AwXtC

Inventory is probably one of the biggest painpoint that I've seen both internally as part of a CSP and externally as an auditor. Heavily containerized environments come to mind as specifically difficult to nail down an "accurate" inventory for at any given time when they change regularly. Then of course putting that data into a spreadsheet is the next problem. 



#### Reply 3

author: [github.com/Bscudera9](https://github.com/Bscudera9)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12680278](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12680278)

created: 2025-03-31T18:49:43Z

id: DC_kwDOOxfoic4AwXxW

Container inventory can be captured with just the image name and SHA hash of the image so in that manner it can be easier than ephemeral OS-style assets



#### Reply 4

author: [github.com/pete-gov](https://github.com/pete-gov)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12680456](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12680456)

created: 2025-03-31T19:11:19Z

id: DC_kwDOOxfoic4AwX0I

One of my first "wut" moments in gov was when someone explained to me that instances could not be added or removed in a massive IaaS enclave without a 30 day window for a human reviewed ticket to add or remove the IP address from a scanning list. 

I'm a strong advocate for replacing "give us scan results of all things" with "give us the aggregated results output of your automated scan process that appropriately scanned all systems in a timely manner during their lifecycle." Just prove that you deployed and operated secure images at all times or properly mitigated instances where they weren't secure. Very open to suggestions about implementing and documenting such an approach.



#### Reply 5

author: [github.com/Bscudera9](https://github.com/Bscudera9)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12680532](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12680532)

created: 2025-03-31T19:20:48Z

id: DC_kwDOOxfoic4AwX1U

The 'scan all things' has been the way to prove that. Containers it made sense to allow for image-based scanning due to immutability and opportunities to scan images outside of the boundary. The "proving it" is difficult and many CSPs are simply not effective at understanding their own tech footprint and keeping up with flaw remediation. 3PAOs show up once a year and find major risks that are missed with ConMon because the scope of scans or the reporting created by CSPs are inaccurate.

My main message is that continuous monitoring NEEDS to be addressed for the lowest-maturity CSPs but a lot of the talk here is about enabling the highest-maturity CSPs (via automation, enhanced trust, etc). Many CSPs, or prospective CSPs simply need to improve on the basics before they can graduate to a sampling or automated approach. Bad data in = bad data out.

That being said, it should be possible for 'gold images' to be scanned and allow for auto-scaling. Auto-scaling itself is already a very easy conversation for a 3PAO to have with a CSP when evaluating the vulnerability management breadth against the inventory. Containers and WebApps we already do this.



#### Reply 6

author: [github.com/jsantore-cgc](https://github.com/jsantore-cgc)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12680752](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12680752)

created: 2025-03-31T19:46:58Z

id: DC_kwDOOxfoic4AwX4w

1) I agree with @Bscudera9 for 'gold image' scanning.   I'd love to see that formally approved as a methodology.
2) The problem with some scan tools is that they also do discovery on a subnet range.  So your scan results have 10.0.0.1, 10.0.0.2, etc...
There is not necessarily an easy way to map that back to 10.0.0.1 is container image X, 10.0.0.2-10.0.0.5 are container image Y (autoscaled), etc.
3) Plus, this also grabs IaaS platform devices (e.g. RDS instances, load balancers, etc, that can't actually be scanned) and then your 'inventory doesn't line up'

As such, you almost need a human intervention to interpret the ephemeral/scaled IPs to map to a known good baseline image.
Because using IP as the primary key is the issue isn't relevant.

With containers, we scan the repo, and then just show that 'these images are deployed at time x.' 
With VMs, though, I like the gold image approach, and then show the items in the gold image structure align to prod deployment (also, it allows for a generic test tenant to be used for the pen test tenant-to-tenant vector)

The fact is, and I know this is sacrilege, but for a cloud environment, tracking one's 'running inventory' is less relevant than enumeration of unique component images instantiated at a given time.   This also aligns with one of my beefs with CM-8(3) (discovery of new inventory within 5 minutes), which seems to have come from the day of hubs and people plugging in laptops into a wall jack in a physical office.  It's less relevant in a cloud environment.

For whatever that's worth.



#### Reply 7

author: [github.com/cb-axon](https://github.com/cb-axon)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12681338](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12681338)

created: 2025-03-31T21:04:38Z

id: DC_kwDOOxfoic4AwYB6

The problems I've seen with that "golden image" approach, is how do you account for when teams inevitably deploy something "old" - even a few days old, or the latest version of the golden image didn't actually replace all of the old and the old ones are still running and live? The thought of this approach sounds great, but I'm contemplating how that will functionally "work" to ensure appropriate level of depth and coverage. Because even with the best pipelines, things fail and images aren't always replaced/updated. I came across this repeatedly as a 3PAO.
Also with the golden image approach, I often see suggestions that scanning through your pipeline at deployment is sufficient, but what if an image was last deployed 90 days ago and hadn't been replaced/restarted? I saw this several times as a 3PAO and I don't personally feel that accepting scan results from 90 days ago this month is sufficient to evaluate risk. There could be hundreds of vulnerabilities discovered since then.



#### Reply 8

author: [github.com/AnievesPANW](https://github.com/AnievesPANW)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12681567](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12681567)

created: 2025-03-31T21:40:10Z

id: DC_kwDOOxfoic4AwYFf

To Colton's point here, and I'm quite confident we've seen a lot of the same... issues.... I think ultimately we need to dig into the development pipeline before production and really get to a shift left mentality to fix a lot of problems that pop up in production environments. Beyond that there's a lot of CSP responsibility with balancing priorities with continuous monitoring. We bug our teams internally about CVEs and patching so much that they would rather hear about literally anything else but the only real ways that I've seen that work is fixing issues ahead of time and then dedicating resources to truly continuously patching and deploying in production. 

Part of the golden image problem that I run into though is underlying libraries that aren't always remediated even when the overall image is updated or hardened. We leverage a lot of GKE images and constantly are up against Go library CVEs that persist from release to release even though we're up to date on things. 



#### Reply 9

author: [github.com/vennemp](https://github.com/vennemp)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12691076](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12691076)

created: 2025-04-01T16:40:06Z

id: DC_kwDOOxfoic4AwaaE

The golden VM image scanning only works if you are constantly updating your VMs to use the golden image - a practice that is more common in containerized workloads - i.e., immutable host images. My take to address this would be approve a sampling methodology once and for all - instead of making it the 3PAO's responsibility to sign off on each implementation.  

Check list for sampling:
1. Are you building images monthly?
2. Are you scanning images at build time during the build process to ensure they are hardened and vulnerability free?
3. Validate you are not making additional changes to image post build (e.g., no userdata/bootstrap scripts).  
4. Demonstrate that all your VMs are deployed using the golden image and refreshed monthly.
5. (Extra credit?) Do you have guardrails to prevent unauthorized images? SCPs/GCP Org Policies?

If you aren't using immutable hosts, e.g., traditional VM workloads.  Then you should be scanning the VMs directly.



#### Reply 10

author: [github.com/kamamanh](https://github.com/kamamanh)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12692623](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12692623)

created: 2025-04-01T19:30:25Z

id: DC_kwDOOxfoic4AwayP

There is a lot of baked in assumption that container=immutable in the language being used.  Immutability is not a guarantee and there should be assurances in place that containers are not being changed after launch. 



#### Reply 11

author: [github.com/vennemp](https://github.com/vennemp)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12692900](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12692900)

created: 2025-04-01T20:09:11Z

id: DC_kwDOOxfoic4Awa2k

That's fair - but I was speaking about VM image scanning - e.g., the worker nodes.  

However, to your point, I've used container runtime monitoring and alerting to enforce container image immutability in some environments.



#### Reply 12

author: [github.com/Bscudera9](https://github.com/Bscudera9)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12694115](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12694115)

created: 2025-04-01T23:42:56Z

id: DC_kwDOOxfoic4AwbJj

With all these approaches it is important to increase level of validation/assurances that unmonitored images or older versions are not active. Existing CM requirements with high expectations of visibility against the assets live at any time are necessary. I've audited CSPs who do sampling methodologies and its the same concept. Based the methodology off of https://www.fedramp.gov/assets/resources/documents/CSP_Vulnerability_Scan_Requirements_Using_Sampling.pdf

Overall this is one of my biggest concerns with 20x though. Many people don't quite understand just how messy prospective (and some current) FedRAMP CSPs are. The "just automate it" crowd needs to come to terms with some companies simply not having a grasp of what is active and not being reliable to get good breadth/depth of coverage for whatever the automation solution might be.



#### Reply 13

author: [github.com/atfurman](https://github.com/atfurman)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12700433](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12700433)

created: 2025-04-02T12:52:12Z

id: DC_kwDOOxfoic4AwcsR

Yes, I believe that is the crux of the problem- if we bake assumptions like `the CSP has processes to ensure only current VMs/container images are deployed; trust us that ad hoc changes aren't sneaking in somewhere` into the inventory and scanning piece, then we have a load bearing element that cannot be easily assessed in an automated fashion. 

Put simply, if the inventory isn't accurately representing everything that it needs to (this, to my mind is a fundamental issue that needs to be properly addressed and which I have attempted to articulate in https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/15) and everything on the inventory isn't scanned using the appropriate scanner(s), then how much value does automated evaluation of inventory and scan results actually bring? 

If the rules for `we don't need to scan this` are going to be put in place, then I'd argue that they must be expressed in a machine-friendly way. For example, in the AWS context for VMs this could look something like the following:
1. All VMs must be inventoried
2. All VMs must list their image as the "baseline configuration"
3. All VMs must be directly scanned UNLESS
- The VM is younger than $PARAMETER - probably something like a few days to a week AND the VM was launched from an image that was created within the last 30 days AND that image is in use by at least one other VM on the inventory that does have scan results. 
- OR the VM hosts an appliance which no scanner supports. How this would get expressed is an open question, but the case would need to be handled. 

Getting and expressing this information as inventory is relatively straightforward, but the validation could get complex, as different types of resources would need explicit rules rules written.




#### Reply 14

author: [github.com/sunstonesecure-robert](https://github.com/sunstonesecure-robert)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-13075038](https://github.com/FedRAMP/community/discussions/26#discussioncomment-13075038)

created: 2025-05-08T12:24:09Z

id: DC_kwDOOxfoic4Ax4Je

> The "just automate it" crowd needs to come to terms with some companies simply not having a grasp of what is active and not being reliable to get good breadth/depth of coverage for whatever the automation solution might be.

I submit that those who cannot, or are not ready to automate don’t belong in the cohort for 20x (or arguably Fed in general, but put that aside.) the bad guys automate everything so we best do the same.



#### Reply 15

author: [github.com/Bscudera9](https://github.com/Bscudera9)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-13075192](https://github.com/FedRAMP/community/discussions/26#discussioncomment-13075192)

created: 2025-05-08T12:31:56Z

id: DC_kwDOOxfoic4Ax4L4

> > The "just automate it" crowd needs to come to terms with some companies simply not having a grasp of what is active and not being reliable to get good breadth/depth of coverage for whatever the automation solution might be.
> 
> I submit that those who cannot, or are not ready to automate don’t belong in the cohort for 20x (or arguably Fed in general, but put that aside.) the bad guys automate everything so we best do the same.

Well, sure.. we're making the same point.. but I am focused on the HOW do we consistently identify those that are not ready / do not belong? I am not seeing enough of that. And I am seeing a lot of comments that make it seem easy to automate this, or comments that make it seem like almost all CSPs are ready to do this, or comments that dismiss that CSPs may choose to skirt the rules if they can avoid being "identified as not ready"



#### Reply 16

author: [github.com/pete-gov](https://github.com/pete-gov)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-13078788](https://github.com/FedRAMP/community/discussions/26#discussioncomment-13078788)

created: 2025-05-08T16:39:25Z

id: DC_kwDOOxfoic4Ax5EE

> Well, sure.. we're making the same point.. but I am focused on the HOW do we consistently identify those that are not ready / do not belong? 

This whole pilot and incremental adoption process is intended to blaze the trail for those folks. If successful, folks that aren't ready now will get access to host-provided features, third party tooling, and standards with proven execution models to follow. 



## Comment 2

author: [github.com/cb-axon](https://github.com/cb-axon)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12679598](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12679598)

created: 2025-03-31T17:33:51Z

id: DC_kwDOOxfoic4AwXmu

POA&M versioning and feedback:
One of the biggest challenges is managing point-in-time spreadsheets. Tracking comments across multiple versions, syncing changes, and ensuring everyone is working from the same file creates unnecessary overhead and confusion.
Suggested improvements FedRAMP could make:
FedRAMP should move away from static, Excel-based POA&Ms and instead support the use of live "trust portals." These platforms should allow agencies to view vulnerabilities, track history, and comment directly in one place. It would streamline collaboration, improve transparency, and reduce version control issues. Excel exports could still be an option, but the portal should be the source of truth.


Risk trigger thresholds don’t scale with system size or risk:
The current static thresholds (e.g., 5 overdue Highs, 10 overdue Moderates) don’t reflect the complexity or size of different CSP environments, especially with container scanning overwhelming vulnerability numbers. A small system and a large, complex one are held to the same standard, regardless of actual systems in use.
Suggested improvement:
Risk triggers should scale based on the number and type of assets, and nature of the system. For example, use percentages of overdue vulnerabilities relative to total findings at that risk rating, and factor in asset classification (e.g., internal-only vs. internet-facing). This would better align risk reporting with real-world impact.

### Replies



#### Reply 1

author: [github.com/acloudcj](https://github.com/acloudcj)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12679757](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12679757)

created: 2025-03-31T17:49:23Z

id: DC_kwDOOxfoic4AwXpN

@cb-axon  Some great points already raised here—especially around ephemeral assets, inventory/scan misalignment, and the limitations of spreadsheet-based POA&Ms. These pain points come up constantly, and it’s clear the community’s ready for a better path forward.

I wanted to surface a related discussion from the [Continuous Monitoring CWG](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/13), where folks have been exploring an idea that dovetails nicely with what’s being discussed here: implementing a security immutable ledger.

The basic concept is to move away from static, one-off artifacts—like spreadsheets—and instead shift toward a real-time, verifiable stream of control evidence. Think: a cryptographically-backed ledger that logs control adherence over time, across systems, in a way that’s tamper-evident and audit-ready. It’s not just about logging—it’s about enabling CSPs, 3PAOs, and Agencies to make faster, more informed decisions based on consistent, trustworthy signals.

There’s a deeper dive into the concept outlined in these blog posts, that were shared in the other working group discussion but I wanted to link them here:
	•	[Part 1: Why FedRAMP Needs a Ledger](https://www.knoxsystems.com/blog/fedramp-ledger)
	•	[Part 2: Scored Control Adherence](https://www.knoxsystems.com/blog/fedramp-ledger-part2)
	•	[Part 3: A Path Forward](https://www.knoxsystems.com/blog/fedramp-ledger-part3)

This kind of model could help anchor ephemeral assets to a known state, track drift or control enforcement over time, and reduce the chaos of constantly versioned POA&M spreadsheets or risk thresholds that don’t scale across modern architectures.

Curious what others think?



#### Reply 2

author: [github.com/AnievesPANW](https://github.com/AnievesPANW)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12680055](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12680055)

created: 2025-03-31T18:22:29Z

id: DC_kwDOOxfoic4AwXt3

I agree 100% with @cb-axon and actually called out the same pain points in another thread. It's already a pretty massive pain to nail down inventory of ephemeral assets, let alone track inventory and POAMs for them in excel spreadsheets. Many CSPs, if not all by this point I'd imagine, have internal dashboards and monitoring that enable more real-time oversight on these items. Creating a spreadsheet for this is cumbersome and is almost immediately out of date the second it's created. 

Vulnerability to asset ratio paints a much more realistic picture of the security posture, as well as internal vs externally facing assets. Current FedRAMP guidance treating all H/M/L equally is also an issue as it's simply untrue in a lot of cases. Treating all CSPs with a one-size fits all approach is a recipe for disaster, especially with vulnerability thresholds. 



#### Reply 3

author: [github.com/Bscudera9](https://github.com/Bscudera9)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12680854](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12680854)

created: 2025-03-31T20:00:39Z

id: DC_kwDOOxfoic4AwX6W

Colton/Alex - i'd be happy to work with you both on proposed solutions for the risk triggers. It is less about # of assets and more about # of unique asset types. All other things equal, a CSP with 1000 RHEL hosts is going to have fewer unique CVEs than a CSP with 5 Windows, 5 RHEL, 5 Ubuntu, and a handful of different container images.

This impacts CSPs that add new feature offerings the most, especially those that are from a different development team or especially an acquisition as the underlying dependencies do not match existing asset types.



#### Reply 4

author: [github.com/AnievesPANW](https://github.com/AnievesPANW)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12681500](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12681500)

created: 2025-03-31T21:28:14Z

id: DC_kwDOOxfoic4AwYEc

Hey @Bscudera9! I'd be happy to collab on these to see what we can make sense of for the future of ConMon. It's always an ongoing struggle within PANW as business pushes for constant feature parity and new bells and whistles. Competing priorities internally mixed with the very rigid requirements of FedRAMP it causes quite the headache. 



#### Reply 5

author: [github.com/Bscudera9](https://github.com/Bscudera9)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12692931](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12692931)

created: 2025-04-01T20:14:11Z

id: DC_kwDOOxfoic4Awa3D

I could see the thresholds and the reporting to agencies all rolling up into a better monthly (or even more continuous) report. We could look at how collaborative ConMon is meant to be managed via https://www.fedramp.gov/assets/resources/documents/FedRAMP_Collaborative_ConMon_Quick_Guide.pdf





#### Reply 6

author: [github.com/sunstonesecure-robert](https://github.com/sunstonesecure-robert)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-13075352](https://github.com/FedRAMP/community/discussions/26#discussioncomment-13075352)

created: 2025-05-08T12:41:49Z

id: DC_kwDOOxfoic4Ax4OY

> Curious what others think?

You can do without a ledger for version 1 at least. Having provenance and Byzantine fault tolerance is all fine, but not the immediate problem. Crawl first: any machine readable API payload > docs. 

Walk next: I’m all for Bayesian crypto fun, but let’s just do pivot table level analytics on unsigned data first. I think this is where most CSPs and agencies will tap out, but we will be better off than today. 

Run: layer on the TEEs, ZKPs, splines, GNNs etc and go whole hog. Especially for High. 

Eventually I would like to see ‘Trust Coin’ where good boys and girls get paid for doing all the things, paid by the neglectful kiddies who pay “gas” penalties for every violation. But let’s get to 80% goodness first. And huge Prometheus fan but also see [fine print](https://prometheus.io/docs/operating/security/#external-audits).



## Comment 3

author: [github.com/sunstonesecure-robert](https://github.com/sunstonesecure-robert)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12679804](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12679804)

created: 2025-03-31T17:54:35Z

id: DC_kwDOOxfoic4AwXp8

There is a lot of useful discussion and transparency on ConMon calls (at least in my experiences) that is not formally required/captured in Rev 5 templates but should be added in (via OSCAL or other means): 

- overall trends and "directionality" of the security posture
- discussion of SCRs, annual review prep and after-action, staffing, schedules on important POA&M items
- controls that have been reviewed outside of RA-5 and evidence from these reviews (eg AU controls, SI controls, etc)
- updates to the threat model, attack model, risks that are not necessarily vulnerabilities in OS or software but in processes/algorithms/posture
- impact of operational or supply chain risks to control implementations 
- all that an much much more on top of also reviewing the full impact and exploit ability of a specific CVE/findings in a complex system across the myriad deployed components, versions, tenants, stacks, multiple clouds, etc. that comprises all but the simplest CSP environment. Today that is at best ad hoc- at worst - the agencies don't know what that should look like or even interpret the Excel files provided.

In short, far more data, analysis, and nuance is needed in ConMon than just vulnerability Scan results + inventory, assuming the goal is to truly replicate the conversations and details that are discussed on the live calls to really assess the ongoing control status and security posture of a given CSO. As an Agency AO - I would certainly want maintain if not increase the precision and accuracy of that whole-system+process+activity situational awareness - with automation and much less labor and cost.

### Replies



## Comment 4

author: [github.com/JosephScarzone](https://github.com/JosephScarzone)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12681037](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12681037)

created: 2025-03-31T20:25:31Z

id: DC_kwDOOxfoic4AwX9N

To Ryan's question #2 - What changes could FedRAMP make today to improve it? ----- I think we will all be pretty consistent on question #1 comments, automation is needed. And so, when I think about Q#2 regarding ConMon improvements, you can't help but think about the entire spectrum of the intent of CWGs 1 thru 4 before making a response to this CWG. The theme is "continuous and automation". And so, we need to structure ConMon improvements in alignment with continuous monitoring, automating assessment, reporting continuously, and applying existing frameworks. Looking down at the problem statements from a 10,000 foot level, the recurring theme is evident, and so determining what that really means is obviously our mission here, but it will be easy to quickly dive down in a tangent, that very well could be relevant, but it will be a tangent, and so I encourage to first nail down the foundational elements that can allow us to achieve the 4 goals of each CWG. And when I say foundational, I truly mean the control set and structure to manage it, and more importantly, present the results in a manner that can be reasonably used to ascertain the security risk posture of any CSO at any moment in time. 

With that said, it makes sense to re-evaluate the stance of how we implement the NIST controls. Right now, it's nitty-gritty control-by-control down to the most minute level, which naturally creates a level of inefficiency in and of itself. So, I like the concept of creating KSIs but feel we should do this at the Control Domain level. Carrying on this notion, I'll re-affirm that yes, I am calling for a completely different perspective on how we assess/evaluate/report on controls.... not by the control level, but by the Control Domain level, i.e. Access Control (1 domain...you can even call it one control if you like) that will contain KSI's that allow for automation to be defined and implemented by CSPs.  18 control families....18 control domains. Now we track 18 "items" instead of 1000. We should evaluate each control domain, define the themes and KSIs for that domain. This allows us to not just get rid of a bunch of controls, but fine-tune to be laser-sharp focused on the things that really matter the most, and NOT DUPLICATE. This also doesn't mean that testing a control domain would come across as being less detailed or in-depth. But tracking 18 items instead of a 1000 is a lot easier for us to aggregate and in my opinion automate most of what we want to govern to ensure a strong security posture. I'd rather have 20 KSI's as part of the Access Control Domain, all contained within 1 domain, all tested within 1 domain, all described within 1 domain. This approach starts to tackle the elephant in the room (the SSP and all that comes with it), coupled with my thoughts in the next paragraph in how we present these things. 

It would also be remiss not to be afraid to talk about the importance of Agency consistency (and in fact Industry consistency) in how results are reviewed and accepted. Many folks have expressed to me their concerns about going back to a "wild, wild, west" scenario, which won't help matters either on all fronts. And so, establishing some semblance of standards on how CSOs can support and report on continuous monitoring results of their offerings is of utmost concern. Presenting domain descriptions, results and obtaining approvals should be standardized in a workflow management tool that is super easy to use, yet effectively efficient to ascertain where a CSO is at security wise and guide all users through the various review/approval steps (seamlessly lol). 

Long way of saying, I think we are forced to talk about NIST 800-53rev5 and re-evaluate how we ascertain compliance to it. I've been an auditor for over 22 years, Audited using most frameworks, FedRAMP, PCI DSS, ISO27001, HITRUST, SOC, SOX, CobiT, ITIL, etc, etc, etc. LOL The current methods we use are...kind of...outdated and I think the world is ready for a refreshed and automated vision, letting technology do most of the labor for us, instead of us spinning our wheels day in and day out. Simplify, but still be in-depth, and automate as much as possible. :-)

### Replies



#### Reply 1

author: [github.com/AnievesPANW](https://github.com/AnievesPANW)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12681530](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12681530)

created: 2025-03-31T21:33:24Z

id: DC_kwDOOxfoic4AwYE6

What success have you seen with the automation piece? It seems like there's dozens of GRC tools that promise all sorts of things but I'm curious in your experience what you've seen that has been helpful? Feel free to not mention specific software if you don't want but maybe the types of automation that provide the biggest positive. 



#### Reply 2

author: [github.com/JosephScarzone](https://github.com/JosephScarzone)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12681659](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12681659)

created: 2025-03-31T21:58:06Z

id: DC_kwDOOxfoic4AwYG7

My perspective I share is from an "auditor" point of view. I've seen some auditing firms go with testing scripts that pull configurations of assets, but most are stuck in manually auditing whatever control set they are auditing against, i.e. FedRAMP, PCI DSS, etc. Very little automation used there on the auditing end. However, worthy to be noted, on the CSP side, absolutely lots of different ways to "scan" for something, depending on the use, depending on the technology at hand. That's why the execution of automation should be in the hands of the CSPs to assemble together and then aggregate up the results. Where I see this process going is to define the standards that requires automation, from a compliance perspective, as well as guardrails on what to report and where. This opens up allowing the CSPs to automate what is required to be automated by the FedRAMP program, and report on the KSI's that are required to be reported on. However, I just want to be clear, when I think of ConMon in this new approach, it doesn't just relate to vulnerability/compliance scans themselves but also control testing results from the NIST rev5 control set. Simplifying the control set concept, to me, helps set the stage for establishing the KSI's per Control Domain. Yes, we have to nail down what should be measured for scans, but scans are just a piece, though big piece, of the puzzle. 

Not sure this answers your question specifically. I'm not fond of any tool on the market per say...to me, to get the automation we need, we should code for it. I always love to say, "Give me a handful of developers, and I'll change the world!" LOL...I truly mean that in this case. We should set the "requirements", let the developers figure out how to code it (driven by CSPs), and let technology do all the hard work for us. For me, I think this means that the standards call for automation and continuous monitoring/reporting. Change the standard, the innovation will come. :-)



#### Reply 3

author: [github.com/atfurman](https://github.com/atfurman)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12682359](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12682359)

created: 2025-04-01T00:23:42Z

id: DC_kwDOOxfoic4AwYR3

@JosephScarzone I could hardly agree more. I come at this from the perspective of a guy who used to do ConMon grunt work (started as an analyst; still have the mental scars from manual nessus scan & inventory review and reporting) and is now engaged in automating as much of that away as humanly possible so that humans can focus on the high value work as opposed to manually uploading files, looking at spreadsheets, and filling out templates.

I'm deeply suspicious of GRC tools; my experience has thus far been that they fall short of their promises, force awkward usage patterns, and often make life worse on the ground, not better. 

What _has_ made life on the ground better are open source tools such as [steampipe](https://steampipe.io/) that allow us to easily and repeatably ask the questions `what is?` and `what are its attributes?`, then express those answers in JSON for further partially automated analysis and update. Unfortunately, at current time no matter what we do with automation, the results have to be flattened into the FedRAMP templates and shipped off for someone to manually review (or at least say they did). 

What I'd love to see from this are standards- not `use these tools` but rather `answer these questions, prove (to some acceptable level of confidence) that you have answered them accurately and completely, and structure the answers according to this open standard`. 



#### Reply 4

author: [github.com/SusanSwanson](https://github.com/SusanSwanson)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12749996](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12749996)

created: 2025-04-07T11:50:44Z

id: DC_kwDOOxfoic4Awoys

While I agree with your philosophy on grouping controls, I don't believe that organizing them solely by Control Families will achieve our goals in terms of how we approach, architect, implement, and assess these controls. Control Families span across multiple teams; for example, Access Control involves not just the Identity and Access Management team, but also Cloud Engineering, Architecture, and Endpoint Management teams, and is linked to Supplier Risk.

To address the questions above:

1. What do you think are the major pain points with the current Rev5 ConMon process?
2. What changes could FedRAMP make today to improve it?

    - Lack of Automation
    - Lack of FedRAMP-authorized GRCs that support all Cloud Environments and FedRAMP templates (AWS, AzureGov, Google, etc.)
    - Alternatively, provide a GRC system to either Federal Agencies and/or CSPs so they can upload their FedRAMP package directly into a system instead of using templates.
    - Training and better support/services for Federal Agencies to ensure they understand their responsibilities as authorizers and can easily address issues with CSP packages, approve POAMs, accept risk, etc.




#### Reply 5

author: [github.com/ajay-stratus](https://github.com/ajay-stratus)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12855043](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12855043)

created: 2025-04-16T13:31:58Z

id: DC_kwDOOxfoic4AxCcD

@atfurman I'll vouch for Steampipe, it's a great tool.

There is so much variability in architectures, configurations, and processes that GRC tools, automation tools, etc. promise everything but end up either having huge gaps or a high implementation LoE.



## Comment 5

author: [github.com/JosephScarzone](https://github.com/JosephScarzone)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12681748](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12681748)

created: 2025-03-31T22:11:20Z

id: DC_kwDOOxfoic4AwYIU

I think the concept of Significant Change needs to be revisited. I've seen many other control frameworks keep Change Control and all types of change control including significant change should be left in the hands of the CSP and allow them to follow their own internal control mechanisms to manage the implementation of it. Having to run by every significant change of a CSO to an Agency or even multiple agencies is a bit overkill, and I'm being polite. Taking a new approach to ConMon via continuous monitoring and automated testing of designated KSIs should suffice. If Change Management is out of control in a CSP, you're going to see lots of issues in testing/scanning, and so the intent of managing risk is ultimately covered. 

I may be over-simplifying on purpose. Though Change Management is a key control, don't get me wrong, but from my vantage point, we have bigger fish to fry. Do us all a favor and walk away from having to get approval on every significant change. 

### Replies



#### Reply 1

author: [github.com/KHFedR](https://github.com/KHFedR)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12782684](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12782684)

created: 2025-04-09T19:10:09Z

id: DC_kwDOOxfoic4Awwxc

This would be extremely helpful. We are consistently updating our product on the commercial side and we get the 3PAO to state that the change is a "paperwork exercise" only to have the agency require a sig change.  When we look at the cost of overhead, we sometimes may not justify the update for our FedRAMP product for something that may be useful, just wouldn't pull in revenue to justify the costs. 



#### Reply 2

author: [github.com/bajosephs](https://github.com/bajosephs)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12822531](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12822531)

created: 2025-04-13T22:45:12Z

id: DC_kwDOOxfoic4Aw6gD

The significant change process hasn't added the value originally intended. What I've seen are that too many controls are considered "impacted". We should empower the CSPs to build a robust security review process internally and have that process audited. CSO feature should not be held back by multiple layers of SAPs and SARs. Changes need to be released to ensure the latest product features are in hands of the federal customers quickly. This drives revenue for the CSP and allows the federal customer to use the latest features quickly. The CSP should continue to give the agency a "heads up" on changes by quarter for example, and the CSP and sponsoring agency should agree on a common definition of what is deemed "significant". If a new technology is deployed within the authorization boundary or there is a change to encryption, that is significant. However, if a new CSO feature leverages existing technology and an approved egress path, or if enabling this feature is a customer option, likely not significant. When controls are selected for SCR testing, these need to be streamlined. For example, if 5 new container images are brought onboard to ConMon this should be done by the CSP as part of their process, not using RA-5 as a control to be tested by the 3PAO with an additional round of vuln scans. This is overkill and does not add value. If the backup control CP-9 is not changing, then it should not be tested by the 3PAO as part of the SCR. Backing up is the CSP's process and is similar to onboarding new container images for scanning. The CSP must be able to demonstrate that its own house is in order during the annual. 



#### Reply 3

author: [github.com/sunstonesecure-robert](https://github.com/sunstonesecure-robert)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-13075494](https://github.com/FedRAMP/community/discussions/26#discussioncomment-13075494)

created: 2025-05-08T12:49:46Z

id: DC_kwDOOxfoic4Ax4Qm

That’s why I would flip the script - change is the only constant, so don’t track “change”…that’s like trying to count water molecules go over Niagara Falls. If we run automated red teaming and show we cover 90%+ (for low, more for higher trust baselines) infra and app pathways, with no successes, just track that one metric. To me that’s better than any other metric cuz that’s what the bad guys (and accidents internally) are literally doing 24/7



## Comment 6

author: [github.com/Telos-sa](https://github.com/Telos-sa)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12687879](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12687879)

created: 2025-04-01T12:14:43Z

id: DC_kwDOOxfoic4AwZoH

### Major Pain Points
**Cohesive, explicit, robust, standards that reduce the subjectivity of the assessment and ATO process.**

There is wiggle room for interpretation in the current Rev 5 process, which leads to delays.  **Solution**: OSCAL Automation, with Rules that can identify package errors.  This already exists, but CSPs need to use the validation features to confirm package meets standards.  

Because of this wiggle room, no package is filled out the same way, so 3PAOs and SCA's have to read the package and try to interpret what the CSP is saying.  There are always blank spaces or N/A's, things not filled out, or not filled out in the same way, repurposed tables, etc..  When switching to OSCAL, the rules are tighter and more clearly defined, but there is still a possibility of subjectivity, where CSPs have always done X, and want to continue to do X, even if Y is the actual standard. 

Which leads to the next bit:

**Formal Key Security Indicators (KSI) and enforcement of requirements are missing**.  
Because of the subjectivity of the packages, there is no guarentee that the information is complete, accurate accounting of the package.  If there are pre-defined KSI, that come from industry standards and are (**Critical Piece:  Automatically testable in a scientific manner to confirm that the requirements are met**) Then CSPs can focus on the critical, and deliver these KSI's in a consistent manner.  As the process grows and changes, the KSI's may to, but as long as they are communicated up front, and the CSPs know what the standard is (and there is no room for interpretation, then executing the standards becomes... standard.  

From the example above, if doing Y would prevent package acceptance, and they KNEW it was going to prevent package acceptance, they are always going to do X.  There are many 800.53A objectives that can be automated, so determining the right ones, and which CSP provided tests meet the requirements is up to the PMO office to determine.  (Look at Azure, AWS, Google, Oracle) Built in scan features.  Can these be linked directly to the security objectives, and if so, what is the reliability that these are accurate (must be scientifically proven to become a standard). 

Finally:
**Level of effort to achieve ATO.**
CSPs are spending a lot of resources (money, people, time).  To create their package, and then spending more on re-writes and tweaks to meet subjective standards.  
If we know the KSI, then the level to achieve ATO should initially be reduce to **JUST the KSI**.  Think critical controls for each baseline, but really think about which ones are critical.  Should be technical, and able to validate from solution above.  Then build on 
Where we start with the 100 Critical Controls that MUST BE DONE and meet this standard for ATO (May be different for each baseline).  
This will save time and money.  Focusing on securing the required, and constantly reporting on the required, while building out the additional elements to get through stages of ATO.  (Could have different ATO tiers that reduce cost of entry).  (Like CMMC L1, L2, and L3). 

### Replies



#### Reply 1

author: [github.com/cb-axon](https://github.com/cb-axon)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12692483](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12692483)

created: 2025-04-01T19:10:45Z

id: DC_kwDOOxfoic4AwawD

While I agree with the theory, In practice even discreet KSIs are pretty subjective. Like taking Pete's point for an encryption KSI, the number of places that encryption is done is vast, and all collected differently. How would we "standardize" this when it can be so incredibly different at a CSP, and as systems change so do the monitoring mechanisms where CSPs would end up in an endless game of whack-a-mile to report on them; and validation that they are doing it comprehensively would still likely require 3PAO validation which as pointed out many times is highly subjective and depends on that 3PAO's technical ability to dig deep enough to ensure it's comprehensively collected. This may be easier at a small SaaS on a IaaS provider only consuming PaaS or SaaS services, but eventually most CSPs branch into more complex areas and start doing things in more than just those service configurations, so where do those gaps get discovered in this model?



#### Reply 2

author: [github.com/atfurman](https://github.com/atfurman)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12700818](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12700818)

created: 2025-04-02T13:21:51Z

id: DC_kwDOOxfoic4AwcyS

@cb-axon I agree- If KSI's are going to be useful, subjectivity must be removed- if they are going to be meaningful, all systems need to be generating them in the same way and essentially be consistently measured against the same objective benchmarks. I'm of the opinion that introducing subjectivity at any layer prior to the actual evidence provision and generation of the KSIs makes them fairly meaningless. 

If there are two systems deployed on AWS, and they have made different decisions about what types of resources get inventoried and how they get presented, then the KSIs for inventory and scans aren't really meaningful. For example:
- Organization A has decided (or been directed by its 3PAO) to inventory ALL VMs regardless of type. Its inventory is effectively a snapshot in time of whatever exists at that moment, and it does automated reconciliation between scan results and inventory whenever an inventory is generated. Unfortunately for Organization A, 40%  of its hosts have lifetimes measured in hours, whereas scanning is daily. As such, there are always hosts on the inventory which are not reflected in scans (even if those hosts are getting agents deployed and are scanned if they happen to be around when scans run). Unfortunate for Organization A- Its KSIs are going to suffer.
- Organization B has decided and been allowed to inventory only persistent VMs since it builds an image each month for its ephemeral VMs and showed **once during an assessment** that every ephemeral host was launched from an image built within the last month. Organization B then maintains an inventory of its persistent hosts and only scans those hosts. Its KSIs in this domain are perfect, but should they be?  I'd argue not- we have no way to determine from Organization B's reporting what their actual risk is- their inventory only loosely reflects reality. 

Nevertheless, in this scenario Organization B would have persistently higher KSIs in this domain than Organization A. As I see it, this is A Problem. If systems are going to be assigned a score that score should to be objective, and ideally should accurately reflect the state we are trying to measure. That means that the rules for collecting the information to generate that score need to be clearly defined and consistently applied everywhere. 



#### Reply 3

author: [github.com/JosephScarzone](https://github.com/JosephScarzone)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12701068](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12701068)

created: 2025-04-02T13:40:48Z

id: DC_kwDOOxfoic4Awc2M

I agree that KSI's should be very clearly defined and consistently applied. As a Certified Six Sigma Black Belt, I learned much about the importance of a clearly defined metric. It will make or break you. To @atfurman comments, spot-on insight. However, we will find that through time and lessons-learned, that adjustments will always have to be made in how we measure results and automation mechanisms, and so I'd encourage us as we define these "KSI's", build in the mindset right away that continuous improvement will and must occur. Defining the KSI's, believe it or not, will be the hard part, and should take some time for us to develop. I see this as a major deliverable of this forum, and suspect we'll spend a lot of time providing ideas and organizing an initial defined set of KSI's for all aspects of the assessment and monitoring processes.  



#### Reply 4

author: [github.com/Telos-sa](https://github.com/Telos-sa)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12729914](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12729914)

created: 2025-04-04T19:29:42Z

id: DC_kwDOOxfoic4Awj46

@cb-axon  you are correct that the collection is all done differently.  
The requirement would be that the reporting is all done the same.  That is the benefit of using OSCAL, and here are the different ownership schemas that I think would work. 
Here is what a good structured monthly submission may look like: [Assessing Monitoring Over Time - OSCAL Structure](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12690088)

@JosephScarzone Requirements are key.  FedRAMP needs to ensure they are explicit.  This may be one of the reasons we are moving off of Rev 5 so quickly and headed to 20x.  I provided feedback on what I think is still needed in the ksi's here: [Assessing Monitoring Over Time - KSI Review](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12689871)

## FedRAMP PMO - Defining KSI's and requirements 
- Define what data elements need to be included in the OSCAL packaging (like monthly submissions may only include Elements from SSP, and the POAM Model
- Establish KSI's for how the data will be interpreted. 
    - If requirement is to show cryptography, and how it is leveraged in the system, then need to establish all of the fields, and links that should be incorporated.  
    - Provide Thresholds for Pass/Fail of KSIs (low water marks across submission and monthly renewal
    - Share these requirements with CSPs.  
- Sharing current Package statistics with appropriate down stream stakeholders/consumers. 

## CSPs -  Normalizing the data 
- Formatting it into a computer readable structure (OSCAL)
I provided an outline to a structure that I think would work.  Yes, you may need to source from different elements in your boundary to collect all of the relevant information.  
- Using FedRAMP Automation Team provided CLI tool to validate Package, check for errors before submission
- Leverage GRC tools to report on KPI deficiencies, like with dashboards.  

## Agencies - Ingesting the Data and upholding FedRAMP Standards
- Leverage FedRAMP CLI tools to validate KPI submission, and completeness of data
- Perform approvals and ConMon (Probably in GRC Tool) - and Provide back to CSP:
    - Findings report (If CSP has KSIs in GRC, this is not a surprise) **Also, that was a lot of acronyms** ; )
    - Or, even better, OSCAL return with updated Observations, and Closed POAMS.
- Communicate To PMO and other Stakeholders on remediations/findings, Maybe also In OSCAL, so they can ingest). 



#### Reply 5

author: [github.com/cb-axon](https://github.com/cb-axon)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12730107](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12730107)

created: 2025-04-04T19:59:19Z

id: DC_kwDOOxfoic4Awj77

To be clear, I'm not disputing that subjectivity needs to be removed or even discussing the format in which these are reported. My comments are way deeper than that - HOW would CSPs even begin to collect this comprehensively, and conversely, how could 3PAOs even reliably validate that when there'd be millions of different ways to gather it and report on it constantly. 
I don't think its possible yet to standardize a KSI like "encryption in transit in use" because with everything growing, deploying, patching, changing etc - even the method of detection for one service like istio could change monthly, and then add in if Go, Python, Java, 100+ Azure Services, 100+ AWS services, and nginx are all in use - How would federal agencies get assurances that these are being checked thoroughly every x minutes without creating a more burdensome and manual 3PAO review process (that would still be point-in-time)?

From what I can tell, this would be a non-stop game of whack-a-mole for CSPs that have even moderately complex environments that is actually going to make the lift to achieve FedRAMP much, much greater. Not to mention it'd be a full time job even in those environments just to maintain that automation, even outside of all the other security work that we do. Sure authorization time might decrease once an assessment is submitted, but the amount of time it takes to put this in place before even procuring an assessment would be far greater. I'd imagine this would increase assessment time too. I just don't see the tools to comprehensively doing this existing right now, and even if they did, the word "comprehensive" is fleeting, because things are changing so much.

How would we address these types of technical concerns in this KSI approach? Most of the discussion has been about which KSIs to report on, and how to report them. But I don't seem to have seen any discussions (yet, I can't keep full tabs on all discussions if already started) of how to functionally even collect the data, which is pretty much the core requirement to even making all of this possible.



#### Reply 6

author: [github.com/aj-stein-gsa](https://github.com/aj-stein-gsa)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12730248](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12730248)

created: 2025-04-04T20:20:58Z

id: DC_kwDOOxfoic4Awj-I

> How would we address these types of technical concerns in this KSI approach? Most of the discussion has been about which KSIs to report on, and how to report them. But I don't seem to have seen any discussions (yet, I can't keep full tabs on all discussions if already started) of how to functionally even collect the data, which is pretty much the core requirement to even making all of this possible.

Well, the [Continuous Reporting CWG](https://github.com/FedRAMP/continuous-reporting-cwg) is next week, stay tuned. 😅  And I do not say that to say (as one of the advocates in that group): "Wow, FedRAMP has the answer and will unilaterally tell you!" In fact, it is very much the opposite. I do think there are industry approaches in other security domains on cross-org monitoring and data verification _we as an industry community_ need to think about. However, I see these topics and the collection, sharing, and verification of data as dreamed up in this CWG as some integrated higher level goal, much like you. My colleagues and I will engage across the CWGs for that reason.

I will say, at a high level, I feel the traditional FedRAMP approach was centralized or very hierarchical hub and spoke sharing of bulky data (and I say that vaguely, USDA Connect uploads anyone?) but that might not suit some of you. It might not suit all of you. So perhaps it is time for a new way of thinking? Again, I and others don't have an immediate answer today, but I think we can start from their and draw a shared vision together.

Hopefully we see you in all the CWGs, but have a shared concern and theme to future contributions on that front, believe you me. 😆 



## Comment 7

author: [github.com/cybersechawk](https://github.com/cybersechawk)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12688406](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12688406)

created: 2025-04-01T13:00:12Z

id: DC_kwDOOxfoic4AwZwW

_What do you think are the major pain points with the current Rev5 ConMon process?_
_What changes could FedRAMP make today to improve it?_

**Lack of Open Source Options**
I believe there is a lack of open source options for small to medium businesses to get started.   The process currently requires generation of multiple reports, so a business is faced with either trying to manage it all on spreadsheets, investing easily 250k+ in dev time building scripts, buying expensive tools which most often require a separate gov instance and higher gov pricing, or some combination of these three.   The company must also carry these high ConMon tooling costs plus gov cloud environment costs for many months and sometimes over a year before they are able to get through the process and deploy their solution to an agency to begin recovering the cost of their investment in FedRAMP.   

The net result is that it becomes a prohibitively expensive.   Some open source tooling would help fill in the gaps in ConMon reporting and greatly reduce the entry costs and time.   

**Spreadsheet Driven and not API / Data Driven**
There is a lot of time wasted in passing around spreadsheets and in document updates that could be better handled through automation and data transfer using secure APIs.

**Differences Between DoD and Civilian Agency Requirements**
It would be helpful to have DISA representation in this effort since for companies like our that intend to sell both to civilian agencies and DoD, the differences in current approaches requires significantly extra work.   It would be good to have DISA representation early if they can spare the time so that whatever is developed here will also work well for DISA assessments.

Significant Change Management
This is probably the most contentious area and the most difficult to address.   CSPs need to evolve their products rapidly.   With government getting smaller, it will be even more difficult for agencies to keep up with CSP changes and to review those changes quickly enough to avoid  slowing down the CSP and stifling their ability to innovate for customers.

I think step 1 is to focus on automating the required ConMon reporting.   This will provide the visibility that AO's need and identify issues / risks quickly before they become significant.

Step 2 could possibly involve certifying the software pipeline itself to ensure it identifies security risks before code is deployed and also the IaaC practices to ensure that any new cloud infrastructure goes appropriately STIG / CIS2 hardened starting on initial deployment.   

I think Step 2 needs more discussion and could be a thread on its own.   However, Step 1 is a bit more clear and could be started soon. 





### Replies



## Comment 8

author: [github.com/JeffWillis-MIS](https://github.com/JeffWillis-MIS)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12691395](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12691395)

created: 2025-04-01T17:17:11Z

id: DC_kwDOOxfoic4AwafD

SCRs. 

For an IaaS (JAB), that is a "black hole." As an IaaS (JAB), we are in a “black hole” when it comes to DRs, SCRs, AAs, SAPs, SARs, etc.  Agencies sit on top and inherit from the underlying IaaS. As a prior JAB, no agency has issued an authorization for the IaaS; their authorizations are specific to their particular and unique applications. They have no concern for the underlying IaaS; all they know and care about is it is authorized (JAB), and they inherit the controls. When there is a DR, SCR, AA, SAP, SAR, etc., who is the reviewer? Not an agency

How will SCRs be handled? In reading the FedRAMP Modernization Memo, it suggests that “…Once a CSO is authorized, the FedRAMP process should generally empower CSPs to deploy changes and fixes at their own pace, without requiring advance approval from FedRAMP or an authorizing official for individual changes to existing FedRAMP authorized products and services;…”

The SCR is a legacy process, and as @bajosephs talked about in another thread – excerpt below
"We should go towards this model and have the CSP's internal testing process and have the changes assessed by the 3PAO at the annual rather than testing each change. Allows CSPs to go to market quicker and supports revenue. We never heard as a FR community what happened to the SCR working groups that were supposed to simplify this process."

The legacy SCR process needs to be updated.


### Replies



#### Reply 1

author: [github.com/JosephScarzone](https://github.com/JosephScarzone)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12782094](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12782094)

created: 2025-04-09T18:12:48Z

id: DC_kwDOOxfoic4AwwoO

Agreed on all points, as I mentioned above in another thread. SCR needs to be addressed within CSP Change Management Process. If a CSP's change management process has been validated as effective, the CSP should be trusted to continue to follow proper due diligence to ensure their security controls are considered and maintained per change. 



## Comment 9

author: [github.com/cybersechawk](https://github.com/cybersechawk)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12692518](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12692518)

created: 2025-04-01T19:15:27Z

id: DC_kwDOOxfoic4Awawm

I agree @JeffWillis-MIS I think the SCR process needs a lot of work and is ripe for automation.

### Replies



## Comment 10

author: [github.com/austinsonger](https://github.com/austinsonger)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12707870](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12707870)

created: 2025-04-03T01:35:34Z

id: DC_kwDOOxfoic4Awege

@ryan-hodges-gsa @atfurman @vennemp @pete-gov @acloudcj @sunstonesecure-robert @cb-axon @Telos-sa @jsantore-cgc @cybersechawk @JosephScarzone @AnievesPANW @kamamanh @Bscudera9 @JeffWillis-MIS  

### **New Ideas for Improving Rev5 ConMon Process**


---

#### **1. Formalize KSIs Across Control Domains**  
Instead of evaluating controls individually, we should organize KSIs by control domains—essentially boiling down to 18 domains mapped to NIST 800-53. This approach ensures that we are not drowning in hundreds of individual controls but rather assessing them within the context of broader security objectives. This structure could streamline reporting, increase automation, and improve consistency across CSPs.  

- **Establish Standardized KSIs:** Define KSIs for each control domain (e.g., Access Control, Incident Response) with clear criteria and measurable outcomes.  
- **Align KSIs with NIST 800-53 Rev5:** Map KSIs to applicable controls in a way that supports automation and continuous monitoring.  
- **Continuous Improvement:** Establish a process for regularly reviewing and updating KSIs to ensure they remain relevant as technologies evolve.  

---

#### **2. Enhance API-Driven Evidence Collection**  
- https://github.com/Elevated-Standards
We need to shift away from spreadsheets and manual processes toward API-driven evidence collection. The goal should be real-time oversight rather than periodic snapshots of security posture.  

- **Standardized APIs:** Define API standards for evidence collection that can be adopted across CSPs.  
- **Live Trust Portals:** Implement systems where agencies can access real-time evidence and progress updates, rather than waiting for static reports.  
- **Improving Consistency:** APIs can help ensure uniformity in how evidence is collected and reported, making it easier for agencies to review.  

---

#### **3. Streamline Significant Change Reporting (SCR)**  
The current SCR process is cumbersome and often becomes a bottleneck. To address this:  

- **Automated SCR Approval Pipelines:** Use automated pipelines to evaluate the impact of changes and generate relevant evidence without manual intervention.  
- **Risk-Based Thresholds:** Instead of static criteria, establish dynamic thresholds for what constitutes a significant change based on impact, scale, and affected control domains.  
- **Unified Reporting Process:** Provide a standardized API for reporting SCRs to FedRAMP, ensuring consistency across agencies and CSPs.  

---

#### **4. Aligning Standards Across Agencies (Including DISA)**  
To minimize duplicated effort and inconsistencies, we need unified standards that can be applied across both civilian agencies and DoD (DISA).  

- **DISA Representation in CWG Discussions:** Ensure early involvement of DISA representatives in developing standards to avoid later divergence.  
- **Establishing Common Baselines:** Where possible, align DISA and FedRAMP requirements to minimize discrepancies and reduce the burden on CSPs.  
- **Flexible Framework:** Allow CSPs to tailor their implementations to specific agency requirements without departing from core standards.  

---

#### **5. Modernizing POA&M Management**  
GREAT NEW TOOL - https://github.com/NSWC-Crane/C-PAT

Current POA&M processes are spreadsheet-driven and inefficient. Instead, we should:  

- **Implement Live POA&M Systems:** Create systems that allow for continuous updates rather than static reports.  
- **Automate Reporting and Feedback:** Allow CSPs to submit POA&M updates via APIs, making it easier to track progress and provide feedback in real-time.  
- **Version Control and Comment Tracking:** Ensure that all updates are logged and easily accessible, eliminating versioning issues.  

---

#### **6. Focus on Automation and Continuous Monitoring**  
As mentioned by others, automation is essential to reduce the overhead of the ConMon process. CSPs should be empowered to build their own automation pipelines provided they meet the required standards.  

- **Encourage CSPs to Build Their Own Automation Pipelines:** Rather than mandating specific tools, we should define what needs to be measured and allow CSPs to meet those requirements through their own pipelines.  
- **Continuous Evidence Collection:** Transition from static reports to real-time evidence collection, allowing agencies to continuously evaluate CSPs’ security postures.  


### Replies



#### Reply 1

author: [github.com/kamamanh](https://github.com/kamamanh)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12728406](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12728406)

created: 2025-04-04T16:30:17Z

id: DC_kwDOOxfoic4AwjhW

I love the idea of standards and clear criteria and the move towards automation in general. I think there's some really good ideas above and it's laid out well. 

Only think I'd suggest is that we look as the above as the end goals, with some interim milestones along the way. For instance, the idea of defining KSIs across a domain rather than at the control level. I prefer the idea of defining at the control level to start allows CSPs to make small iterative steps towards the end goal that can be used both for ConMon and for assessments. If I have an automated way to prove a given control, I can use that as evidence to my 3PAO, as well as just keeping it in place year round for ConMon. If we're only doing at the domain level, then I'm having to cross walk generic domain KSIs to the controls being audited in a given AA, and I'm less likely to adopt those changes in the near future.  

My views are heavily colored by the fact that we're a small CSP.  Resourcing to build (and maintain - remember, there is always an upkeep cost) new automation, or manage a new tool, or pay for someone else to mostly manage a tool, is harder for me to achieve than some of the larger CSPs who have dedicated teams for these types of things.  For things that are nice-to-have like this, I'm often having to chip away at the mountain with a handpick.  



#### Reply 2

author: [github.com/AnievesPANW](https://github.com/AnievesPANW)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12844667](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12844667)

created: 2025-04-15T16:47:28Z

id: DC_kwDOOxfoic4Aw_57

I'll respond to each point below but TLDR version is that I agree with most of what's here as a goal to drive towards. 

**1. Formalizing KSIs across domains**
I agree with this in theory and it's something that I believe many 3PAOs and CSPs already do in an attempt to group things together to make it all make sense. What is currently missing is then translating this to audits, testing and deliverables that are specifically provided and required that are strictly control-based. If this is the end-state in mind then it will also require FedRAMP to either accept our own deliverables with this mindset in place or produce new templates for us to follow that adhere to KSI groupings, rather than SRTMs that list out every control. There is too much redundancy already across the NIST baseline where we end up answering the same questions 10 different times in the same way because it's called out in different controls or objectives. 
**2. API evidence**
I agree with the approach here as it makes sense to leverage this rather than taking dozens or hundreds of screenshots during audits to capture evidence of configurations and things like that. Obviously not everything can be covered in this way, but the more repeatable and efficient processes that can be put in place. the better. I think part of this will rely on what FedRAMP deems as "adequate" evidence so that CSPs can better align on what's provided rather than 10 different CSPs providing 10 different types of evidence. 
**3. SCR process**
Anything that reduces the time and burden of the SCR process is a win in my book. The SCR guidance is entirely too vague and can be interpreted to mean that just about anything is an SCR. There should be a lot more decision-making power within the CSP to determine this and if external verification or agreement is needed, it should not mean going through a full SCR audit and submission process. Personal aside, we've had to go through an entire SCR process to upgrade 7 VMs from RHEL 7 to RHEL 9 for EOL purposes which we were not permitted to actually implement for nearly 8 months because of audit + submission + review and approval timelines. 
**4. Aligning standards across agencies**
I don't know of the most feasible or realistic way to do this but it's necessary. Many of us work with several different agencies that have their own interpretations and requirements and there just simply isn't alignment across the industry of what's acceptable or not. Different agencies have different risk appetites and often will just disagree with the conclusion made by another agency review. What I'm quite concerned with right now is how the DoD or StateRAMP or any other body is going to change, or not, based on FedRAMP 20x. I'm not seeing any reason to be confident right now that the potential changes of the FedRAMP process are reflected in any way across other requirements that we have. So much builds off of FedRAMP and if FedRAMP changes, logic would tell you that others will also change, but to date we've been told that isn't the case. 
**5. POA&Ms**
POA&Ms suck. I get that they're necessary but having to create a spreadsheet for external review is not only a dated process but becomes a full-time job to maintain. Moving toward a more real-time dashboard or tracking of these would allow CSPs to focus on other, more important tasks, and would alleviate 3PAOs, agencies and the PMO from having to read through spreadsheet after spreadsheet. Getting away from spreadsheets in general should be the goal for the FedRAMP program. A lot of the POA&M fields can be automated and that's great, but some can't and those are really the areas that cause a lot of manual effort and maintenance, such as milestones. We've been seeing a larger emphasis on milestones specifically and are being asked to provide highly detailed and specific information for these and that is a very large ask. We have dozens of products working together and being asked to go to dozens of different teams, have them provide milestone updates and track progress is already a lot and we're being asked to provide even further details of headcount requirements, as well as costs associated to fixing these issues. 



#### Reply 3

author: [github.com/pete-gov](https://github.com/pete-gov)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12847376](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12847376)

created: 2025-04-15T22:04:12Z

id: DC_kwDOOxfoic4AxAkQ

>  POA&Ms suck. I get that they're necessary but having to create a spreadsheet for external review is not only a dated process but becomes a full-time job to maintain. 

This is a whole thing that deserves its own detailed discussion IMO. There's a great opportunity for folks in this working group to share best practices about how companies manage the documentation of planned remediation activities for both internal and commercial customers - government customers should be able to reuse whatever the business is using the track this for itself, and FedRAMP would be open to exploring a translation layer here.

Maybe [we should go all the way back to the original 2001 memo and use something that simple](https://georgewbush-whitehouse.archives.gov/omb/memoranda/m02-01.html).



#### Reply 4

author: [github.com/atfurman](https://github.com/atfurman)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12854449](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12854449)

created: 2025-04-16T12:43:32Z

id: DC_kwDOOxfoic4AxCSx

My perspective of the POA&M (and DR) is it is the spreadsheet format which causes much of the problem. I have seen countless hours spent attempting to make that spreadsheet match reality in some way, when the truth is its simply not a very good tool for the job once one is attempting to represent state for more than a few dozen findings. Add that to the fact that many of the conventions around its use [appear to be bound more by tribal knowledge than clear specification](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/18#discussioncomment-12843553) and yes, it really is a pain point. 

I won't pretend that we have solved it fully, but we have reduced the pain of dealing with it by.... not using it for tracking state. 

Instead, we track all findings from all sources in an internal issue tracker. Those issues are themselves created and updated by automation from integrated scanners. Humans are assigned finding issues, and document planning and activity in accordance with run books which specify exactly how they should be handled and documented. Issue metadata indicates its attributes including if it is a POA&M entry, has a DR, etc. These issues _are_ the POA&M - the spreadsheet is simply an arbitrary output we generate on demand from issue data.

As with other automation approaches, our approach here is somewhat bound by the conventions required in the spreadsheet, but in all cases we have attempted to distill down to `what actually matters` for engineers, capture that information in the lowest friction manner possible, and then translate that information into the conventions required by the FedRAMP template. An example of what this looks like in practice:

![image](https://github.com/user-attachments/assets/6524f28c-3562-478f-912f-6fa6bad517c7)

There is a fair deal that is out of frame on this, but the essence of it is:
- Automation creates the finding issue based on integrated scanner results and assigns the due date based on the earliest affected resource within the scope
- Automation continually updates the issue based on scanner results, including closing it if all affected resources are remediated
- Humans are assigned to the issue and collaborate in it alongside the automation, which posts comments and updates labels like any other user
- If the issue is going to be on the POA&M, it is documented and labeled according to strict convention requiring collaboration by the finding owner, analysts and ISSM. You'll note that in this case the actual justification was written in part by the automation user- we have source controlled justifications for common findings based on our standard deployments
- All this information is read and validated by our solution and flattened into the POA&M and DR templates for distribution




#### Reply 5

author: [github.com/jdettweiler](https://github.com/jdettweiler)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12854834](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12854834)

created: 2025-04-16T13:15:13Z

id: DC_kwDOOxfoic4AxCYy

@pete-gov - I'm a colleague of @atfurman and would like to point out that our approach encapsulates not only the POA&M, but all aspects of the authorization package. We're trying to move away from the idea of `point-in-time` and get to what represents the system state and then provide that state to meet the specifications that have been laid out. 

We've taken this approach with everything (e.g., inventory, ports, protocols, services, etc.) this also includes the system documentation, which is managed as code within the same repository as the POA&M, to dynamically generate a package upon request, as opposed to fighting the battle of maintain Word and Excel files over time. Putting this information closer to the source (i.e., managing documentation as code in the same repository where the engineers manage the IaC) allows for a more streamlined and efficient process to ensure that as changes are rolled out in the environment, those same engineers making the changes, are updating the impacted document components (e.g., update a component definition (which generate the implementation statements) when there is a change to a DB service). 

I don't know that the solution is to make the process `simpler`. I was around when that 2001 POA&M memo was still the `law of the land` and it was so broad and open for interpretation that it created more confusion than it solved. 

I'd like to see us move in a direction where the specification for everything is better understood and clear and allow the individual organization to report on those specifications using whatever format makes the most sense for them. FedRAMP could provide something akin to: 

```
POA&M Requirements - For each item on the POA&M, the following data, at a minimum must be represented: 
- Identifier
- Description
- etc., etc,. 
``` 
``` 
Inventory Reporting Requirements - For each component on the inventory the following data, at a minimum must be represented: 
- Identifier
- Component Description 
- Purpose
- etc.
```
The specifications need to be specific. For each Hyperscale provider, I'd like to see FedRAMP say something like `The following AWS Services need to be reported on the system inventory. The following GCP services need to be reported on the system inventory, etc.`

I know FedRAMP has always taken the stance that it needs to remain agonistic in these areas, however, I don't think these type of specifications would `cross any boundaries` and would only serve to provide further clarity. There is also a government precedent for this that was set in [M-21-31](https://whitehouse.gov/wp-content/uploads/2021/08/[M-21-31](https://whitehouse.gov/wp-content/uploads/2021/08/M-21-31-Improving-the-Federal-Governments-Investigative-and-Remediation-Capabilities-Related-to-Cybersecurity-Incidents.pdf)-Improving-the-Federal-Governments-Investigative-and-Remediation-Capabilities-Related-to-Cybersecurity-Incidents.pdf) where for the hyperscale IaaS/PaaS providers (i.e., AWS, GCP, Azure) the specifications around exact services that needed to be audited were provided. 

The more exact the specification, the easier it will be for organizations to report on those specifications using a format most suited to the organization's processes. I'd love to be able to provide an SSP that includes all the necessary information, without being locked to the Word SSP templates :)  





#### Reply 6

author: [github.com/pete-gov](https://github.com/pete-gov)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12884329](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12884329)

created: 2025-04-19T10:38:55Z

id: DC_kwDOOxfoic4AxJlp

@atfurman - We're trying hard to build a better awareness of the tribal knowledge problems (hence these CWGs!) so we can create updated information that is more accessible. It's a huge problem, especially when "wise sages" are sharing ten year old knowledge as ground truth even when it's no longer accurate or reasonable. Folks stop interrogating things after a while and everything becomes "that's just the way it is, you have to do it." That is a huge problem with FedRAMP.

> Instead, we track all findings from all sources in an internal issue tracker.

I love this and your example. 



@jdettweiler - Thanks for sharing these insights. As we explore the future of POA&Ms I agree that a specification approach is better than a spreadsheet approach, 100%.

> The specifications need to be specific. For each Hyperscale provider, I'd like to see FedRAMP say something like `The following AWS Services need to be reported on the system inventory. The following GCP services need to be reported on the system inventory, etc.`

The big issue is use case - we don't care about systems that don't handle federal information or likely impact CIA of federal information. If CSO A uses an S3 bucket to store federal information then it needs to be in the inventory, but if CSO B only uses S3 to store training videos or something then it doesn't need to be in the inventory.

Beyond that, we'd need hyperscalers to maintain information about services that could reasonably need to be reported on, rather than expecting FedRAMP to do that. I think most of them would be happy to do this as part of their service. 

Maybe the best practice can be something as simple as engineers using tags when deploying services to tag things that are inside the FedRAMP assessment scope or not based on how the eng team intends to use them. Then it's easy to generate an inventory of every asset that has the right tags?

I hope we can get some folks to explore this in our 20x pilot and maybe retrofit learnings to rev5 POA&Ms and assessment down the road. ;)





#### Reply 7

author: [github.com/atfurman](https://github.com/atfurman)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12898490](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12898490)

created: 2025-04-21T12:51:37Z

id: DC_kwDOOxfoic4AxNC6

Glad to hear that the goal is to move towards a specification @pete-gov; I'm keen to contribute and test these out as they are iterated upon.

However, as for the question of use case determining whether a resource is inventoried, I must differ. I'm of the opinion that its better to report _all_ instances of a specific type of resource within a certain context (probably an account/project/subscription) since the alternative is inimical to automation; a human would have to make the decision `this s3 bucket should be on the inventory because it stores federal data` (The question of what exactly constitutes federal data/metadata probably warrants a separate discussion since I have never observed a consistently applied definition there). 

If the inventory is meant to be machine readable, then listing 3 versus 50 S3 buckets on the inventory should not matter one bit; except in the latter case we _have confidence that we are seeing the full view of what is in this specific context_. If 47 of the buckets don't contain sensitive data? Fine, that should be indicated using a tagging specification! We already do something like this; its fairly straightforward:

```sql
select 
    arn as inv_asset_id,
    'TRUE' as inv_is_virtual,
    case 
        when bucket_policy_is_public is true then 'PUBLIC ACCESS ALLOWED'
        else 'Internal Access Only'
    end as inv_is_public,
    'Object Store' as inv_asset_type_and_function,
    'No - Inherited from AWS' as inv_in_latest_scan,
    case
      when tags['DiagramLabel'] is not null then tags['DiagramLabel']
      else '"S3"'
    end as inv_diagram_label,
    tags['InformationImpactLevel'] as inv_comments,
    tags['Function'] as inv_function,
    tags['SystemAdministrator'] as inv_system_administrator,
    'securityhub' as inv_scanner
from aws_s3_bucket;
```

Whether or not a bucket has the required tags is trivially testable. 

Whether or not the bucket data `Information Impact Level` is correctly assigned based on the data stored is relatively easy for the CSP or a 3PAO to validate.

If we introduce subjectivity into the inventory at the base level by saying `only some resources of a given type need to be inventoried based on this criteria where a human must decide whether or not they get recorded up front` then I believe that will **fundamentally impair the usefulness of the inventory**. It really would not be much better than maintaining a spreadsheet manually in terms of actual reflection of system state; we'd be relying at a foundational level on an engineer to consistently and correctly do an arbitrary thing which is divorced from actual system functionality. I'm an engineer of sorts, and work with engineers on a daily basis across multiple systems, and my experience is that **if it doesn't directly impact functionality of the thing, then getting it done consistently and correctly at scale is a Sisyphean task**. 

Instead, I believe that the better approach is to inventory _all_ resources of a type within the set of accounts/projects/subscriptions that belong to the system. Know that what we are getting with the inventory is as accurate a representation of state as can reasonably be sourced, and structure the system such that CSPs are incentivized to do this.

- S3 bucket doesn't have a tag indicating information impact level? This should be reflected as a KSI penalty (actual impact to score is something that would need to be established)
- Query for S3 buckets returns a permission error in a system account? Large KSI penalty!
- Account declared for the system is not queried for the inventory? Large KSI penalty! Possibly should trigger some form of external review. 

The criteria above are relatively easy to validate, both manually for a 3PAO during reviews and automatically during and after system inventory generation. 



#### Reply 8

author: [github.com/pete-gov](https://github.com/pete-gov)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12899069](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12899069)

created: 2025-04-21T14:02:02Z

id: DC_kwDOOxfoic4AxNL9

> However, as for the question of use case determining whether a resource is inventoried, I must differ. I'm of the opinion that its better to report _all_ instances of a specific type of resource within a certain context (probably an account/project/subscription) since the alternative is inimical to automation; a human would have to make the decision

Considering the entire point of federal law, OMB guidance, and both NIST and FedRAMP standards are explicitly based on securing federal information, I would argue that a CSP that doesn't know where federal information is flowing in their architecture or which components/services have a likely impact on CIA of that information is fundamentally insecure. And that this assessment should be done by machines, not humans, because only machines should be routing this information in most cases (a human working at a CSO isn't deciding by hand which s3 bucket to put a new document from DHS into, I hope).

Concur that more clarification is always needed, expect updated boundary information soon... 

Edit to add: I love all this btw (sorry, did a quick read before jumping into a meeting). I'd counter that really the goal of a FedRAMP assessment is to _limit_ the scope as much as possible. Simpler systems are more secure, simpler systems are easier to assess, simpler systems are easier to operate, etc. From the government's standpoint, limiting the inventory _in the assessment_ to explicitly things that handle federal information or likely impact the CIA saves money/time/complexity/etc. and reduces the review and adoption burden. To clarify, a CSP should always have a full inventory/etc., they just don't necessarily need to include it in the FedRAMP scope.

For example if a CSP has a whole pipeline that is designed to translate documents in a format for a foreign country in a way that would never be used by a USG agency customer then it might make sense to architect a gov tenant to not be able to access that pipeline at all and entirely exclude it from the scope of FedRAMP assessment. 




#### Reply 9

author: [github.com/sunstonesecure-robert](https://github.com/sunstonesecure-robert)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12899662](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12899662)

created: 2025-04-21T15:04:36Z

id: DC_kwDOOxfoic4AxNVO

>  a human working at a CSO isn't deciding by hand 

Yes - and no.  If humans can do things manually, they will - and do. This would be easy to test - just ask them for their Cloud trail, etc logs and show all the ConsoleLogin events.  I'm pretty sure you'd see that CSPs do a lot of manual ops in the boundary.

 So in this specific case we have seen a CSP have an S3 bucket without the Block Public because it was used for CSS, etc. Great. Until...someone accidentally put backups there with federal data. (a real world case)

Should the inventory include all S3 buckets that *might* be used for inadvertent (or malicious) exfil viz-a-viz their bucket policies - or - only those that at this moment hold Federal data.

It depends on the definition of "secure". If I have a huge lock on my door - it is secure. but if I put the key under the mat...

> only machines should be routing this information 

+100 - so just make this easy: use IaC and show that NO human has IAM access to do anything manual...20x FedRAMP faster ATO/annual.  Don't show that - no Fast Track for you and you get 20X more manual scrutiny by expensive 3PAOs.  

>  this assessment should be done by machines, not humans 

Also fully agree - so have the 3PAOs stop reading Word docs, and instead just insist on an automated (OSCAL or not) comprehensive attack tests from their threat model (has a 3PAO *ever* asked for a CSP's threat model?) and if all paths are successfully defended by automation - ATO;  If not - try again later.

>  From the government's standpoint, limiting the inventory _in the assessment_ to explicitly things that handle federal information or likely impact the CIA saves money/time/complexity/etc. 

Until there's an incident. For a CSP even a minor incident costs minimum 6 figures. Whereas adding IaC costs let's say 1 FTE. So in CA that might be a wash. In other states - that might be a no brainer.  I get that the point of FedRAMP is NOT to guarantee 100% security - that's a fool's errand. It's to make sure agencies have the info they need to make a rational risk assessment.  So I agree make it easier *and* more comprehensive. Those are not mutually exclusive with code.

> To clarify, a CSP should always have a full inventory/etc., they just don't necessarily need to include it in the FedRAMP scope.

I would urge you to test this theory with each and every CSP in the Marketplace today. If more than 20% actually have their full inventory known I'd be surprised. It is *knowable* ie scripts and AWS Config and Azure Graph, etc etc. But - do they actually know it today? Not typically.

EDIT: I also agree that if we are scoping this discussion to Low - then by definition of what Low means - we probably don't care if the system is breached  every day (ignoring the cost) - especially if no other Med/High system connects to, or relies upon, that Low system.




#### Reply 10

author: [github.com/atfurman](https://github.com/atfurman)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-12902829](https://github.com/FedRAMP/community/discussions/26#discussioncomment-12902829)

created: 2025-04-21T21:24:28Z

id: DC_kwDOOxfoic4AxOGt

> Considering the entire point of federal law, OMB guidance, and both NIST and FedRAMP standards are explicitly based on securing federal information, I would argue that a CSP that doesn't know where federal information is flowing in their architecture or which components/services have a likely impact on CIA of that information is fundamentally insecure. And that this assessment should be done by machines, not humans, because only machines should be routing this information in most cases (a human working at a CSO isn't deciding by hand which s3 bucket to put a new document from DHS into, I hope).

I cannot agree more. We may be saying essentially the same thing here, but I view the question of the inventory and its relationship to the boundary as critical enough to what I understand to be the intent behind 20x that I'd like to pull this thread a little further. 

The problems as I see them are something like this:
1. There has, in my experience frequently been a delta between the system as described and the system _as it is_
2. Frequently, these deltas occur and persist because the people who are describing the system and the people who are assessing the system are viewing a sketch of the system (as expressed in inventory, SSP, ports and protocols, etc) and manually checking that against tiny focused snapshots of the system. What is _not_ evaluated typically exceeds what is, and what is evaluated sometimes isn't the best indicator of actual risk. Stuff gets missed. 
3. If the entering assertion `only these 3 S3 buckets store federal data` is used to inform the generation of the inventory, then we arbitrarily ignore the other 47 buckets in this hypothetical account. This introduces a subjectivity to the _generation_ of the inventory, which I view as highly undesirable, since CSP A's inventory isn't necessarily going to reflect their state in the same way that CSP B's inventory does. The inventory then reflects what the CSP _says_ it is versus _what is_. In my opinion this is a critical distinction.

Approaching this from another angle: I think simplicity is good. Simpler systems are easier to understand, explain and maintain than complex ones. Risk is easier to identify and understand, and often easier to manage. 

What gives me pause are complex systems presented in an _artificially simplified manner_. My current position is that if an environment has resources deployed which are not related to the function of the system but share the same operational context (account/project/subscription) then that is itself a source of risk. Why do they exist there in the first place if they aren't contributing to the authorized service? If the inventory is artificially limited in scope close to the source, then this information is lost, or at best obfuscated, whereas if a complete accounting of the system's resources by type from some level downwards (probably account/project/subscription) is provided in accordance with a specification, then the end users can filter down to the resources they care about based on metadata (standards required here) while not losing visibility of a view of the whole that is going to be derived in the same way for every CSP.



## Comment 11

author: [github.com/bthomas85](https://github.com/bthomas85)

url: [https://github.com/FedRAMP/community/discussions/26#discussioncomment-13018563](https://github.com/FedRAMP/community/discussions/26#discussioncomment-13018563)

created: 2025-05-02T21:42:25Z

id: DC_kwDOOxfoic4AxqXD

What other controls would you find helpful to include in the continuous monitoring documentation process aside from the FedRAMP assigned ones?

### Replies

