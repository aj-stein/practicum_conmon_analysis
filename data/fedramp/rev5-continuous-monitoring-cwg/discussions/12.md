# Metadata

title:Assessing Monitoring Over Time (KSI Approach)

author: [github.com/ryan-hodges-gsa](https://github.com/ryan-hodges-gsa)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12)

created: 2025-03-31T16:10:50Z

id: D_kwDOOK0ax84AfF2b



# Post

How do you objectively evaluate the effectiveness of continuous monitoring processes of a cloud service provider over time?  How would you do this in a way that doesn't add more process or burden to stakeholders? 

Our Continuous Monitoring Program Manager and his team have built a concept leveraging metrics that can be obtained through existing required continuous monitoring information and developing key security metrics that can show trends over time.  

https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/blob/main/conmon_ksi_report.md 

Please note: this is solely a proof of concept intended to spark discussion.


# Comments




## Comment 1

author: [github.com/trumant](https://github.com/trumant)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12679246](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12679246)

created: 2025-03-31T17:01:42Z

id: DC_kwDOOK0ax84AwXhO

I like the general thrust of the KSIs. Are any agencies currently consuming similar reporting or any CSO's currently providing similar KSI-centric reporting?

### Replies



## Comment 2

author: [github.com/trumant](https://github.com/trumant)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12679522](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12679522)

created: 2025-03-31T17:27:56Z

id: DC_kwDOOK0ax84AwXli

https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/blob/main/conmon_ksi_report.md#ksi-1-inventory-visibility-coverage-rate

> unauthorized assets found (e.g., there are inventory items not targeted by scans, or scan targets not tracked as inventory components)

In highly dynamic environments with many transient assets (ex: containers or hosts that run for only minutes or hours) in inventory, I'm not sure the "targeted by scan" criteria is very helpful. If a transient asset is "targeted", but never scanned, that seems like relevant data to include in the KSI.

### Replies



#### Reply 1

author: [github.com/aparkar-panw](https://github.com/aparkar-panw)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12679562](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12679562)

created: 2025-03-31T17:30:28Z

id: DC_kwDOOK0ax84AwXmK

I like the concept of where you are going with this. It is along the lines of what I was thinking of KSI. Rather than provide heaps of raw data, high level data that describes the internal operations makes more sense to consume as a customer, as well as fedramp. 



#### Reply 2

author: [github.com/sam-aydlette](https://github.com/sam-aydlette)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12688597](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12688597)

created: 2025-04-01T13:14:45Z

id: DC_kwDOOK0ax84AwZzV

@trumant I've also given a lot of thought about how to best represent inventory in dynamic environments that use ephemeral assets. I've not seen industry align on a single "best practice" for this. IMO using a resource tagging approach combined with monitoring using Prometheus (or similar approach) works decently well. However, given the ephemeral nature of the assets tracking by a tag based on specific version can lead to misrepresentation of risk when existing vulnerabilities are suddenly "new" again because the registry image has been upgraded. Would be very interested in yours or others thoughts on how to best approach this challenge.



#### Reply 3

author: [github.com/aj-stein-gsa](https://github.com/aj-stein-gsa)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12688647](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12688647)

created: 2025-04-01T13:18:39Z

id: DC_kwDOOK0ax84AwZ0H

>  I'm not sure the "targeted by scan" criteria is very helpful. If a transient asset is "targeted", but never scanned, that seems like relevant data to include in the KSI.

It would be great to hear more about what criteria you find helpful, besides the "transient but targeted" one you summarized. That is great, I too would love to hear more from community people on this topic.



#### Reply 4

author: [github.com/atfurman](https://github.com/atfurman)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12701465](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12701465)

created: 2025-04-02T14:07:39Z

id: DC_kwDOOK0ax84Awc8Z

> @trumant I've also given a lot of thought about how to best represent inventory in dynamic environments that use ephemeral assets. I've not seen industry align on a single "best practice" for this. IMO using a resource tagging approach combined with monitoring using Prometheus (or similar approach) works decently well. However, given the ephemeral nature of the assets tracking by a tag based on specific version can lead to misrepresentation of risk when existing vulnerabilities are suddenly "new" again because the registry image has been upgraded. Would be very interested in yours or others thoughts on how to best approach this challenge.

We engaged extensively with the PMO last year on the topic of container images. The approach we arrived at is to scan the deployed image version(s), but to track vulnerabilities against the base image sans tag. This avoids the issue of new container image versions resetting the clock on vulnerability aging. I'm not sure off the top of my head if this guidance has been formalized outside of emails from the PMO, but I think it is a sensible and defensible approach. 



#### Reply 5

author: [github.com/kamamanh](https://github.com/kamamanh)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12704562](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12704562)

created: 2025-04-02T17:48:53Z

id: DC_kwDOOK0ax84Awdsy

> The approach we arrived at is to scan the deployed image version(s), but to track vulnerabilities against the base image sans tag. This avoids the issue of new container image versions resetting the clock on vulnerability aging.

Does this approach still have the issue of resetting the clock on vulns by just updating the base image without patching the vulns? I would think so unless there was something specific to the vuln tracking/scanning system and tooling that accounted for a CVE first having been spotted elsewhere. 



#### Reply 6

author: [github.com/atfurman](https://github.com/atfurman)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12705036](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12705036)

created: 2025-04-02T18:25:15Z

id: DC_kwDOOK0ax84Awd0M

> > The approach we arrived at is to scan the deployed image version(s), but to track vulnerabilities against the base image sans tag. This avoids the issue of new container image versions resetting the clock on vulnerability aging.
> 
> Does this approach still have the issue of resetting the clock on vulns by just updating the base image without patching the vulns? I would think so unless there was something specific to the vuln tracking/scanning system and tooling that accounted for a CVE first having been spotted elsewhere.

It does not. The actual "resource" we are tracking aging and inventorying against is the container image sans tag as that is the persistent functional entity. The tagged versions being scanned are essentially considered instances of that resource. This logic is handled by our internal tooling, but the implementation is relatively straightforward. It is this kind of standard approach which I believe is very important to get established and documented as `the accepted approach` at the inventory and scan level, lest it cause a cascade of issues downstream. 



#### Reply 7

author: [github.com/kamamanh](https://github.com/kamamanh)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12705486](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12705486)

created: 2025-04-02T18:52:57Z

id: DC_kwDOOK0ax84Awd7O

> The actual "resource" we are tracking aging and inventorying against is the container image sans tag as that is the persistent functional entity. The tagged versions being scanned are essentially considered instances of that resource. 

Asking for my own clarity, because I still find the language around containers a bit confusing:
If you had among your pod/stack/etc a container called "helloworld"...  In your description above would "helloworld" the containerized product thing be the "container image sans tag"? And then the tagged images would be different updates/versions of that "helloworld" because you patched the libraries, or updated the code? And then from each tagged instance you would then have ephemeral instances running?  





#### Reply 8

author: [github.com/trumant](https://github.com/trumant)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12706828](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12706828)

created: 2025-04-02T22:06:08Z

id: DC_kwDOOK0ax84AweQM

@kamamanh Your summary is how I understood it as well.







#### Reply 9

author: [github.com/atfurman](https://github.com/atfurman)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12713815](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12713815)

created: 2025-04-03T13:19:33Z

id: DC_kwDOOK0ax84Awf9X

Good points @kamamanh @trumant - I should have clarified with examples. We aren't just using the image name e.g. `nginx` but the unique path to the image in the repository. Provenance is important! So it would be `aws_account_id.dkr.ecr.region.amazonaws.com/nginx`, or `registry.gitlab.com/namespace/project/nginx`. I recall at least one case where the same image was being sourced from different repositories in the same system. I think that detail matters for the inventory! 

For clarity I'm including a snippet of our internal documentation; should have done this to start:

> TCS integration tracks against the container image, sans the version tag. Not including the version tag is key, as this allows for persistence of findings across multiple versions of the same image. For example, if nginx:1.21.3 is found to have a vulnerability, and is updated to nginx:1.21.4 which also has the same vulnerability, there is no change from the perspective of TCS. The finding is still associated with nginx and the aging continues without any change. Only if the container image name changed would the resource be considered changed. This is a key difference from the Trivy integration, which tracks against the namespace, name, kind, and container and is completely independent of the image name.

> The TCS resource will have different format depending on where the image is sourced from. Examples are shown below:
> Docker Hub can have the shortened form such as nginx or the long form such as `docker.io/aquasec/trivy-operator`
> `013241004608.dkr.ecr.us-gov-west-1.amazonaws.com/amazon-k8s-cni` for an image hosted in an ECR repository in us-gov-west-1
> `ghcr.io/external-secrets/external-secrets` for an image hosted on GitHub Container Registry
> `registry.gitlab.com/gitlab-org/gitlab-runner` for an image hosted on GitLab Container Registry

> In all cases, the resource is the uniquely identified image including sourcing information, and detail regarding the affected software will be presented in the list of affected resources maintained by FLM:




#### Reply 10

author: [github.com/aj-stein-gsa](https://github.com/aj-stein-gsa)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12713986](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12713986)

created: 2025-04-03T13:34:10Z

id: DC_kwDOOK0ax84AwgAC

So I can better understand you, @atfurman, you track a container image and vulnerability state against something like the full URI as the "name" or "identifier" of the container image, without version number? That looks like the example below.

`docker.io/nginx`

Your commentary on tags is interesting, but tags can be mutable (release teams can update to them; yes I know there are some systems that can enforce "write once, never change again," but it is not a hard requirement for the OCI specs for registries). And so that said, you track vulnerabilities with that container name above as it goes through the tags, or the addressable SHA addresses of image scans? Both?

- `docker.io/nginx`
  - `docker.io/nginx@sha256:bc2f6a7c8ddbccf55bdb19659ce3b0a92ca6559e86d42677a5a02ef6bda2fcef` (current hash of image with `1.27.3` tag for the `amd64` architecture on Docker Hub; has CVE-3000-9998)
  - `docker.io/nginx@sha256:124b44bfc9ccd1f3cedf4b592d4d1e8bddb78b51ec2ed5056c52d3692baebc19` (current hash of image with `1.27.4` tag for the `amd64` architecture on Docker Hub; has CVE-3000-9998 _and_ CVE-3000-9999)




#### Reply 11

author: [github.com/atfurman](https://github.com/atfurman)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12714441](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12714441)

created: 2025-04-03T14:17:12Z

id: DC_kwDOOK0ax84AwgHJ

@aj-stein-gsa yes, we use the full URI as the container image identifier. In general, we have taken the approach of trusting the repository as minimally as possible. Its job is to serve out the requested container image and little more. As you point out tags can be mutable and we don't want to go down that rabbit hole if we can avoid it. We spent a good deal of time collaborating with Kenneth Payne on this in early 2024 before settling on the current approach.

The high level approach we have settled on is this:
1. The scan orchestrator queries the container orchestrator (EKS, Cloud Run, etc) to compile a list of every distinct container image (e.g. `docker.io/nginx@sha256:124b44bfc9ccd1f3cedf4b592d4d1e8bddb78b51ec2ed5056c52d3692baebc19`  that _could_ run with the current configuration. We can also target  specific repositories, but generally avoid that approach as it has a number of sharp edges since you don't want to be scanning unused versions, and cannot safely assume that the latest version is the only one used. In this example it could be that within the system both `docker.io/nginx@sha256:124b44bfc9ccd1f3cedf4b592d4d1e8bddb78b51ec2ed5056c52d3692baebc19` AND `docker.io/nginx@sha256:bc2f6a7c8ddbccf55bdb19659ce3b0a92ca6559e86d42677a5a02ef6bda2fcef` are in use so we MUST enumerate both as we don't want to have a blind spot. 
2. Pull each image from that list and scan it. Raw scan results are against the image version e.g. `docker.io/nginx@sha256:bc2f6a7c8ddbccf55bdb19659ce3b0a92ca6559e86d42677a5a02ef6bda2fcef` However, this identifier is effectively ephemeral- it might be in use for a few days but will (should) soon be replaced by an updated version. We therefore do NOT want to track vulnerability aging against this, which means that we don't want to treat this as the "resource"
3. Load and normalize scan results. Represent in issue tracker. Findings for _every_ scanned version are associated with `docker.io/nginx`, which is what we represent in the inventory and what we track vulnerability aging against. As long as `CVE-3000-9999` exists on any version of `docker.io/nginx` in use within the context, its going to be considered open. 




#### Reply 12

author: [github.com/aj-stein-gsa](https://github.com/aj-stein-gsa)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12714473](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12714473)

created: 2025-04-03T14:20:06Z

id: DC_kwDOOK0ax84AwgHp

Very cool, thank you for that level of detail. I guess I am loathe to ask: you have all this cool information and then you get to send it to us in the inventory spreadsheet? 😆 



#### Reply 13

author: [github.com/aj-stein-gsa](https://github.com/aj-stein-gsa)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12714523](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12714523)

created: 2025-04-03T14:24:14Z

id: DC_kwDOOK0ax84AwgIb

OK wait, I guess I do have follow-up questions, @atfurman. If you have a vulnerability (good ol' `CVE-3000-9999`) that is only one image version in use in your inventory, how do staff consume information after 3 (we can call it 3b or 4 lol) and know moving from `docker.io/nginx@sha256:bc2f6a7c8ddbccf55bdb19659ce3b0a92ca6559e86d42677a5a02ef6bda2fcef` to `docker.io/nginx@sha256:124b44bfc9ccd1f3cedf4b592d4d1e8bddb78b51ec2ed5056c52d3692baebc19` _may_ mitigate that vuln depending on risk appetite? Or that "a different problem different process" kind of situation?



#### Reply 14

author: [github.com/sam-aydlette](https://github.com/sam-aydlette)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12714807](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12714807)

created: 2025-04-03T14:43:32Z

id: DC_kwDOOK0ax84AwgM3

@atfurman or others what is a good term to use for just the "docker.io/nginx" as opposed to the specific image version? In the Vulnerability Scanning Requirements, FedRAMP used the term "class of asset" but even though the intent of using that phrase was to describe precisely what you're saying, I feel that doesn't adequately describe it. Hence the lack of consensus. I'm curious if there is a better term to align on that makes this more clear.



#### Reply 15

author: [github.com/atfurman](https://github.com/atfurman)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12716331](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12716331)

created: 2025-04-03T16:37:46Z

id: DC_kwDOOK0ax84Awgkr

> Very cool, thank you for that level of detail. I guess I am loathe to ask: you have all this cool information and then you get to send it to us in the inventory spreadsheet? 😆

Yep. 😅 We have taken the approach of expressing and processing our data in JSON and flattening into FedRAMP templates at the last possible stage. Its not entirely possible to escape from the confines of spreadsheet-centric requirements - we have been driven into what I'd consider to be sub optimal structuring in some cases and information is lost during translation to excel, but we have done our best to design with the goal of eventually expressing in a less constraining format. 

Regarding your second question: Ideally we'd know if `nginx@sha256:124b44bfc9ccd1f3cedf4b592d4d1e8bddb78b51ec2ed5056c52d3692baebc19` would mitigate `CVE-3000-9999` because it already got scanned outside the system before it ever got promoted into the authorization boundary. However to your point, knowing _all_ the specific instances of `docker.io/nginx` can be quite an undertaking in larger systems, and we are still updating the way we present this to users in the issue tracker so that enough information is presented without degrading the signal to noise ratio unduly. Some historical context, in the form of an excerpt from an internal description for a **legacy and now deprecated approach**:

> Container scans are notably different from traditional server vulnerability and compliance scans, since container images are what is scanned, and these are (or should be) updated with some regularity. FedRAMP PMO guidance is notably vague on this topic as it does not really appear to grok the concept of an ephemeral resource. In November 2022 the PMO provided the following guidance regarding what should be considered a resource for the purposes of tracking container findings:

> You would track by Asset Identifier which is an identifier specified in the inventory. 
> This is a unique string associated with the asset, it could just be IP, or any arbitrary naming scheme.

> This guidance is not particularly helpful, as it does not address the fact that container images are not persistent, and that the same image may be deployed in multiple locations, even within the same scope. Since there was no coherent guidance for inventorying containerized workloads, we read this as “Do what makes sense and is technically defensible”. So we did.

> Instead of tracking against container images (which should be getting replaced with some regularity) or against containers (which might be used in multiple places within the scope of the cluster), the integration tracks findings against a composite key comprised of the **namespace**(e.g. `kube-system`), **name** (e.g. `aws-node-56ph1`), **kind** (e.g. `DaemonSet`, `StatefulSet`, `ReplicaSet`) and **container** (e.g. `nginx`, `redis`, etc).

> This approach allows unambiguous identification of _exactly_ which components within a cluster are affected by a vulnerability, and as a bonus it is independent of the images in use. If `kube-system.aws-node-56phl.DaemonSet.container` is affected by [CVE-2022-32206: HTTP compression denial of service](https://nvd.nist.gov/vuln/detail/CVE-2022-32206) and is updated to use a new or differently named container image which is also affected by the vulnerability, then the finding aging is **not** reset. The same finding still affects `kube-system.aws-node-56phl.DaemonSet.container` thus the overall system, so FLM continues to track aging without any change.  Conversely, if a container such as `nginx` is used in multiple namespaces across the cluster it does not matter- we track each occurrence independently.

> This approach is defensible from a finding management perspective, but since early 2024 the PMO and 3PAOs have indicated that they prefer to track against the container image, as it is much easier for them to understand and reconcile to inventory when taking that approach. As such, stackArmor has developed the ThreatAlert Container Scanner integration to align with this new guidance and advises phase out of the Trivy integration in favor of TCS.

@sam-aydlette we currently call it a `Resource` when presenting internally on the issue tracker as that is the generic term we use internally for `thing that has a discrete inventory entry and against which vulnerability aging is tracked`. That could probably stand to be clarified to something along the lines of `an inventory resource in the container context is the untagged image reference e.g. docker.io/nginx` or `an inventory resource in the container context is the full image repository path e.g. docker.io/nginx`. 

The current approach of considering this to be a resource has a number of advantages as `docker.io/nginx` is going to be providing a specific set of functions within the system, and as long as it is in use then we have the persistent concept of `this thing exists for these purposes, and has this set of attributes including source, configurations, vulnerabilities (current and historical), active uses, etc`. It is as clean as I think it can reasonably be from the top down view, but causes some level of hassle from the bottom-up operational view. Example: 

> I have 15 uses of  `docker.io/nginx` within a single context (i.e. cluster) within my system. They aren't all using the same version! I see I have an active finding for `CVE-3000-9999`. WHICH SPECIFIC ONE IS IT AND WHAT CONFIGURATION DO I NEED TO UPDATE TO REMEDIATE? This is part of why we settled on the original approach- `kube-system.aws-node-56phl.DaemonSet.container` is great because it tells me _exactly_ what needs to be updated, while `docker.io/nginx` operating under the spreadsheet paradigm doesn't. I need to go parse the scans and read cluster metadata directly, which is what I set out to avoid in the first place. 

Fortunately, if we move away from the spreadsheet-centric approach its reasonably straightforward to provide metadata showing what specific versions are affected by the vulnerability and where specifically they are in use. 





#### Reply 16

author: [github.com/sunstonesecure-robert](https://github.com/sunstonesecure-robert)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-13075698](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-13075698)

created: 2025-05-08T12:58:11Z

id: DC_kwDOOK0ax84Ax4Ty

> > The approach we arrived at is to scan the deployed image version(s), but to track vulnerabilities against the base image sans tag. This avoids the issue of new container image versions resetting the clock on vulnerability aging.
> 
> Does this approach still have the issue of resetting the clock on vulns by just updating the base image without patching the vulns? I would think so unless there was something specific to the vuln tracking/scanning system and tooling that accounted for a CVE first having been spotted elsewhere.

Link the clock to the git commit that introduced the vulnerable image. That date is immutable forever.



## Comment 3

author: [github.com/cybersechawk](https://github.com/cybersechawk)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12679609](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12679609)

created: 2025-03-31T17:34:35Z

id: DC_kwDOOK0ax84AwXm5

For container environments, once you have proven to a 3PAO that you are using a technology that is set to replicate a base image, then what matters is the configuration and any vulnerabilities on the base image.    The data model chosen for representing inventory should account for this as an inventory object type.   Everything starts with the inventory.

### Replies



#### Reply 1

author: [github.com/sunstonesecure-robert](https://github.com/sunstonesecure-robert)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12679915](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12679915)

created: 2025-03-31T18:05:50Z

id: DC_kwDOOK0ax84AwXrr

I generally agree - and - images in a complex environment can change multiple times per day.  So the inventory is really dynamic in more complex containerized systems. A static inventory was great for physical systems, and even early cloud VM based systems, and yes even for relatively static containers.... but the concept sort of falls flat in a modern microservices/cloud-native world.  By the time I produce my inventory and scan it, it might already be out of date.



#### Reply 2

author: [github.com/cb-axon](https://github.com/cb-axon)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12681374](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12681374)

created: 2025-03-31T21:09:39Z

id: DC_kwDOOK0ax84AwYCe

I would be careful identifying that the vulnerabilities only come from the base image in a container. Many can argue that AL2 is a "base image" and "secured" in their pipeline, but devs can take that base and add not only configuration to it, but that configuration can pull in entirely new packages that have vulnerabilities and do other things than the originally intended "base" allows. I think this oversight could create gaps that eventually require more manual validation by a 3PAO, and doesn't lend to the intent of truly automating these evaluations. There'd have to be a LOT of trust that the base image that you are scanning didn't get ANYTHING added to it before implemented in prod, and thats a pretty big if, especially for highly distributed teams.



#### Reply 3

author: [github.com/atfurman](https://github.com/atfurman)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12682155](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12682155)

created: 2025-03-31T23:30:08Z

id: DC_kwDOOK0ax84AwYOr

I concur with @cb-axon. I have personally observed numerous cases where an AMI was built to resolve vulnerabilities but not all instances got replaced, where a VM was launched from a hardened image but hardening was partially undone during bootstrap, where an older container image than expected was in use, etc.  

In all these cases, if we were not directly assessing current state these oversights and their associated risks likely would have been missed for much longer than they were. I touched on this some in https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/15, as I don't currently see a way to accurately represent the system's risk at a given time unless the inventory accurately represents what exists at that point in time.



#### Reply 4

author: [github.com/sam-aydlette](https://github.com/sam-aydlette)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12717038](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12717038)

created: 2025-04-03T17:19:04Z

id: DC_kwDOOK0ax84Awgvu

@cb-axon couldn't you address this with container image signing along with using admission controllers like OPA or Kyverno to verify signatures and additional verification using image scanning (Trivy, etc.) to validate hardening and vuln free images in production? That way there is immutable infrastructure enforced in code. Or am I missing something here?

What I'm driving at here is, is the issue a lack of ability to do it at all or is the issue technical debt?



#### Reply 5

author: [github.com/atfurman](https://github.com/atfurman)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12717553](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12717553)

created: 2025-04-03T18:22:28Z

id: DC_kwDOOK0ax84Awg3x

@sam-aydlette my take on it is that there are a number of ways to meet the core requirements. Saying `container image signing and admission controllers will solve the problem` adds opinions about _how_ to solve the problem. I'm curious to hear other's perspectives, but I don't believe ConMon necessarily wants to (or reasonably can) go there. 

At this point I have directly supported 10+ distinct FedRAMP systems and had some level of visibility into many more. They are all different to some degree. Their purposes, architecture, tech stack, working practices etc all vary. I don't believe its viable to say `this is how you must solve this problem`, _particularly_ not in the context of ConMon. 

Rather, I think that the focus must necessarily be `here are the attributes we care about and which you must measure, here is how they should be measured and evaluated, and here is how you report it`. 

I think that from a monitoring perspective we must be as agnostic as reasonably possible about _how_ the requirement(s) are achieved and instead focus on faithfully assessing and reporting state in a manner which reliably indicates if the requirements are being met, and can be reliably reproduced across the broadest set of systems possible. 



#### Reply 6

author: [github.com/sam-aydlette](https://github.com/sam-aydlette)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12717998](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12717998)

created: 2025-04-03T19:07:53Z

id: DC_kwDOOK0ax84Awg-u

Absolutely, and to be clear I am not suggesting that the example I gave is *the* way to solve the problem. It was just an example. I just wanted to get on the same page with folks that it is not necessarily a matter of this being inherently unachievable, but rather a matter of how to get there and how to "faithfully assess and report" as you eloquently said it.



#### Reply 7

author: [github.com/cb-axon](https://github.com/cb-axon)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12718910](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12718910)

created: 2025-04-03T21:16:33Z

id: DC_kwDOOK0ax84AwhM-

> ble infrastructure enforced in code. Or am I missing something here?
> 
> What I'm driving at here is, is the issue a lack of ability to do it at all or is the issue technical debt?

How would signing or restricting initial deployment of a container prevent it from accumulating vulnerabilities over time? That only guarantees they were that way at deployment. Signing pre-deployment is good to make sure your pipeline wasn't poisoned, but it doesn't really do much for vulnerability management in the known containers themselves. Also, its almost never possible to have a container with zero vulnerabilities. There might not be any detected at one specific point in time, but we see all the time that those containers eventually will have a vulnerability. And we need to detect those to identify the risk to the system today. If we are only scanning at deployment, we will miss those entirely. Not saying we shouldn't also do this at deployment, I'm saying that shouldn't have anything to do with monthly ConMon/POA&Ms since its extremely point-in-time, and that point-in-time could be from 90 days ago.  

Also, signing/pipeline also is separate from the suggestion that "only scanning the Base OS of the container" is sufficient, which is what this thread originally started from.



#### Reply 8

author: [github.com/sam-aydlette](https://github.com/sam-aydlette)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12750346](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12750346)

created: 2025-04-07T12:27:37Z

id: DC_kwDOOK0ax84Awo4K

@cb-axon @atfurman @cybersechawk @sunstonesecure-robert 

The intent behind my comment was to suggest an approach that *could* potentially work as a starting point for objectively demonstrating that all container images in use in production are hardened and free of all (or at least the most critical) vulnerabilities on an ongoing basis. I agree that there's more than one way that this can be achieved, and that signing and restricting alone wouldn't be enough - the process would have to also include post-deployment monitoring of existing runtime containers and ensuring drift does not occur.

The reason I am interested in understanding how a specific approach could generate a reliable KSI that sufficiently demonstrates holistic inventory, vulnerability, and configuration management in container workloads is not to say "this is how it must be done." Rather, the idea is to come up with concrete examples of what *does* work so that the requirements are less abstract and more understandable for engineers, thus reducing uncertainty and ambiguity. I'd love to hear thoughts on additional ways to demonstrate security through code other than the digital signing + policy as code with admission controllers (terraform, OPA, kyverno etc) + SAST/SCA + post-deployment drift monitoring approach.



## Comment 4

author: [github.com/trumant](https://github.com/trumant)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12679898](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12679898)

created: 2025-03-31T18:03:44Z

id: DC_kwDOOK0ax84AwXra

https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/blob/main/conmon_ksi_report.md#overall-risk-score

> The Overall Risk Score is calculated by taking the average of all 8 KSIs, and grading the result. An "A" is 90%-100%, a "B" is 80%-90%, etc.

While seemingly convenient to aggregate the individual KSIs into some uber score, I find it to be a troubling oversimplification.

I think agencies should be left to draw their own conclusions on risk based on each KSI alone, absent the average.

### Replies



#### Reply 1

author: [github.com/Telos-sa](https://github.com/Telos-sa)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12690138](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12690138)

created: 2025-04-01T15:12:41Z

id: DC_kwDOOK0ax84AwaLa

I like the Idea of the Standard provided by FedRAMP.  If Sponsors can choose their own adventure, it can increase the complexity of package maintenance.   IF AO wants a score of X, calculated by weighing the KSI's in a different way, and Another Agency AO Wants a score of Y, calculated in a completely different way, CSPs are going to have to provide two packages.  PMO should be establishing the High water mark, and Agencies can flex, but should not be MORE stringent than FedRAMP, else, package will have to be modified.  



#### Reply 2

author: [github.com/sam-aydlette](https://github.com/sam-aydlette)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12717061](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12717061)

created: 2025-04-03T17:21:54Z

id: DC_kwDOOK0ax84AwgwF

@Telos-sa do you have suggestions for how to standardize the reporting in a way that is transparent for all (or at least most) agencies? If so head over to [continuous reporting](https://github.com/FedRAMP/continuous-reporting-cwg) and I'd love to hear your thoughts on that.



#### Reply 3

author: [github.com/sunstonesecure-robert](https://github.com/sunstonesecure-robert)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-13075836](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-13075836)

created: 2025-05-08T13:05:05Z

id: DC_kwDOOK0ax84Ax4V8

> I like the Idea of the Standard provided by FedRAMP. If Sponsors can choose their own adventure, it can increase the complexity of package maintenance. IF AO wants a score of X, calculated by weighing the KSI's in a different way, and Another Agency AO Wants a score of Y, calculated in a completely different way, CSPs are going to have to provide two packages. PMO should be establishing the High water mark, and Agencies can flex, but should not be MORE stringent than FedRAMP, else, package will have to be modified.

I see it as additive - if you have the underlying data, do whatever slicing and dicing you see fit. If I was the AO I would just ask for all the raw data and do my own KSI py script. 



## Comment 5

author: [github.com/trumant](https://github.com/trumant)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12679940](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12679940)

created: 2025-03-31T18:08:12Z

id: DC_kwDOOK0ax84AwXsE

https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/blob/main/conmon_ksi_report.md#significant--changes

> For these changes, a completed significant change form (the "sig-change-report-template.md") must be delivered to agency customers

Is this a work in progress? If so, perhaps that work could be done in the open in a draft pull request

### Replies



#### Reply 1

author: [github.com/sam-aydlette](https://github.com/sam-aydlette)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12690271](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12690271)

created: 2025-04-01T15:25:21Z

id: DC_kwDOOK0ax84AwaNf

We have developed a template internally for reporting SCR details. I'll work with the team to see if we can release that as an informal draft to foster further discussion.



#### Reply 2

author: [github.com/sam-aydlette](https://github.com/sam-aydlette)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12750352](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12750352)

created: 2025-04-07T12:28:17Z

id: DC_kwDOOK0ax84Awo4Q

@trumant PR submitted that adds context for significant changes.



## Comment 6

author: [github.com/trumant](https://github.com/trumant)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12679987](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12679987)

created: 2025-03-31T18:13:13Z

id: DC_kwDOOK0ax84AwXsz

https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/blob/main/conmon_ksi_report.md#significant--changes

> Ensure customer responsibilities are documented for the new feature or service. There must be a clear path for customers to opt-in of changes if they do wish to accept the risk.

I'd like to see this be rephrased to something like:

Significant features should be deployed in a disabled by default / opt-in for access manner. Customers, once informed of their responsibilities for the feature, may choose to opt-in for feature access if they wish to accept those responsibilities and any associated risks.

### Replies



#### Reply 1

author: [github.com/cb-axon](https://github.com/cb-axon)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12718931](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12718931)

created: 2025-04-03T21:19:06Z

id: DC_kwDOOK0ax84AwhNT

How would this account for SCRs where a CSP is changing something in the architecture and customers can't just "opt-out."?



#### Reply 2

author: [github.com/sam-aydlette](https://github.com/sam-aydlette)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12725827](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12725827)

created: 2025-04-04T12:38:32Z

id: DC_kwDOOK0ax84Awi5D

@cb-axon great point. Not everyone is aware of that distinction - just because a customer doesn't opt in to a service, they still may be subject to additional risk introduced by the change due to shared backend infrastructure. IMO this is fundamental to the cloud business model.

Very interested in folks thoughts on this.



## Comment 7

author: [github.com/sunstonesecure-robert](https://github.com/sunstonesecure-robert)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12680016](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12680016)

created: 2025-03-31T18:17:03Z

id: DC_kwDOOK0ax84AwXtQ

Should we comment on separate items on the list here? or break those out?  I can edit this if this is the wrong place to put individual item comments...

re
> KSI 7: Cryptographic Module Compliance Rate

I wouldn't really see much value in tracking FIPS as a metric. Either you are in compliance or not, you have a DR or not, and if you aren't that should be a POA&M and tracked in those metrics.  If I have crypto (FIPS or not) deficiencies, them mark them as Highs on the POA&M, give me 30 days to fix, and off we go. Measure my performance on that goal.

ASIDE: That said, as a cryptographer, I overall don't understand the focus on FIPS, especially in a PQ world. I'd rather focus on automating the security testing - continuously. NIST had a great event in the summer on formal verification and testing of crypto (pre- and post-Quantum). That's where we should be investing, not in lab certs. But I digress and don't want to troll.

### Replies



## Comment 8

author: [github.com/tnnrjmsn-eit](https://github.com/tnnrjmsn-eit)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12680470](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12680470)

created: 2025-03-31T19:12:22Z

id: DC_kwDOOK0ax84AwX0W

> How do you objectively evaluate the effectiveness of continuous monitoring processes of a cloud service provider over time? How would you do this in a way that doesn't add more process or burden to stakeholders?
> 
> Our Continuous Monitoring Program Manager and his team have built a concept leveraging metrics that can be obtained through existing required continuous monitoring information and developing key security metrics that can show trends over time.
> 
> https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/blob/main/conmon_ksi_report.md
> 
> Please note: this is solely a proof of concept intended to spark discussion.

Wouldn't this proposed ConMon KSI Report be better suited to the [Continuous Reporting CWG](https://github.com/FedRAMP/continuous-reporting-cwg/blob/main/plan.md#key-community-working-groups-activities), and the underlying KSIs be best suited for the [Automating Assessment CWG](https://github.com/FedRAMP/automating-assessment-cwg/blob/main/plan.md#key-community-working-groups-activities)?

Seems to me that we may have a dependency issue where the bulk of the work will be under the [Automating Assessment CWG](https://github.com/FedRAMP/automating-assessment-cwg/blob/main/plan.md#key-community-working-groups-activities) (hopefully in close consultation with the [Existing Frameworks CWG](https://github.com/FedRAMP/applying-existing-frameworks-cwg/blob/main/plan.md#key-community-working-groups-activities)) and the [Reporting](https://www.fedramp.gov/20x/working-groups/reporting/) and [ConMon](https://www.fedramp.gov/20x/working-groups/rev5-monitoring/) will flow from there.

Please confirm or correct, @ryan-hodges-gsa.
(Cc @pete-gov)

### Replies



#### Reply 1

author: [github.com/pete-gov](https://github.com/pete-gov)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12680502](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12680502)

created: 2025-03-31T19:17:44Z

id: DC_kwDOOK0ax84AwX02

If we can port some of the KSI concepts to on-going Rev 5 continuous monitoring (outside of year assessments), I think we'd all benefit. I wouldn't expect Rev 5 continuous monitoring to use the exact same KSIs as an eventual 20x authorization path, but trying out the most important ones to support Rev 5 security beyond vuln scans/etc. seems worth exploring to me!



#### Reply 2

author: [github.com/aparkar-panw](https://github.com/aparkar-panw)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12680794](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12680794)

created: 2025-03-31T19:52:27Z

id: DC_kwDOOK0ax84AwX5a

@pete-gov  is there a way for FedRAMP to help determine what the end fed customers see as a priority for ConMon reporting? 

If we were to come up with a proposed list can we poll agencies? 

Ex a customer might really care if a CSP has a very small environment and cannot remediate quickly, vs a CSP that has a very very large requirement and would have a slightly lower mean time to remediate given additional complexity. 

They could glean that from KSI1 and other KSIs to determine what is acceptable to them. I highly doubt the details of hundreds or thousands of POAMs are of interest to an agency as long a they are being addressed in a timeline manner, esp the KEVs which I agree would be a separate KSI. 



#### Reply 3

author: [github.com/pete-gov](https://github.com/pete-gov)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12681052](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12681052)

created: 2025-03-31T20:27:58Z

id: DC_kwDOOK0ax84AwX9c

> @pete-gov is there a way for FedRAMP to help determine what the end fed customers see as a priority for ConMon reporting?

Yes - we'll be working that behind the scenes a bit with OMB and the FedRAMP Board to ensure that our standards align with agency needs.





## Comment 9

author: [github.com/JosephScarzone](https://github.com/JosephScarzone)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12681193](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12681193)

created: 2025-03-31T20:43:20Z

id: DC_kwDOOK0ax84AwX_p

Metrics over time is very ideal and worth its weight in gold. As we build automation, let's also create those metrics that report on trend over time. A Compliance-as-Code/Reporting-as-Code approach will help make this a lot easier to pull off. 

### Replies



## Comment 10

author: [github.com/mjprager](https://github.com/mjprager)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12682046](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12682046)

created: 2025-03-31T23:04:46Z

id: DC_kwDOOK0ax84AwYM-

Other possible KSI's for debate:

- Raw count of vulnerabilities (total count at that level)
- Unique count of vulnerabilities
- Unique last month
- New this month
- Remaining / carry over from last month
- % of the service/offering that are out of SLA
- % of the assets that are out of SLA
- Rate of recurrence, to measure how often previously remediated vulnerabilities reappear

Especially when we're talking about Linux vulns, the distinct count can shoot sky high quickly, even if they are only one month past SLA. That doesn't necessarily mean it's at a larger or smaller risk than a Windows asset behind one month past SLA, even though Windows will roll up all vulns into a single monthly patch. 


### Replies



## Comment 11

author: [github.com/Telos-sa](https://github.com/Telos-sa)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12689871](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12689871)

created: 2025-04-01T14:49:26Z

id: DC_kwDOOK0ax84AwaHP

## Report Details Feedback:
### Overall Risk Score:
 1. Are all of the KSI's weighed the same?  
 2. What Score/s are a *Pass* and which are a *Failed* is it sliding?  Do all of them have to be Passed to Pass?  If one is a Fail, do you fail? **Scoring Rubric is critical**
 3. I see trending is important, Is this to monitor slippage and improvement?  How long before Loss of ATO, No improvement across 6 months?  Is there a plan to create a "Get to Pass" initiative for all CSO's that have active ATO but do not meet Pass/Failed. 
 
### KSI 1: Inventory Visibility Coverage Rate
This is tied to Inventory that has gone through CCB.  With CICD pipelines, I imaging tagging is key, so that elements don't come through as unauthorized.  
1.  Is expectation, based on inventory data, that IP Address, FQDN, Container Tag, and/or hostname the elements to identify uniqueness of an asset over time. Which ones should denote an unidentified asset? Can we have more detail here?
2. What if sampling is used for scanning, is there a metric for this?  Do we need to scan X% of assets, based on their baseline.  

### KSI 2: Mean Time To Remediate Vulnerabilities
FedRAMP traditionally has tracking of findings as HIGH (30 Days), MODERATE (90 Days), LOW (180 Days).  How is FedRAMP going to define critical vulnerabilities, and the expected remediation date?  
Explain how the Deviation Requests are going to fall into this, especially for reporting on Vendor Dependencies of Upstream Leveraged Authorizations, and establishing an ownership to these findings/impacts downstream. 

### KSI 3: Percentage Late CISA KEV Findings
Connection to CISA will require an interconnection, on the CSOs to secure, since it will always be a get.  Does CISA have FedRAMP ATO in marketplace for us to leverage for the interconnection, or is this interconnection going to be part of standard deployment?  What is the recommended requirement to pull and report?  Is this still a monthly submission to PMO, so I can pull daily from CISA, and report monthly to PMO/AO, or daily/daily?  

### KSI 4: Secure Configuration Rate
1. What is going to be the standard process for approving configuration deviations against baseline.  Should it be a POAM with an operational dependency, linked specifically to the failed Control, that references the FAILED STIG V-ID or CIS benchmark rule ID?  Do we need to correlate this information to NIST 800-53A through CCI from DISA?  
2. What about when baseline requirements change, what is the time to update to comply with new STIG checklists?  Do the scanners that are FedRAMP authorized have requirements to produce new STIGs/CIS checklists in X amount of time to support downstream consumers. 
3. How should we manage the False positives, for configuration that is set, but scanner's cannot identify.  Are we approved to modify the audit files to get more accurate results?  Who validates that the change to the audit file did not compromise the integrity of the scan?  Or do we Create POAM and apply for False Positive?  If False positive from Baseline, do we need to renew annually?  

### KSI 5: Mean Time To Investigate Alerts
1. What would be acceptable evidence?
2. What would be the different scoring indicators based on severity?  (10 within 1 hour, etc...)
3. Are you distinguishing between Resource alerts vs Access alerts? 

### KSI 6: Privileged Access Compliance Rate
1. Should tie back to SSP as well, want to see if they are documented and approved user accounts, like with inventory, and that their permissions align with what was approved.  Recommend an adjustment to the % calculation here, see inventory coverage rate.  
2. Need to identify acceptable methods of reporting, IE automated creation of a report vs user generated report. And procedure/runbook approved for creation.  CSPs should have a method of auto-generating these reports, and a way to validate the completeness and accuracy of the report. 
3. If IaaS, should also incorporate Physical security? 

### KSI 7: Cryptographic Module Compliance Rate
1. is this going to tie directly to NIST CMVP? Do the modules have to be validated? 
2. What is the process for approved exceptions, is it a POAM with an OR?
3. Recommend leveraging OSCAL for this, and establishing the requirements for relationships.  IE, Cryptographic component, is deployed on a service that is used in an inventory item, an interconnection is secured to move the data, and the OS component has a validated module to encrypt at rest.  In this scenario, I would report the component on the OS, Service, Interconnection, and relate each of these to the inventory item.  
4. Need to ensure requirements are specific on what is interacting with the data, to know when crypto modules need to be leveraged.  
5. What about inheriting risk from Providers who are also failing, Risk rolls downstream?  Is this a lower/higher risk for consumer?

### KSI 8: Agency Statutory Compliance Report
- Recommend that these are reported via the CONTROL level, for Agency Controls (another Regulation, or modification to the baseline).  
- Is this reporting percentages of percentages.  (IE I do 80% of 508 compliance, and the others are on my roadmap (CSP should have POAM for the remaining items?)) And 90% of offered services support IPV6.  To Calculate, do I add all of my compliance percentages together?  

### New Services:
Pre-Deployment
1. Are we submitting a ticketed change request to the AO, PMO or both, or do you mean CSP internal tickets?  
2. Is there a standard rubric or template for what you want in the SIA, since this is the source of the reporting metric.  There is currently NO template in the FedRAMP Docs repository.  
3. Do these significant changes need to be approved after Pre-Deployment (Ticket?) With AO/PMO before deployment, or internal CCB suffices.
4. What about the different risk Tolerance of different Agencies. Do I need to modify my package to support the different Agency Requirements?  Does that mean that I will need to maintain different Packages for different Agencies consuming my CSO, or only have to meet AO requirements, and PMO communicates changes downstream to consumers? 



### Replies



## Comment 12

author: [github.com/Telos-sa](https://github.com/Telos-sa)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12690088](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12690088)

created: 2025-04-01T15:08:07Z

id: DC_kwDOOK0ax84AwaKo

## Directory and Project Details Feedback
After indepth review of the requirements, I would recommend some tweaks to conform with OSCAL, which should be able to provide PMO with the core data elements needed to generate the reports.  

Here is the breakdown of the recommended structure. I kept it pretty high level, but I would recommend this over markdown, so that the data can be ingested via API, and instead of looking at the artifact, AO, CSP, and PMO can work with the data to review elements for reporting.  Even if it is not Direct OSCAL ingestion, but extraction and conversion into Reporting metrics - which would be really COOL!

### Repository containing the following
Reports:
- OSCAL SSP
```
{
    "system-security-plan": {
        "uuid": 
            "(Leverage Method 5, of hash of    content.  If UUID is different, content is different.  If UUID is same as previous submission, no data has changed (Either could be an indicator of futher investigation))"
        ,
        "metadata": {
            title: "cso-name-report-name-in-token-format"
            last-modified: "Date in OSCAL Format"
            version: "Current Version"
        },
        "system-characteristics": {
            "system-ids": [
                {
                    "identifier-type": "http://fedramp.gov/ns/oscal",
                    "id": "FedRAMP Package ID"
                }
            ],
            "system-name": "CSO Name"
        }
        "system-implementation": {
            "props": [
                {
                    "name":"system-id",//prop pre-defined in OSCAL Model
                    "value": "The Internet Protocol v4 Address of the asset."
                },
            ]
            ,
            "leveraged-authorizations": []// a type of inventory, referencing the services that CSPs are consuming from upstream (need method for reporting risks downstream so consumers can mitigate risks)
            ,
            "users": []// a type of inventory, if we are looking at user account types that have access to the system and their permissions. Part of CONFIGURATION DATA
            ,
            "components": []// a type of inventory, listing elements such as cryptographic modules, software, Operating systems, service components (PPSM) (including components that are inherited from lev-auth), interconnections into the boundary. Part of CONFIGURATION DATA
            ,
            "inventory-items": [
                "uuid": "Identifier to track Inventory across submissions.  (Use Method 5, or static)",
                "description": "PMO to Provide standard of contents",
                "props": [
                    {
                        "name":"ipv4-address",//prop pre-defined in OSCAL Model
                        "value": "The Internet Protocol v4 Address of the asset."
                    },
                    {
                        "name":"ipv6-address",//prop pre-defined in OSCAL Model
                        "value": "The Internet Protocol v6 Address of the asset."
                    },
                    {
                        "name":"fqdn",//prop pre-defined in OSCAL Model
                        "value": "The full-qualified domain name (FQDN) of the asset."
                    },
                    {
                        "name": "asset-id",//prop pre-defined in OSCAL Model, can be leveraged for HostName, or FedRAMP Can create and own new PROP content
                        "value": "An organizationally specific identifier that is used to uniquely identify a logical or tangible item by the organization that owns the item."
                    },
                    {
                        "name": "asset-tag",//prop pre-defined in OSCAL Model, can be leveraged for Container Registry Image Version Tag (i.e., RHEL 8), or FedRAMP Can create and own new PROP content
                        "value": "An asset tag assigned by the organization responsible for maintaining the logical or tangible item."
                    }
                ],
                "links": [
                    {
                        "href": "uuid fragment thank links back to users element"
                    }, 
                    {
                        "href": "fragment that links down to backmatter resource" //artifacts for scan and/or evidence
                    }
                ]
                "implemented-components": [
                    {
                        "component-uuid": "uuid of software/service/os/interconnection that links back to components elements"
                    }
                ]
            ]// a list of traditional inventory (SSM Appendix M), with logical links back to the components that are supporting, installed on, or connecting with.  
                
        },
        "back-matter": {
            "resources": [
                "uuid": "unique identifier of artifact to reference in links of inventory",
                "title": "Name of File in token format. Example: awesomecloud-conmonreport-20240129.pdf",
                "rlinks": [

                ]// if document is going to be stored in the artifact folder.
                ,
                "base64": {
                    "filename": "token format",
                    "media-type": "mime",
                    "value": "base64 encoded data"// this will greatly increase size of SSP and will have to decode each artifact (on PMO for action).  Recommend rlinks instead
                }
            ]
        },
    }
}
```
- OSCAL POAM
```
{
"plan-of-action-and-milestones": {
    "uuid": 
            "(Leverage Method 5, of hash of    content.  If UUID is different, content is different.  If UUID is same as previous submission, no data has changed (Either could be an indicator of futher investigation))"
        ,
        "metadata": {
            title: "cso-name-report-name-in-token-format"
            last-modified: "Date in OSCAL Format"
            version: "Current Version"
        },
        "import-ssp": {
            "href": "reference to SSP above, to logically link the two reports and UUID of components for reporting"
        },
        "system-id": {
            {
                "identifier-type": "http://fedramp.gov/ns/oscal",
                "id": "FedRAMP Package ID"
            },   
        "observations": [
            {
                "uuid": "Method 5, generated from the scan identifier",//if we do it this way, we can see the plugin across all packages, great for AO and PMO
                "title": "Title of test from tool",
                "description": "Description from tool",
                "method": ["INTERVIEW", "EXAMINE", "TEST"],// if from tool, use default Test.  If warning, may be test and Examine, if INFO may be test, examine, and interview.
                ""
            }// Test Results from all scanners bundled here. Only if you want to see the specific tests for reporting.  ELSE, remove and use Links to the scan files instead.  
            {
                "uuid": "UUID generated from content",
                "title": "DR TYPE",
                "description": "Deviation Justification",
                "method": ["INTERVIEW", "EXAMINE", "TEST"],// Will probably be Examine, for evidence to review for DR. 
                "collected": "Date Evidence was collected",
                "expires": "Date DR expires"// Should be required for FP and OR
            }// Observations for Deviation Requests and Vendor Dependencies
        ]
        "risks": [
        ]// Collection of all Failed Results/POAMs to store milestones, remediation, etc..  If that data is not required, Dont Include.
        ,
        "poam-items": [
            {
                "uuid": "Must Be unique, and static, for tracking"
                "Title": "POAM Title",
                "Description": "Weakness Description",
                "links": [
                    {
                        "href": "fragment that links down to backmatter resource" //artifacts for scan and/or evidence
                    }
                ]
                "origins": [
                    {
                        "actors":[
                            "type": "choice of: tool, assessment-platform, party",
                            "actor-uuid": "Recommend based on Name"// Want to be able to track toolusage across other packages, for confirmation of industry standard.  
                        ]
                    }
                ],
                "related-observation": [
                    {
                        "observation-uuid": "UUID of corresponding observation/s from the observation list"
                    }
                ]
            }
        ],// Recommend that this only reports on ACTIVE FINDINGS, IF FP DR is already approved, and OR is already approved, dont report until DR expires
        "back-matter": {
            "resources": [
                "uuid": "unique identifier of artifact to reference in links of inventory",
                "title": "Name of File in token format. Example: awesomecloud-conmonreport-20240129.pdf",
                "rlinks": [

                ]// if document is going to be stored in the artifact folder.
                ,
                "base64": {
                    "filename": "token format",
                    "media-type": "mime",
                    "value": "base64 encoded data"// this will greatly increase size of SSP and will have to decode each artifact (on PMO for action).  Recommend rlinks instead
                }
            ]
        },
    }
}
```
- Artifacts Folder
    - contains files referenced in the POAM and SSP. All Backmatter resources should be staged here, and organized based on file type (media type?) for ease of sorting.  


### Replies



#### Reply 1

author: [github.com/cybersechawk](https://github.com/cybersechawk)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12690264](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12690264)

created: 2025-04-01T15:24:53Z

id: DC_kwDOOK0ax84AwaNY

The one thing that might be helpful is to allow agencies to adjust risk scoring weights if they have legitimate reason for unique risk requirements.  This would be a bit of a slippery slope but if we handled this way then it would be easy for CSPs to ensure they invest sufficiently in tools and automation needed for meeting unique requirements and it would also bring more transparency to the community around special requirements and what things are unique that would require special handling.   Also it would be helpful on the this topic to have representation from DISA since DoD has more stringent requirements in general 



## Comment 13

author: [github.com/trumant](https://github.com/trumant)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12691487](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12691487)

created: 2025-04-01T17:28:26Z

id: DC_kwDOOK0ax84Awagf

I hacked together a very quick and dirty Markdown report generator and have made the project available at https://github.com/trumant/fedramp-ksi-report

An example report generated by the tool can be found in https://github.com/trumant/fedramp-ksi-report/blob/main/sample_report.md

While I'm not sure how much utility the code itself has, I found it useful to review the proposed report format and serve as a basis for further discussion and hopefully, automation.

### Replies



## Comment 14

author: [github.com/kamamanh](https://github.com/kamamanh)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12705630](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12705630)

created: 2025-04-02T19:11:36Z

id: DC_kwDOOK0ax84Awd9e

How does the KSI approach, which assumes automation for all things, plan to deal with things for which automation is incomplete or has gaps? 

For example, one of the common vulnerability scanning tools identifies vulnerabilities on Red Hat operating systems by means of checking whether a given RHSA has been applied to the system.  An RHSA is not generated until a CVE(s) fix is completed and bundled.  A CVE can exist unpatched on a system without an RHSA because there is no patch available and thus not detected by the scanner.  Currently the methods to deal with this are manual. Would this major tool no longer be acceptable? Would CSPs be required to write custom automation to make up the gap? Would CSPs be able to just rely on the output from the tool, and not have to report CVEs which are unpatched and do not have an RHSA for the scanner to find? 

### Replies



#### Reply 1

author: [github.com/sam-aydlette](https://github.com/sam-aydlette)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12705844](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12705844)

created: 2025-04-02T19:44:36Z

id: DC_kwDOOK0ax84AweA0

Can you be more specific about which vulnerability scanning tool you're referring to? Ideally with a link to some documentation?



## Comment 15

author: [github.com/austinsonger](https://github.com/austinsonger)

url: [https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12708215](https://github.com/FedRAMP/rev5-continuous-monitoring-cwg/discussions/12#discussioncomment-12708215)

created: 2025-04-03T02:31:01Z

id: DC_kwDOOK0ax84Awel3

@ryan-hodges-gsa @trumant @atfurman @pete-gov @sunstonesecure-robert @sam-aydlette @cb-axon @mjprager @Telos-sa @aj-stein-gsa @tnnrjmsn-eit @cybersechawk @JosephScarzone @kamamanh @aparkar-panw


### Proposal: **Adaptive Continuous Monitoring Framework (ACMF)**

The ACMF is a multi-faceted approach aimed at enhancing the KSI model by incorporating **dynamic inventory tracking, historical risk evaluation, predictive analysis, and API-centric reporting**. This framework addresses concerns around ephemeral assets, containerized environments, and the challenge of maintaining accurate, real-time inventory states.

---

#### 1. **Dynamic Inventory Synchronization (DIS)**
Instead of relying solely on traditional scanning methods, DIS continuously synchronizes inventory data from multiple sources, including cloud APIs, agent-based monitoring, and infrastructure-as-code pipelines.

- **Tag-Based Identification:** Uses resource tagging standards to track assets, including ephemeral containers and serverless functions.
- **API Integration:** Fetches live inventory data from cloud providers like AWS, Azure, and GCP to detect configuration changes in real-time.
- **Consistency Assurance:** Compares incoming data with pre-established baselines to identify discrepancies and unauthorized assets.

---

#### 2. **Historical Risk Evaluation (HRE)**
Building upon the current KSI approach, HRE provides a structured mechanism for understanding how risk levels evolve over time.

- **Risk Aging Metrics:** Introduces new metrics like Vulnerability Persistence Rate (VPR) to track the lifespan of unresolved findings.
- **Cumulative Exposure Index (CEI):** Aggregates unresolved vulnerabilities over time to measure overall risk exposure, accounting for both aging vulnerabilities and new findings.
- **Risk Regression Detection:** Alerts CSPs when previously resolved issues reappear, indicating ineffective remediation or new configuration changes.

---

#### 3. **Predictive Analysis Module (PAM)**
To enhance proactive risk management, PAM employs machine learning models to forecast potential vulnerabilities and areas of concern.

- **Trend Analysis:** Uses historical data to predict which assets are most likely to encounter recurring vulnerabilities.
- **Preemptive Remediation Suggestions:** Provides CSPs with recommendations based on observed trends to address anticipated risks before they manifest.
- **Scenario Simulations:** Allows CSPs to simulate the impact of proposed changes or vulnerabilities on their overall security posture.

---

#### 4. **API-Centric Reporting Mechanism (ARM)**
ARM shifts away from static reports and embraces a continuous, API-based approach to providing relevant data to agencies.

- **On-Demand Reporting:** Agencies can query real-time data directly from the CSP’s monitoring system through standardized APIs.
- **JSON and OSCAL Integration:** Supports both JSON and OSCAL formats for seamless compatibility with existing FedRAMP frameworks.
- **Access Control and Auditing:** Ensures that agencies only access authorized data, while maintaining comprehensive logs for auditing purposes.

---

#### 5. **Automation-Friendly Compliance Monitoring**
The ACMF is designed to be easily integrated with Compliance-as-Code solutions to automate the generation of monitoring artifacts.

- **Automated POA&M Creation:** Automatically generate POA&Ms based on unresolved findings and update them as remediation progresses.
- **Self-Validation Tools:** Provide CSPs with tools to verify compliance before submitting reports to agencies, reducing the likelihood of rejections or discrepancies.

### Replies

